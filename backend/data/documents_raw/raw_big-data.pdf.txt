
Adding Value to Manufacturing, Retail, Supply Chain, and 
Logistics Operations with Big Data Analytics 
  
Ishita Gupta and Manjunath Kamath 
School of Industrial Engineering and Management 
Oklahoma State University, Stillwater, OK 74078 
 
Introduction 
The concept of big data has been around for many years.  Only in the last few years have organizations 
started to understand how they can use big data to gain insightful knowledge about their business operations, 
which is enabling them to make better business decisions. While there is no single definition, big data 
usually works on the principles of four Vs - Volume, Velocity, Variety, and Veracity. As the name suggests, 
big data is really big, meaning a huge amount of data is being generated daily, reaching the scale of 
petabytes. This data comes in all forms -  structured, semi-structured, and unstructured and is pouring in 
from all directions and generated by many systems and devices, such as transactional systems, log files, 
GPS devices, smartphones, RFID readers, surveillance cameras, sensor networks, Internet of Things (IoT), 
and social media. Finally, as big data becomes an important asset for enterprises, the focus is also on the 
trustworthiness of data and its sources. 
According to Gartner, Inc., “Big data is high-volume, high-velocity and high-variety information assets that 
demand cost-effective, innovative forms of information processing for enhanced insight and decision 
making.”a In this article, we first elaborate on the big data concept and present the storage and processing 
technologies that have been developed to deal with big data.  We then briefly discuss the evolution of 
traditional analytical processing to today’s big data analytics.   Through several applications and use cases, 
we illustrate how big data analytics is adding value to manufacturing, retail, supply chain, and logistics 
operations.  Finally, we conclude by discussing key challenges that businesses have to face as the use of 
big data analytics becomes more widespread. 
Understanding Big Data 
Regardless of the decision to be made - optimized production/work schedules, accurate forecasts, customer 
preferences - data nowadays has the potential to help businesses succeed more than ever before.  From an 
organizational perspective, big data is a holistic approach of obtaining actionable insight to create a 
competitive advantage over others.1 There are two distinct approaches to applying big data - improve the 
existing processes by focusing on the current business needs or create products and services as new value 
propositions. A challenge that organizations increasingly face is finding and working with trusted data. 
Working with inaccurate and untrusted data can be worse than having no data at all. As data requirements 
and regulations become more complex, organizations must be aware of where all their data is coming from, 
where it is getting stored, and who is interacting with this data as conclusions are drawn.2 
 
 
 
a https://www.gartner.com/it-glossary/big-data/ 

 
2 
What is Big Data? 
The evolution of the world wide web has redefined the kind 
of data that needs to be handled and tracked, the speed at 
which the information is flowing into online systems, and 
the number of customers a company must deal with on a 
regular basis. Because of the changes happening in the Web 
environment, new definitions for big data have been 
proposed, with a focus on technologies that handle this data. 
O’Reilly defines big data as “Big data is data that exceeds 
the processing capacity of conventional database systems. 
The data is too big, moves too fast, and doesn’t fit the 
structures of traditional database architectures. To gain 
value from this data, organizations must choose an 
alternative way to process it.”3 
To understand how big data is transforming businesses, we 
need to understand its nature as most definitions of big data 
focus on the size of data in storage.4 Size is important but 
there are other aspects to big data namely variety, volume, 
and more recently, veracity.2 Together they are called the 4 
Vs of big data: Volume, Velocity, Variety, and Veracity. 
Going Beyond Traditional Data Warehouses 
Big data is not limited to traditional storage methods where 
structured data was stored and retrieved from relational 
databases, data warehouses, and data marts.6 Here, the data 
is uploaded to operational data stores using Extract, 
Transform, and Load (ETL) tools which extract data from 
internal and external sources, transform the data to fit the 
operational needs, and finally load the data into the data 
warehouse. The key point is that the data is getting cleaned, 
transformed, and cataloged before being made available for 
data mining and online analytical functions. This traditional 
data warehouse approach discourages the incorporation of 
new data sources until they are cleansed and integrated.  
Since data is ubiquitous these days, big data storage 
environments need to be “magnetic” in nature, attracting 
data from all sources. Hence, big data calls for Magnetic, 
Agile, and Deep (MAD) analysis skills, which differs from 
the traditional data warehousing approach. Given the growing number of data sources and the sophisticated 
tools for data analysis, big data storage should allow analysts to easily process and use data rapidly. 
Solutions like distributed file systems and Massive Parallel Processing (MPP) databases are available 
nowadays for providing high query performance and platform scalability. Non-relational databases such as 
Not Only SQL (NoSQL) were developed for storing and managing unstructured data.7 These newer 
technologies aim for scalability, data model flexibility, and simplified application development and 
deployment. They separate data management and data storage and focus on high performance scalable data 
4 Vs of Big Data 
Volume. The ability to process a large 
amount of information available and 
produced from transactional records to 
social media, from Internet of Things to 
system logs, etc. 
Velocity. The rate at which data is 
getting created every second of the day. 
With digitization being a major 
contributor, 
more 
data 
is 
being 
generated and logged than ever before.5 
Also, the rapid adoption of social 
media and the Internet of Things has 
created a deluge of data. Advances in 
machine learning and AI have made 
previously useless data much more 
useful now. 
Variety.  It is the diversity of data 
which organizations are witnessing. 
Companies are used to managing and 
processing a limited set of data, such as 
transactional 
records 
and 
logs. 
Advances in technology have enabled 
the analysis of unstructured data which 
includes images, voice recordings, 
videos, and texts generated from 
several platforms including social 
media to deliver new insight. 
Veracity.  It is not just the quality of 
data, but also the trustworthiness of 
data sources.  Basic issues are the 
accuracy and applicability of data.  
Accuracy of data is affected by 
uncertainty due to inconsistencies, 
incompleteness, ambiguities, etc. 

 
3 
storage, allowing management tasks to be written in the application layer instead of having it written in 
database specific languages.   
Why Big Data? 
When organizations adopt big data as a part of their business model, the first tangible question is usually 
what value this big data will provide to the company.7 Data must be used to make better decisions, to 
optimize resource consumption, and improve process quality and performance. It should also aim to 
perform precise customer segmentation, optimize customer satisfaction, and increase customer loyalty. 
New business models should be enabled with the use of big data which complement the revenue streams 
from existing products and create additional revenue from new products. 
Newer Data Sources, Newer Opportunities 
The new sources of big data include industries which are taking a big step step towards digitization, and as 
a result, data growth in the past few years has been phenomenal. Some of the areas where data is coming 
from include social media, internet browsing pattern data, advertising response data, financial forecasts, 
location information, driving patterns, vehicle diagnostics, and traffic and weather data from sensors, 
monitors, and forecast systems. Other sources of data include data from healthcare, where the healthcare 
industry is implementing electronic medical records and digital imaging, which is used for short-term public 
health monitoring and long-term research programs.  Similarly, low cost gene sequencing can generate 
hundreds of terabytes of data that must be analyzed to look for genetic variations and potential treatment 
effectiveness in life sciences.8 Another area is data from video surveillance which is transitioning from 
CCTV to IPTV cameras and recording systems that many organizations want to analyze for behavioral 
patterns for security and service enhancement. Transportation and logistics industry has been generating 
and storing enormous amount of data coming from sensors, GPS transceivers, RFID tag readers, smart 
meters, cell phones, material handling equipment enabled with sensors, etc. This data can be used to 
optimize operations and derive operational business intelligence to realize immediate and future business 
opportunities. 
Analytics and Big Data Analytics 
Data analytics is the science of analyzing raw data with the purpose of drawing conclusions about 
information contained therein.9 It involves applying algorithmic processes to derive insights. Analytics is 
used to extract previously unknown, useful, valid, and hidden patterns and information from large data 
sets.6 While the focus of analytics has been on inference, it can also provide prescriptive insights as 
explained later in this section.  Hence, analytics has a significant impact on research and technology, as 
businesses recognize its great potential in helping them gain competitive advantage. 
“Big data analytics is the use of advanced analytic techniques against very large and diverse data sets that 
include structured, semi-structured, and unstructured data from different sources, and in different sizes from 
terabytes to zettabytes.”b It helps in uncovering hidden patterns, unknown correlations, market trends, 
customer preferences and other useful information. Advanced analytics can help organizations discover 
what has changed and how they should react.  Analytics is the best way to discover new customer segments, 
identify the best suppliers, associate products of affinity, understand sales seasonality and so on.4 
Organizations are implementing specific forms of analytics tools and techniques which include data mining, 
statistical analysis, data visualization, artificial intelligence, machine learning, and other data capabilities 
 
b https://www.ibm.com/analytics/hadoop/big-data-analytics 

 
4 
which support analytics4. Though these techniques have been around for many years, organizations are 
using them now as most of these techniques adapt well to very large, multi-petabyte data sets. 
Big data’s worth is only realized when businesses can indulge in decision making using this data. To enable 
such data-driven decision making, organizations must use efficient processes to turn the high volume of 
fast moving and diverse data into meaningful insights. Analyzing big data allows researchers and businesses 
harness their data and use it to identify new opportunities which in turn leads to better and smarter business 
moves, more efficient operations, higher profits and satisfied customers and an overall competitive 
advantage.6 Big data analytics could be viewed as a sub-process in the complete process of knowledge 
extraction from big data.  
Evolution of Data Analytics Tools 
As organization began to adopt data analytics in the late 1990s and early 2000s, they faced many hurdles.  
Data was not that easily accessible as it is now and was mostly locked down and managed by IT 
professionals. Analysts used to spend more time collecting and preparing data than analyzing it.  They 
focused on finding more accurate and reliable solutions to business problems, while keeping the solutions 
simple at the same time so that business users could understand it.  Some examples of tools used during 
this time period are SAS, a tool for building backend data inference and modeling; Oracle and Teradata, 
detailed solution suites for easy development of solutions; IBM CPLEX, a tool for solving large 
optimization problems; and Cognos and MicroStrategy, tools for visualization, mostly in the form of 
reports. 
In late 2000s, social media giants like Google and Facebook and other internet-based companies in general 
started uncovering, collecting, and analyzing newer types of data which later evolved into big data. In 
addition to the data generated by companies in their internal operations and transactions, newer data was 
brought in from external sources including public data sources, social media, and mobile devices. Analysts 
realized this new data was qualitatively different (e.g., unstructured text, pictures, audio, and video) along 
with the much larger volumes as compared to internal company data.  This led to the development of newer 
tools and technologies, examples of which are Hadoop, a pioneer in distributed data storage and processing 
with low cost, flexibility, and scalability; Python and R, open source programming languages with vast and 
ever-evolving libraries for statistical data analysis; Tableau, Looker, and Microsoft Power BI, popular 
visualization products to develop, customize, and build visually appealing and interactive web dashboards. 
Descriptive, Predictive, and Prescriptive Analytics 
Analyzing data is not limited to deriving insights from the past, but it can also help businesses in predicting 
future outcomes and optimizing business performance. Currently organizations use three types of analytics 
at different stages in their decision-making process - Descriptive, Predictive, and Prescriptive analytics as 
shown in Figure 1. The latter two are also referred to collectively as advanced analytics. 
Descriptive analytics does exactly as the name suggests, ‘describe’ or summarize the data and convert it 
into something useful. It is the most basic type of analytics and almost 90% of the organizations today use 
this technique. Descriptive analytics is the analysis of historical data using data aggregation or data mining 
and lies at the bottom of the big data analytics value chain. However, it is extremely valuable because it 
provides insight into past behaviors which can help in understanding how several factors can influence the 
organization’s future.  
Descriptive analytics is an important step to make raw data understandable to its users, and it helps in 
answering questions like “What is happening?” Consider for example, a metric that companies get from 

 
5 
web servers using Google Analytics tools, namely page views.  It can be used to determine if a strategy was 
a success or not. The main objective in descriptive analytics is to find the reasons behind the previous 
performance patterns of the organization and to identify and address areas of weakness and strength so that 
it can help the organization in strategizing.  
The majority of the statistics we use comes from descriptive analytics – e.g., calculations as simple as 
averages and standard deviations. Descriptive models use basic mathematical and statistical techniques to 
derive key performance indicators that can highlight the historical trends in data. STATA, MS Excel, and 
SPSS represent the older generation of descriptive analytics tools, while R and Python are quickly becoming 
the preferred tools in industry because of vast open-source libraries and the ease of development and 
deployment. Descriptive analytics can yield historical insights into an organization’s production, inventory 
levels, sales, operations, financials, and customer behavior.  
 
Figure 1. Analytics Framework by Tom Davenport26 
Predictive analytics can be defined as the ability to “predict” what might happen and a better understanding 
of future outcomes. It is one of the more sophisticated types of analytics techniques and employs statistical 
techniques and machine learning. It is used to detect clusters, tendencies, and exceptions, and to predict 
future trends, making it a valuable tool for forecasting. The foundation of predictive analytics is probability. 
It takes the data which the user has and tries to fill in the missing data values with best guesses. It helps in 
finding the answer to ‘What could happen?’ With properly tuned models, predictive analytics can support 
complex forecasting in marketing and sales. This helps an organization to set realistic goals for business, 
restrain expectations, and do effective planning. 
Tools used to apply predictive modeling vary by the nature of model’s complexity, but some commonly 
used tools are SAS, MATLAB, R, Python, among others. The common functionality of these tools is that 
they combine historical data found in POS, ERP, CRM, and HR systems to identify patterns in the data and 
apply algorithms such as random forest and Generalized Linear Model (GLM) for prediction, and K-means 
clustering for identifying clusters. Finally, simulation can be employed to statistically predict the outcomes 
of specific decision scenarios.   

 
6 
An application of predictive analytics is to produce credit scores, which are used by financial services to 
determine the probability of customer making timely payments. Other business uses include, how sales 
might close at the end of a year, inventory level forecasts, predicting what items a customer might purchase 
together and other customer purchasing patterns. Despite all the advantages that predictive analytics brings 
to the table, it is important to understand that forecasting is just an estimation, and its accuracy depends on 
the quality and stability of data.  
Prescriptive analytics is the most sophisticated analytics approach which makes use of optimization 
techniques to explore a given set of options and prescribe the best possible solution for a given scenario. 
As the name suggests, it “prescribes” a solution to a specific problem. One approach is machine learning 
employing neural networks where optimization models are used to determine coefficients or weights of 
neurons using training data sets.  Once trained, the neural network model can suggest the optimal course of 
action supporting the business objective for a given set of business inputs. Simulation, a predictive analytics 
tool at its core, can also be part of a powerful prescriptive analytics approach when combined with 
appropriate search or optimization techniques.  Prescriptive analytics not only predicts ‘What will happen?’, 
but also determines “What the company should do?” It provides recommendations for the actions to be 
taken to achieve optimal business performance. Because it has power to suggest optimal solutions, 
prescriptive analytics is the ultimate frontier for advanced analytics. 
Prescriptive analytical models are complex in nature. However, when implemented efficiently, prescriptive 
analytics can have a significant impact on the decision-making effectiveness of the organization. Technical 
advancements such as cloud computing have made deployment of these complex models much easier. 
Companies which have access to analytics experts and the powerful computing resources needed are using 
prescriptive analytics to optimize production and inventory decisions in supply chains, optimize customer 
experience, and to make sure that the right product is being delivered at the right time.  Airline systems use 
sophisticated prescriptive models for optimal seat inventory allocation for a given price structure based on 
travel factors, demand levels, purchasing patterns, timings, etc., in order to maximize the revenue generated.  
Increasing number of organizations are realizing that big data analytics gives a competitive advantage and 
hence, they are ensuring to choose the right kind of analytics solutions to reduce operational cost, enhance 
service quality, and increase ROI. 
Big Data Analytics Applications and Use Cases 
Supply chain activities produce a huge amount of data, which is being continuously generated by systems 
and devices such as POS, ERP, SCM, RFID, GPS, blogs, and wiki entries, not to mention the unlimited 
data generated from sources like CCTVs, digital clickstreams, imagery, social media posts, and discussions 
on various forum platforms. Advanced connected devices and technologies which support today’s supply 
chain such as sensors, smart devices, and tags are continuously gathering real-time data and providing an 
end-to-end visibility in the supply chain. It becomes the task of supply chain managers to tap and process 
this data to make insightful decisions which could help boost productivity and reduce costs. 

 
7 
 
Application 
Area
Company
Technique/Technology/
System
Impact
Supplier Insight Program Greater insight into suppliers' financial stability, 
performance, and ability to provide services.
Immersive Design Center
Achieved product excellence, reduction in time-to-
market through co-development and co-production. 
Better alignment between engineers, suppliers, and 
customers.
Lennox 
International
Integrated Forecasting 
System
Better service level; accurate prediction of customer 
needs and demand; automated planning and 
forecasting operations.
Walmart
Data Café
Inventory management with streaming analytics, real-
time data delivery and updates every few hours, and 
accurate performance analysis of each store.
Groupe 
Danone
Machine learning-based 
planning system
Improved forecasts and sales with better prediction 
accuracy and greater profit margins.
Granarolo
Machine learning 
application
Accurate forecasts, reduction in delivery time by 
upto 50%, and better service levels.
Levi Strauss 
& Co.
IoT and Predictive 
Analytics application 
using Intel's Trusted 
Analytics Platform
Better tracking of in-store items using RFID tags; 
updating item location and inventory; helping 
salesperson track misplaced item to avoid lost sales.
Morrisons
Data-intensive 
forecasting method
Increase in forcasting accuracy; reduced inventory, 
stockouts, and obsolescence; better access to 
company's logistics needs.
Anticipatory shipping
Orders packed and pushed into logistics network 
before actual customer orders.
 Flexible automation
Robots bring items from storage locations to picking 
and packing area.
Drone-based delivery
Goods delivered to locations less than 30 minutes 
away from an Amazon warehouse
Cloud-based 3D 
warehouse layout 
planning
Improve storage efficency and picking productivity 
of an exiting warehouse by simulating new 
configurations
Camera guided AGVs 
and Tracking
Optimize picking accuracy, inventory turns, and 
warehouse productivity in real-time using inputs 
from sensors, such as  shelf weight and weight on 
forklift.
Quality early-warning 
system
Reduced rework, increased productivity and cost 
savings, higher quality standards, and improved 
service levels, by detecting and prioritizing quality 
related issues much sooner in the supply chain.
Buying analysis tool
Greater demand and supply visibility, better 
distribution channel management, better service level, 
and improved inventory management.
Accounts receivable tool Optimized the resources needed for revenue 
collection.
Merchandise 
Warehouse 
Co.
Real-time monitoring and 
tracking
Greater visibility for customers, better pallet 
management, optimized space utilization, greater 
labor productivity, inventory accuracy of 99.9%, and 
improved customer satisfaction.
UPS
ORION
Optimized 55,000 delivery routes in North America, 
saving close to $400 million annually. Reducton in 
cost and emissions by selecting the right mode of 
transportation.
SmartTruck
Optimized initial route planning based on incoming 
shipment information, reduction in mileage and cost, 
and improved CO2 efficiency.
Resilience 360
Accuracy in risk detection, prevent production 
inefficiencies and revenue losses, maintain service 
levels, and reduce emergency cost by efficiently re-
routing shipments in case of unforeseen events.
Geovista
A tool for small and medium scale industries to 
analyze potential business opportunities. Real-time 
information provides realistic forecast of competitors 
in a given location.
Address Management
Improves shipment delivery accuracy in areas where 
quality of address information is poor. Real-time 
ddress verification to optimize route planning.
DHL
Logistics
Applications of Big Data Analytics
IBM
Raytheon
Manufacturing
Amazon
Logivations
Retail
Supply Chain 
and 
Warehouses

 
8 
Applications and Use Cases in Manufacturing 
Raytheon, a major U.S. based defense contractor and industrial corporation, made use of data analytics to 
reduce costs within their supply chain and production operations. They developed a Supplier Insight 
program, which integrated structured and unstructured data from internal and external sources.10 With more 
than 10,000 suppliers, they needed a platform that could provide rapid, data-driven decision making 
capability. With this new system, they could track suppliers’ financial stability, performance, and their 
ability to provide services in the face of disruptive events. Raytheon was able to immediately identify if a 
supplier could provide what they needed, and quickly made decisions that reduced any adverse impact on 
their customers. Supplier Insight has allowed them to negotiate the cost better, by engaging in long-term 
contracts with suppliers for multiple programs.10 They now have an ability to look across all their suppliers 
and programs to achieve cost reductions. Raytheon has also developed smart factories which have the 
capacity to handle big data coming from different sources like sensors, instruments, CAD models, internet 
transactions, simulations, and digital records in the company, which equips them with real-time control of 
various elements of the production processes. For example, their Immersive Design Center (IDC) makes 
use of a 3-D immersive environment to achieve product excellence and decrease time-to-market through 
co-development and co-production of products by immersive data visualization and interaction.10 This also 
resulted in better alignment between their engineers, suppliers, and customers. They work together to refine 
the design and detect potential problems without the work and rework associated with expensive prototypes, 
resulting in reduced costs.10  
Lennox International, a U.S. based cooling and heating devices manufacturing company, integrated 
machine learning into its forecasting system to ultimately improve customer satisfaction while coping with 
their expansion throughout North America.11 With the help of machine learning algorithms, they accurately 
predicted customer needs, while understanding customer demand better. It also helped the company to 
automate its planning and forecasting operations. 
Many companies gather data on supplier information and purchasing volumes for annual supplier 
performance review, spend analysis, and cost savings analysis functions to support strategic decisions.  For 
example, a pharmaceutical company created a database of all the bids submitted for packaging.12 This data 
was then evaluated to understand the cost structure of suppliers and to create detailed cost models for 
different packaging options. Such models can help in the selection of the most cost-effective supplier for 
new packaging.12 Another example is how IoT with its network of sensors embedded in millions of devices 
can enable new opportunities in manufacturing. For example, real-time information on a machine’s 
condition can initiate a production order for a spare part, which then can be shipped using a drone to the 
plant engineer for replacing the faulty or near faulty part.12 It also helps in determining when and how 
critical maintenance is required by a specific machine, thereby avoiding costly equipment breakdowns and 
improving the overall production efficiency.  
Daily production needs to be monitored to maintain the efficiency and output of a company. Big data 
analytics uses the data collected from operational machines, employee records, and data logs of the number 
of units produced, to provide insights to the operations manager, helping him/her to make changes that are 
profitable for the company. Manufacturers are also exploring predictive analytics to realize significant 
savings in product testing and improving product quality. Since different products and parts require 
different tests, instead of performing numerous quality tests on each part, data mining and pattern 
recognition can be used to determine the type and number of tests truly needed for each part or product.13  
 
 

 
9 
Applications and Use Cases in Retail 
Walmart, the number one fortune 500 company, has the world’s largest private cloud, which helps support 
real-time data feeds to its decision makers. Walmart’s Data Café based at their Bentonville, Arkansas 
headquarters takes care of most of this cloud architecture.14 Their original data infrastructure only enabled 
managers to get weekly reports, which prevented them from making decisions based on real-time market 
conditions. Also, the reports were standardized with little room for customization. Data café, which was 
built on SAP’s HANA in-memory analytics engine, enabled inventory management with streaming 
analytics, and provided an enterprise view of timely information flow for a large cross-sectional staff 
looking to resolve every-day business issues.14 The data delivered through this system is almost real-time 
and updated every few hours.  Furthermore, the system was designed to be responsive to providing reports 
and queries required by managers in the given time frame, which helped them gain timely insight and make 
better decisions. These insights are derived from “200 streams of internal and external data which includes 
40 petabytes of recent transactional data, and can be manipulated, modeled, and visualized.”14 The 
importance of near real-time insights is crucial since it helps managers respond to challenges in real-time 
as they arise. For example, on Black Friday, Walmart’s Data Café provides near real-time insights on the 
performance of east-coast stores, which enables Walmart to make pricing adjustments for west-coast stores 
before they open.14 During a recent Halloween, sales analysts were able to see that two stores were not 
selling a novelty cookie that was very popular in most stores. Using near real-time data from Data Café, it 
was discovered that simple stocking oversight led to the cookies not being put on shelves in these stores.14 
The company was able to react in real-time to avoid additional lost sales. Data Café also provides automated 
alerts to managers when a metric falls below a threshold in a department. This tool has reduced the problem-
solving time from weeks to minutes using reliable internal and external sources of data. 
Levi Strauss & Co, a leading American clothing company, provides better in-store shopping experience for 
its customers by helping them find the product they want and avoids missing sales by locating misplaced 
items using IoT technology coupled with advanced analytics. Levi’s in collaboration with Intel® 
implemented a solution using Intel’s Trusted Analytics Platform (TAP), which helped salespersons to 
quickly find misplaced items in the store.15 This application made use of RFID tags woven into clothing 
items, in-store antenna sensors installed in the ceiling of the store to continuously track the RFID tags, and 
a gateway system located in the store to collect data from these sensors and send smaller data sets to a 
cloud-based analytical tool built on TAP for detailed analysis. This technology helped determine when 
items are no longer in their correct place or no longer available at that time. TAP algorithms use data 
collected overnight to determine the exact location of various groups of items, and during store hours 
sensors track the location of items and an algorithm determines if an item is in its assigned location. If an 
item is placed in its assigned group location, no action is generated by the algorithm.  Suppose a pair of 
jeans is lying in the T-shirt section or left in the fitting room, the TAP algorithm will generate an alert on 
the mobile application instructing the salesperson to put the item back in its assigned location.15 This helps 
the salesperson to keep the item where it belongs and avoid lost sales. Levi’s also aims to generate customer 
insight using big data analytics with the data collected from sensors tracking customers’ in-store behavior 
to better understand their preferences.15 
Groupe Danone, a French multinational food-product corporation, found itself making accurate predictions 
only 30 percent of the time for responses to promotional offers, which was resulting in significant losses to 
the company.11 When they implemented machine learning in their planning architecture, they saw 
significant improvement in both sales and forecasting. Similarly, Granarolo, an Italian dairy company, used 
machine learning to increase its forecasting accuracy by 5 percent, decreased delivery times by up to 50 
percent of the original time, which resulted in better service levels.11 Morrisons, one of UK’s largest food 

 
10 
retailers, was able to dramatically improve same store sales and achieve a 30% reduction in shelf gap and 
a 2 to 3 day reduction in store inventory by implementing a demand forecast and replenishment solution 
from Blue Yonder, which uses AI technology to “improve demand planning and reinvigorate replenishment 
based on customer behavior in every store.”16 Blue Yonder’s data-intensive forecasting methods deployed 
as cloud-based services is making such advanced capabilities accessible to other retailer’s as well.12 
Applications and Use Cases in Supply Chains and Warehouses 
In supply chain operations, planning and forecasting are among the most data-driven operations, which use 
an array of supply chain planning tools supported by ERP systems. With the use of supply chain analytics, 
it is now possible to re-envision the planning processes by using external and internal data sources to make 
real-time decisions based on market trends, uncertainty, seasonality, and other fluctuations.  
IBM understood the value of big data analytics early and employed it in optimizing their supply chain 
operations. They have used various analytical tools to solve a range of problems, and a few of them are 
discussed here.17 IBM’s Quality Early-Warning System (QEWS) was typically deployed upstream at 
suppliers, IBM’s operations, and in the field.  QEWS detects and prioritizes quality related issues much 
sooner than the traditional quality control processes. Analyzing big data coming from across their supply 
chain, IBM was able to reduce rework, increase productivity, ensure higher quality standards, and improve 
customer satisfaction, leading to significant cost savings. For a company like IBM, ensuring correct 
inventory levels with so many business partners was challenging. They made use of IBM Buying Analysis 
Tool, which not only provided demand and supply visibility, but ensured better distribution channel 
management, delivery of the right product at the right time to meet customer demand, while maintaining 
proper inventory levels. IBM also used a tool named Accounts Receivable, which uses advanced analytics 
to optimize the resources needed to collect revenues. They also make use of supply chain social listening, 
an innovative way to use social media to monitor channels and provide valuable data on events which may 
disrupt the supply chain.17 It also helps them obtain timely information and feedback on their products. As 
an early adopter, IBM has been using predictive and prescriptive analytics in its supply chain over the last 
several years. 
Warehousing is another area where big data analytics is creating new opportunities.  Logivations, a German 
supply chain solutions provider, offers a cloud-based 3D warehouse layout planning and optimization tool, 
camera-guided AGVs and tracking, and various other supply chain analytics solutions.18 Such technologies 
help in warehouse design optimization and in improving storage efficiency and picking productivity of an 
existing warehouse by simulating new configurations. Another example is the analysis of images and videos 
captured by AGVs, and sensor inputs including shelf weight and weight on the forklift, to monitor picking 
accuracy, inventory turns, and warehouse productivity in real-time.12 Also, forklift drive picking 
productivity and route optimization can be achieved by analyzing the route choices and driving behaviors.12  
A leading forklift provider is looking into all these opportunities, and figuring out how a forklift truck can 
be used as a big data hub - collecting  real-time data to identify additional sources of waste in the warehouse 
operations, using a hybrid of analytics and ERP and WMS data. Amazon is another warehouse automation 
pioneer, deploying Kiva robots that bring the items (racks) to the picking and packing area in their 
fulfillment centers. With increasing pressure to reduce order-to-delivery times, warehouses are turning to a 
flexible automation strategy by using autonomous technologies such as Amazon’s Kivac robots and 
GreyOrange’s Butlerd system to increase their picking efficiency.  Amazon has also tried to deliver goods 
 
c https://www.amazonrobotics.com/#/  
d https://www.greyorange.com/butler-goods-to-person-system  

 
11 
to people living less than 30 minutes away from an Amazon warehouse or distribution center via a drone. 
Amazon has also patented an “Anticipatory Shipping” technology to identify which orders should be 
packed and pushed into the logistics network before the actual customer orders are placed.12 
Merchandise Warehouse Co. (MW), a logistics provider of multi-temperature warehouse services in the US 
mid-west, provides services such as tempering, inspection, blast freezing, temperature monitoring, labeling, 
import/export, and packaging.19 With such operations there is little room for error, since clients’ food 
products could get spoiled if they are not maintained at correct temperatures.19 MW needed real-time 
information technology which would help them track the state of items in its warehouses at all times and 
enable quality assurance with comprehensive traceability.  They wanted this for all operations including 
inspections and holds.19 Technologies such as CCTV, WMS, electronic data interchange, mobile 
computers, and scanners were employed to help track and analyze data to get real-time information in the 
warehouse and manage inventory. It helped MW’s customers gain visibility by having on-line access to 
temperatures, activity reports, and information about inventory levels. MW’s solution also includes tools 
for pallet management for tracing every pallet from the time it arrives in the warehouse to until it leaves. 
Special functionalities for cold storage such as temperature reading and recording and the ability to restrict 
inventory to marked temperature zones were provided by the new system. It also ensures greater labor 
productivity and accuracy using workflow-based warehouse management and could automate processes 
designed for specific customer needs. MW reaped various other benefits from this initiative like accurately 
capturing billing events in real-time resulting in reduced labor used for billing and paperwork. The system 
helped the company deal with the issue of “catch weight”, where the actual weight of the product,  especially 
meat, varies when it hits the retail shelves, a common problem in cold storage warehouses and food 
industry.19  Increased customer satisfaction levels were also achieved, since clients had real-time access to 
information and reports when needed. The solution helped MW achieve an inventory accuracy of 99.9 
percent from a previous 98.6 percent.19 
Applications and Use Cases in Logistics 
Logistics companies need to keep the goods moving at all times, even in the face of disruptions such as 
storms, cargos getting stranded due to ship crashes, and geopolitical events in order to keep the businesses 
running.  A Netherlands based logistics management company uses big data analytics on Microsoft’s Azure 
cloud to keep its customers informed about the number of goods in each container, their location at a given 
time, and expected delivery times.20 Purchase orders are tracked using mobile applications to identify 
challenges which could delay the delivery of an order. Tariff calculations and fees related to the movement 
of shipping containers are calculated by another application which can be accessed by the client, giving 
them a greater insight into financial risks.20 These mobile applications make use of big data analytics in 
conjunction with Microsoft cloud technologies to combine and analyze data coming from news feeds and 
internal supply chain operations to provide actionable business insights. Previously the time it took to 
identify a challenge and develop a solution to address it could be anywhere from 3 to 9 months. With the 
use of big data technologies, this time has been brought down to a couple of weeks depending upon how 
complex the problem is.20  
Companies managing their own supply chains and those outsourcing to third-party logistics providers 
manage a massive flow of freight, goods, and products daily while at the same time creating vast data sets. 
Millions of shipments are tracked daily from origins to destinations, generating information such as the 
content, weight, size, location, and route of each individual shipment, across a large number of networks. 
Companies are exploiting and analyzing these large data sets to improve their operational efficiencies, 
effectiveness, and customer service. A study by the Council of Supply Chain Management Professionals 
shows that 93 percent of shippers and 98 percent of 3PL providers feel that data-driven decision making is 

 
12 
a crucial supply chain activity.21 Also, 71 percent of these believe big data improves performance and 
quality. Logistics companies can utilize big data analytics to consolidate, interpret, and store the data 
coming from various sources for immediate or future use based on their requirements.  
Courier and delivery companies like UPS use real-time routing of deliveries using the trucks’ geo-location 
and traffic information data. UPS spent almost 10 years developing its On-Road Integrated Optimization 
and Navigation system (ORION) to optimize close to 55,000 routes in North America in its delivery 
network.22 This system saves the company $300 million to $400 million annually by saving about 100 
million miles per year, which is a reduction of 10 million gallons of fuel consumed and reducing CO2 
emissions by almost 100,000 metric tons.22  Data mining techniques also help logistics companies deliver 
services with fewer delivery attempts, by using predictive analytics to predict when a customer is more 
likely to be available at home.12 Costs and carbon emissions can also be reduced by selecting the right mode 
of transportation for deliveries. An example is the use of supply chain analytics to understand the priority 
of shipments and determine which ones need immediate air or truck deliveries and which still have time 
and can be delivered by rail.12  
Better transportation planning can be achieved with the use of Transportation Management System (TMS) 
which can help identify future shipping patterns, optimize routes, carrier selection, or loads, and secure 
necessary capacity. This is achieved by tracking shipment frequency and identifying the endpoints of supply 
chains by studying precise inbound and outbound statistics. Direct application of predictive analytics is 
helping logistics providers make real-time decisions which result in reduced costs, greater reliability, and 
improved customer satisfaction. For example, data streams produced by sensors on delivery trucks, beacons 
which broadcast their presence to nearby devices such as computers and smartphones, radar devices, and 
IoT help a company determine the likelihood of a shipment arriving on time or getting delayed by 
employing simulation models.23 When a shipment is going to be late, a carrier can make real-time 
adjustments to prevent bottlenecks further down the supply chain.23 
DHL, a global logistics provider, has extensively explored big data analytics in their supply chain activities 
and is currently employing several smart systems around their services. Increasing the last mile efficiencies 
is often the most expensive step in the supply chain.24 Last mile optimization is an extensively studied area 
and researchers have found promising applications of big data analytics here. Data analytics is applied to 
achieve real-time optimization of delivery routes, where streams of data are processed to maximize the 
performance of the delivery fleet. Rapid processing of real-time information supports the goal of route 
optimization on the last mile, saving time in the delivery process. When the vehicles are loaded and 
unloaded, manual sequencing of shipments is eliminated by the use of sensors, and dynamic calculations 
are used to find the optimal delivery sequence. Based on real-time traffic conditions on the road, telematic 
databases are used to change the delivery route automatically. DHL’s SmartTruck uses data mining, 
machine learning, and other data analytics techniques to optimize the initial tour planning based on 
incoming shipment on a daily basis.24 Dynamic routing system recalculates the routes depending on the 
traffic situations and delivery times. This also results in cost reduction and improved CO2 efficiency by 
reducing the miles travelled. 
It is vital for robust supply chains to be able to cope with unforeseen events in today’s rapidly changing 
world. Apart from being flexible and resilient, businesses need accurate risk detection systems to keep 
running smoothly. Big data analytics and complex event processing algorithms are used to alert businesses 
when a pattern falls in the set of critical conditions such as tornadoes or floods in an area, or breakdown of 
fleet. These alert systems send a report on the probability and impact of the risk and provide suitable 
actionable insight to alleviate potential interruption. With this information on hand, customers can re-route 
their shipments or manage supplies from other distribution locations. DHL’s Resilience 360 risk 

 
13 
management solutions aims to provide such functionalities.24 It is equipped with two components, a risk 
assessment portion and supply chain monitoring instruments, both operating in real time. This improves the 
resilience of the supply chain and prevents production inefficiencies and revenue losses. Resilience 360 is 
designed to maintain prescribed service levels, protect sales and operations, and reduce emergency costs, 
creating a competitive advantage for the company.24 
Future economic development is often modeled on global transportation of goods and services. The type of 
goods shipped indicate the local demand and supply preferences. Logistics providers make use of big data 
analytics tools to extract detailed microeconomics insights from data generated by millions of daily 
shipments by their distribution networks. These shipment records are a valuable resource for market 
intelligence research, and logistics providers refine this data to substantiate existing market research. 
Regression analysis techniques are used to produce demand and supply forecasts with the use of the 
shipment records and market research outcomes. The primary target group for these advanced data analytics 
services are small and medium-sized enterprises, which lack capacity to conduct their own market research. 
The results from regression-based analytics have high predictive value, which can help these enterprises 
serve a larger customer base, and generate accurate forecasts based on industry, geography, and product 
category. DHL Geovista is one such online geo-marketing tool available for small and medium-sized 
enterprises to analyze potential business opportunities.24  
DHL Address Management system is another useful tool making use of big data techniques to deliver 
shipments more accurately.24 Customer’s delivery address verification is a fundamental requirement for 
any logistics provider. This can be troublesome in developing countries and other remote areas, where the 
quality of address is usually poor due to lack of structured naming schemes for streets and buildings in an 
area. Address Management uses daily freight and parcel delivery data and matches this data with reference 
data and returns the incorrect incoming data with validated data from the database, in order to verify the 
address in real-time and optimize route planning for retailers and public sector entities.  
Other Applications 
There are several other applications of big data analytics which a company can encounter on a regular basis. 
Locating a new store is a strategic decision for a company, and big data analytics could play an important 
role here. Extensive data analysis is performed by the analysts in exploring customer data, demographic 
factors, retailer network, location of other competitors in the area, and market potential. A recent example 
of this is the location for Amazon’s HQ2. Visualizing the growth of a company has become easier with the 
use of data analytics, since it is now possible to quickly compare the performance matrix of different sites 
and identify the reasons behind such results. Predictive analytics comes in handy in analyzing the market 
and gaining insight on questions related to global growth strategy, site relocation, new product introduction, 
and supplier selection. 
Price optimization is crucial for a company as having the right price for both customer and retailer keeps a 
business profitable.25 Data analytics tools simplify the process of price formation, which not only accounts 
for the cost of production of an item, but also the spending capacity of the customers and presence of 
competitors in the market. Price flexibility, buying patterns of the customers, competitors’ prices, and 
seasonality are analyzed using the data coming from various sources. Machine learning algorithms help 
identify the costs which meet the business standard by using customer segmentation to record the responses 
to changes in prices. Furthermore, using real-time price optimization techniques, retailers can attract new 
customers and retain existing customers by adjusting the price as per market trends. Recommendation 
engines is another great way of predicting customers’ behavior, since they give a retailer insight into 
customers’ reviews and opinions. It also helps the retailers to increase sales and stay abreast with trends. 

 
14 
Based on machine learning algorithms, recommendation engines make adjustments depending on customer 
preferences, previous shopping and browsing experience, demographic data, need, and usefulness. 
Collaborative or content-based data filtering is used in this process to gain useful insight which gives 
leverage to retailers on customers’ opinions.  
Big Data Challenges 
Companies often fail to understand what big data is, its benefits, and more importantly the computing and 
the human infrastructure required to realize its true potential. Without a clear understanding of the concept 
of big data, adopting and implementing a project using big data tools can seriously challenge its success. 
Having discussed various applications and use cases of implementing big data technologies in 
manufacturing, retail, supply chain, and logistics, it is important to understand the associated challenges. 
We can say that handling big data is complex and companies should identify what they aim to achieve when 
they decide to invest in technologies using big data.  
The first challenge that a company is likely to face is making sense of the complex big data landscape and 
reducing their dependence on legacy systems. Even though the industry is shifting its focus to the digital 
age with adoption of IoT and artificial intelligence, it is still a long way before the full potential of big data 
is realized. Industry has to develop an awareness of the various elements of the big data landscape, which 
include sensors to social media that collect data, in-memory to cloud for data storage, data mining to deep 
learning to convert data into useful business insights or actions.  Any new business solution will involve a 
combination of these elements and the role of people in the resulting work system is likely to change 
significantly. Most people are resistant to change, and it shows in companies when workers stick to to an 
old way of thinking and doing work. An example is the use of Excel, which to the present day remains one 
of the popular tools in many companies, despite having  many limitations when compared to newer tools.27 
While there is a need to educate industry to change this legacy mentality, there is no need for an abrupt or 
complete shift to newer tools. A viable option is to slowly augment existing systems with big data analytics 
tools and capabilities.   
With the phenomenal increase in the size of data, the problem of storage space for big data has become a 
real problem for many companies. Cloud storage is soon becoming the only viable alternative with the ever-
increasing need for storage space.  With the maturity of the cloud computing infrastructure, which includes 
storage, applications, and computing platforms, companies are beginning to consider shifting to the cloud 
infrastructure for most of their computing needs. But transitioning from the traditional in-house computing 
infrastructure to the cloud infrastructure has its own challenges.  According to McAfee, “Most organizations 
that have been around awhile have a hodgepodge of hardware, operating systems, and applications, often 
described as ‘legacy spaghetti’.”28 First, companies have to address legacy system issues and simplify their 
system before moving to the cloud. For the most part, cloud is cost-effective compared to building and 
running an IT infrastructure.  However, a company needs to carefully evaluate the cost factor based on their 
specific needs, for example, in-house applications requiring continuous transfer of large data sets. 
Academic institutions have begun to address the need for skilled professionals in the field of big data 
analytics with specialized MS degrees in Data Science.  These degree programs are housed mostly in 
business schools or computer science departments.  Engineering schools to a large extent are still lagging 
in providing adequate training in data science to their graduates.   Data science professionals can manage 
and analyze large volumes of real-time data coming from multiple sources and in different formats. With 
several new technologies such as the NoSQL data management framework, Hadoop, cloud computing, and 
in-memory analytics, their skills are vital for the rapidly changing computing landscape. Given that 
engineering schools are still looking for the right curriculum mix (e.g., minors, degree options, and 

 
15 
certificates) to train engineers in data science, training employees at entry level is a challenging and 
expensive proposition for companies dealing with these newer technologies. When industry hires data 
science professionals, akin to software developers and programmers, they need guidance from subject 
matter experts (SMEs) to build the right tools and techniques that can help industry harness the power of 
big data in the long-run.  Industry needs to quickly educate SMEs to understand the big data analytics 
capabilities and empower them to develop big data strategies working alongside with the data science 
professionals.   
As seen in recent times, data privacy has become one of the major concerns of organizations. With recent 
threats like hacking of personal data, individuals and companies have become apprehensive about linking 
data from multiple sources as it may compromise an individual’s privacy. Also, with an increase in the 
number of connected devices within the industry, data security has also become a big concern and presently 
this risk is greater than ever. Big data analysis uses huge amounts of data for analysis and mining purposes 
to reach some meaningful conclusion, and security of this big data can be enhanced by using techniques 
such as authentication, authorization, and encryption.   
Effective flow and sharing of information among supply chain partners is critical to the success of today’s 
digital supply chains.  Unauthorized disclosure and data leakage of information shared among supply chain 
partners have been identified as two main threats in today’s digital supply chains.29 Visibility needed within 
a supply chain and consumers’ demand for transparency seem to be at odds with security requirements.  
With newer, secure technologies such as blockchain and data cleanroom, it is possible to achieve both 
visibility and transparency.30  Data cleanroom is a shared environment between two or more supply chain 
partners that is completely secure from external access and where each partner can decide the level of 
visibility to their data. Blockchain, a decentralized, distributed database is one of the most secure options 
available for supply chain partners for real-time information tracking.  Another important, but often 
overlooked challenge is the ethical use of data.  The legal infrastructure has not kept up with the rapid 
development in technology, which is able to collect and store vast amounts of consumer data with or without 
their knowledge.  While it may be legal, certain use of the data may be considered unethical.  Such actions 
may have a negative impact on a company as today’s consumers are more educated and have experienced 
negative consequences of such unethical usage. 
In a recent survey of supply chain professionals conducted by APQC, “lack of people with the needed 
skills” was identified as the biggest barrier to advanced analytics applications in industry.31  In addition, 
these employees need “a good understanding of the business to provide solid advice.”29  Resistance to 
change and lack of access to data across disparate systems were the second and third biggest barriers, 
respectively.  In addition to lack of access to data, issues such as inconsistent and unorganized data are also 
issues in some cases as different companies record their data in different formats, platforms, and systems.27 
This results in difficulties in reconciling data from multiple sources and its subsequent analysis to gain 
useful insights.  
The Way Forward 
As companies make a push for big data analytics applications, they should first establish a clear business 
need such as “solving a problem or seizing an opportunity.”7  According to Watson, “big data initiatives 
should start with a specific or narrowly defined set of objectives rather than a ‘build it and they will come’ 
approach.”7 Pilot schemes are a good way to demonstrate the value of big data analytics.32  It is common to 
focus the initial business case for big data analytics on customer-centric objectives.7 The various 
applications and uses cases discussed earlier cover many different areas that have benefited from big data 
analytics. Whatever be the area, it is desirable that the pilot project address a problem tied to a specific 

 
16 
business outcome.  The pilot project should not only help solve a business problem, but also demonstrate 
the effectiveness of big data analytics for the organization and its stakeholders. Finally, for successful big 
data initiatives it is essential to have strong, committed sponsorship and alignment between the business 
and analytics strategies.7 In the early stages of adoption, the sponsor could be the CIO and then shifting to 
function-specific executives as business opportunities are identified.   
To benefit from big data analytics companies must also establish a data-driven decision-making culture, 
which calls for acting on insights from data rather than on pure managerial intuition.32 Promotion of data-
sharing practices, increased availability of training in data analytics, and communication of the benefits of 
data-driven decision making are some of the strategies for promoting a data-drive culture.7 While workforce 
training needs to focus on improving technological and digital proficiency, the future work environment 
also demands training in certain soft skills.  The work environment is changing with the rapid introduction 
of AI, automation, and analytics-driven solutions.  Workers need to be open to new ways of working and 
have openness to agility, adaptability, and working in teams to cope with a constantly changing external 
environment.  In the long-run, big data needs to become an integral part of the organization’s operating 
model. There also needs to be clear ownership for big data in the organization with leadership positions 
such as a chief analytics officer.32 Data science should become another established skill in the organization.   
Acknowledgements 
We would like express our gratitude to William Ferrell for his constant assistance and encouragement 
during the development of this white paper.  We would like convey our appreciation to Scott Wahl for his 
guidance and feedback during the formative stages of this effort.  We would also like to thank John 
Ashodian, John Hill, Ying Tat Leung, Juan Ma, Hari Padmanabhan, and John Paxton for carefully reading 
an earlier version of this white paper and providing several constructive suggestions and feedback, which 
have helped us greatly improve the quality of the white paper. 
 

 
17 
References 
1. Morten Brinch, Jan Stentoft, and Jesper K. Jensen, “Big Data and its Applications in Supply Chain 
Management: Findings from a Delphi Study,” Proceedings of the 50th Hawaii International Conference 
on System Sciences, 2017: 1351-1360.  
2. IBM Corporation, “The Path to Data Veracity,” IBM Big Data and Analytics Hub, May 2018, 
https://www.ibmbigdatahub.com/whitepaper/path-data-veracity 
3. DataStax Corporation, “Big Data: Beyond the Hype,” October 2013, 
https://www.datastax.com/resources/whitepapers/bigdata 
4. Phillip Russom, “Big Data Analytics,” TDWI Research, 2011, 
https://tdwi.org/research/2011/09/best-practices-report-q4-big-data-
analytics.aspx?tc=page0&m=1  
5. DXC Technology Company, “Five Industries Where Big Data is Making a Difference,” November 
2015, https://assets1.dxc.technology/analytics/downloads/DXC-Analytics-
Five_Industries_Where_Big_Data_is_Making_a_Difference-4AA5-6292ENW.pdf 
6. Nada Elgendy and Ahmed Elragal, “Big Data Analytics: A Literature Review Paper,” In: Perner P. 
(eds) Advances in Data Mining. Applications and Theoretical Aspects. ICDM 2014. Lecture Notes in 
Computer Science, vol 8557, Springer, Cham., 2014, https://link.springer.com/chapter/10.1007/978-
3-319-08976-8_16  
7. Hugh J. Watson, "Tutorial: Big Data Analytics: Concepts, Technologies, and Applications," 
Communications of the Association for Information Systems, 34 (2014), Article 65. 
http://aisel.aisnet.org/cais/vol34/iss1/65  
8. Richard L. Villars, Carl W. Olofson, and Matthew Eastwood, “Big Data: What It Is and Why You 
Should Care,” International Data Corporation, 2011. 
http://www.tracemyflows.com/uploads/big_data/idc_amd_big_data_whitepaper.pdf 
9. Sunil Tiwari, H.M. Wee, and Yosef Daryanto, “Big Data Analytics in Supply Chain Management 
Between 2010 and 2016: Insights to Industries,” Computers and Industrial Engineering, 115 (2017): 319-
330.  
10. Bob Trebilcock, “Supply Chain, Data Analytics, and Big Data,” Logistics Management, August 2015.  
https://www.logisticsmgmt.com/article/supply_chain_data_analytics_and_big_data  
11. Kaushik Pal, “How Machine Learning Can Improve Supply Chain Efficiency,” Techopedia, February-
2018.  https://www.techopedia.com/2/31846/trends/big-data/how-machine-learning-can-improve-
supply-chain-efficiency  
12.  McKinsey & Company, “Big Data and the Supply Chain: The Big-Supply-Chain Analytics 
Landscape: Part 1,” February 2016,  https://www.mckinsey.com/business-functions/operations/our-
insights/big-data-and-the-supply-chain-the-big-supply-chain-analytics-landscape-part-1#  
13. Lorenzo Romano, “Big Data Analytics: A Key Ingredient for Agility in Manufacturing,” May 2019, 
https://www.orange-business.com/en/blogs/big-data-analytics-key-ingredient-agility-manufacturing  

 
18 
14. Joe McKendrick, “Walmart’s Gigantic Private Cloud for Real-Time Inventory Control,” RT 
Insights.com, January 2017. https://www.rtinsights.com/walmart-cloud-inventory-management-
real-time-data/  
15. RT Insights team, “Levi’s Real-Time Tracking of Jeans: RFID in Retail,” RT Insights.com, April 
2016. https://www.rtinsights.com/rfid-in-retail-customer-experience-levis/ 
16.  JDA, “Store Replenishment at Morrisons,” 2017, https://jda.com/knowledge-center/collateral/by-
morrisons-case-study  
17.  Hans W. Ittmann, “The Impact of Big Data and Business Analytics on Supply Chain Management,” 
Journal of Transport and Supply Chain Management, 9, no. 1 (2015). 
https://jtscm.co.za/index.php/jtscm/article/view/165/331  
18. Logivation, https://www.logivations.com/en/solutions/plan/design_efficiency.php  
19. RT Insights team, “Using Mobile Device for a Real-Time Warehouse,” 2016, 
https://www.rtinsights.com/zebra-omnii-xt15-datek-real-time-warehouse/  
 20. Motifworks, “How Big Data Analytics Can Benefit Supply Chain and Logistics Industry,” 2017. 
https://motifworks.com/2017/02/23/how-big-data-analytics-can-benefit-supply-chain-logistics-
industry/  
 21. “2017 Third-Party Logistics Study,” https://jda.com/-/media/jda/knowledge-center/thought-
leadership/2017stateoflogisticsreport_new.ashx  
 22. UPS, “ORION Backgrounder,” 2019, 
https://www.pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=Factsheet
s&id=1426321616277-282  
23. “Data-Driven Logistics: The Growing Use of Predictive Analytics,” July 2018, https://www.smith-
howard.com/data-driven-logistics-the-growing-use-of-predictive-analytics/ 
 24. Martin Jeske, Moritz Grüner, and Frank Weiẞ, “Big Data in Logistics – A DHL Perspective on How 
to Move Beyond the Hype,” December 2013. 
http://www.dhl.com/content/dam/downloads/g0/about_us/innovation/CSI_Studie_BIG_DATA.pdf  
25.  McKinsey & Company, “Big Data, Analytics, and the Future of Marketing and Sales,” March 2015, 
https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/Marketing%20and%20Sales/Our
%20Insights/EBook%20Big%20data%20analytics%20and%20the%20future%20of%20marketing%20sal
es/Big-Data-eBook.ashx 
26. Gurobi Optimization, “The Power of Analytics,” accessed September 8, 2019. 
http://www.gurobi.com/resources/prescriptive-analytics,  
27.  Transmetrics, “ Big Data and Big Roadblocks:  How the Logistics Industry can Overcome its Big 
Data Challenges,” March 2018, https://www.youredi.com/blog/logistics-industry-can-overcome-big-data-
challenges 
28. Andrew McAfee, “What Every CEO Needs to Know About the Cloud,” Harvard Business Review, 
Nov. 2011: 124-132. 

 
19 
29. Bharat Bhargava, Rohit Ranchal, and Lotfi Ben Othmane, “Secure Information Sharing in Digital 
Supply Chains,” 3rd IEEE International Advanced Computing Conference, May 2013, 
https://www.cs.purdue.edu/homes/bb/Bhargava-Supply_Chain-Feb2013-india.pdf  
30. Megan Ray Nicholas, “How to Share Data Safely Across your Supply Chain,” 
https://www.smartdatacollective.com/share-data-safely-across-supply-chain/  
31. APQC, “APQC Quick Poll:  The Current State of Big Data & Advanced Analytics in Supply Chain,” 
May 2019, 
https://www.scmr.com/article/apqc_quick_poll_the_current_state_of_big_data_advanced_analytics_in_su
pply  
32. David Meer, “A Call to Action on Big Data,” Forbes, October 2014, 
https://www.forbes.com/sites/strategyand/2014/10/28/a-call-to-action-on-big-data/#6a4b6c22314  
 
 

