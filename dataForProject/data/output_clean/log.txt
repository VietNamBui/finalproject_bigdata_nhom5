
Removed lines from 2_iis_2015_81-90.pdf:
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
81 
BIG DATA ANALYTICS 
Jasmine Zakir, Minot State University,	  jasminezakir@outlook.com 
Tom Seymour, Minot State University,	  tom.seymour@minotstateu.edu 
Kristi Berg, Minot State University,	  kristi.berg@minostatateu.edu 
ABSTRACT 
Today Big Data draws a lot of attention in the IT world. The rapid rise of the Internet and the digital economy has 
fuelled an exponential growth in demand for data storage and analytics, and IT department are facing tremendous 
challenge in protecting and analyzing these increased volumes of information. The reason organizations are 
collecting and storing more data than ever before is because their business depends on it. The type of information 
being created is no more traditional database-driven data referred to as structured data rather it is data that 
include documents, images, audio, video, and social media contents known as unstructured data or Big Data. Big 
Data Analytics is a way of extracting value from these huge volumes of information, and it drives new market 
opportunities and maximizes customer retention. This paper primarily focuses on discussing the various 
technologies that work together as a Big Data Analytics system that can help predict future volumes, gain insights, 
take proactive actions, and give way to better strategic decision-making. Further this paper analyzes the adoption, 
usage and impact of big data analytics to the business value of an enterprise to improve its competitive advantage 
using a set of data algorithms for large data sets such as Hadoop and MapReduce.  
Keywords: Big Data, Analytics, Hadoop, MapReduce 
Big Data is an important concept, which is applied to data, which does not conform to the normal structure of the 
traditional database. Big Data consists of different types of key technologies like Hadoop, HDFS, NoSQL, 
MapReduce, MongoDB, Cassandra, PIG, HIVE, and HBASE that work together to achieve the end goal like 
extracting value from data that would be previously considered dead. According to a recent market report published 
by Transparency Market Research, the total value of big data was estimated at $6.3 billion as of 2012, but by 2018, 
it’s expected to reach the staggering level of $48.3 billion that’s almost a 700 percent increase [29]. Forrester 
Research estimates that organizations effectively utilize less than 5 percent of their available data. This is because 
the rest is simply too expensive to deal with. Big Data is derived from multiple sources. It involves not just 
traditional relational data, but all paradigms of unstructured data sources that are growing at a significant rate. For 
instance, machine-derived data multiplies quickly and contains rich, diverse content that needs to be discovered. 
Another example, human-derived data from social media is more textual, but the valuable insights are often 
overloaded with many possible meanings.  
Big Data Analytics reflect the challenges of data that are too vast, too unstructured, and too fast moving to be 
managed by traditional methods. From businesses and research institutions to governments, organizations now 
routinely generate data of unprecedented scope and complexity. Gleaning meaningful information and competitive 
advantages from massive amounts of data has become increasingly important to organizations globally. Trying to 
efficiently extract the meaningful insights from such data sources quickly and easily is challenging. Thus, analytics 
increase their market share. The tools available to handle the volume, velocity, and variety of big data have 
improved greatly in recent years. In general, these technologies are not prohibitively expensive, and much of the 
software is open source. Hadoop, the most commonly used framework, combines commodity hardware with open-
source software. It takes incoming streams of data and distributes them onto cheap disks; it also provides tools for 
analyzing the data. However, these technologies do require a skill set that is new to most IT departments, which will 
need to work hard to integrate all the relevant internal and external sources of data. Although attention to technology 
isn’t sufficient, it is always a necessary component of a big data strategy. This paper discusses some of the most 
commonly used big data technologies mostly open source that work together as a big data analytics system for 
leveraging large quantities of unstructured data to make more informed decisions.  
https://doi.org/10.48009/2_iis_2015_81-90
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
82 
Big Data is a data analysis methodology enabled by recent advances in technologies that support high-velocity data 
capture, storage and analysis. Data sources extend beyond the traditional corporate database to include emails, 
mobile device outputs, and sensor-generated data where data is no longer restricted to structured database records 
but rather unstructured data having no standard formatting [30]. Since Big Data and Analytics is a relatively new 
and evolving phrase, there is no uniform definition; various stakeholders have provided diverse and sometimes 
contradictory definitions. One of the first widely quoted definitions of Big Data resulted from the Gartner report of 
2001. Gartner proposed that, Big Data is defined by three V’s volume, velocity, and variety. Gartner expanded its 
definition in 2012 to include veracity, representing requirements about trust and uncertainty pertaining to data and 
the outcome of data analysis. In a 2012 report, IDC defined the 4th V as value—highlighting that Big Data 
applications need to bring incremental value to businesses. Big Data Analytics is all about processing unstructured 
information from call logs, mobile-banking transactions, online user generated content such as blog posts and 
tweets, online searches, and images which can be transformed into valuable business information using 
computational techniques to unveil trends and patterns between datasets. 
Another dimension of the Big Data definition involves technology. Big Data is not only large and complex, but it 
requires innovative technology to analyze and process. In 2013, the National Institute of Standard and Technology 
(NIST) Big Data workgroup proposed the following definition of Big Data that emphasizes application of new 
technology; Big Data exceed the capacity or capability of current or conventional methods and systems, and enable 
novel approaches to frontier questions previously inaccessible or impractical using current or conventional methods. 
Business challenges rarely show up in the appearance of a perfect data problem, and even when data are abundant, 
practitioners have difficulties to incorporate it into their complex decision-making that adds business value. In 2012, 
McKinsey & Company conducted a survey of 1,469 executives across various regions, industries and company 
sizes, in which 49 percent of respondents said that their companies are focusing big data efforts on customer 
insights, segmentation and targeting to improve overall performance [10] An even higher number of respondents 60 
percent said their companies should focus efforts on using data and analytics to generate these insights. Yet, just 
one-fifth said that their organizations have fully deployed data and analytics to generate insights in one business unit 
or function, and only 13 percent use data to generate insights across the company. As these survey results show, the 
question is no longer whether big data can help business, but how can business derive maximum results from big 
data. 
Predictive Analytics is the use of historical data to forecast on consumer behavior and trends [18]. It is the use of 
past/historical data to predict future trends. This analysis makes use of the statistical models and machine learning 
algorithms to identify patterns and learn from historical data [25]. Predictive Analysis can also be defined as a 
process that uses machine learning to analyze data and make predictions [22].  
future, and 68% sight competitive advantage as the prime benefit of predictive analysis [17]. Broadly speaking, 
predictive analysis can be applied in ecommerce for product recommendation, price management, and predictive 
search. Typically a large e-commerce site offers thousands of product and services for sale. Navigating and 
searching for a product out of thousands on a website could be a major setback to consumers. However, with the 
invention of recommender system, an E-Commerce site/application can quickly identify/predict products that 
closely suit the consumer’s taste [24].  
Using a technology called Collaborative Filtering a database of historical user preferences is created. When a new 
customer access the ecommerce site, the customer is matched with the database of preferences, in order to discover a 
preference class that closely matches the customer taste. These products are then recommended to the customer [24]. 
Another technology that is used in ecommerce is the clustering algorithm. Clustering algorithm works by identifying 
groups of users that have similar preferences. These users are then clustered into a single group and are given a 
unique identifier.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
83 
New customers cluster are predicted by calculating the average similarities of the individual members in that cluster. 
Hence a user could be a partial member of more than one cluster depending of the weight of the user’s average 
opinion [24]. Advanced analytics is defined as the scientific process of transforming data into insight for making 
better decisions. As a formal discipline, advanced analytics have grown under the Operational Research domain. 
There are some fields that have considerable overlap with analytics, and also different accepted classifications for 
the types of analytics [2].  
Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large 
amounts of log data from many different sources to a centralized data store. Flume deploys as one or more agents, 
each contained within its own instance of the Java Virtual Machine (JVM). Agents consist of three pluggable 
components: sources, sinks, and channels. Flume agents ingest incoming streaming data from one or more sources. 
Data ingested by a Flume agent is passed to a sink, which is most commonly a distributed file system like Hadoop. 
agent to be the sink of another. Flume sources listen and consume events. Events can range from newline-terminated 
strings in stdout to HTTP POSTs and RPC calls — it all depends on what sources the agent is configured to use. 
Flume agents may have more than one source, but at the minimum they require one. Sources require a name and a 
type; the type then dictates additional configuration parameters. 
Channels are the mechanism by which Flume agents transfer events from their sources to their sinks. Events written 
to the channel by a source are not removed from the channel until a sink removes that event in a transaction. This 
allows Flume sinks to retry writes in the event of a failure in the external repository (such as HDFS or an outgoing 
network connection). For example, if the network between a Flume agent and a Hadoop cluster goes down, the 
channel. Sink is an interface implementation that can remove events from a channel and transmit them to the next 
agent in the flow, or to the event’s final destination and also sinks can remove events from the channel in 
transactions and write them to output. Transactions close when the event is successfully written, ensuring that all 
events are committed to their final destination.  
Apache Sqoop is a CLI tool designed to transfer data between Hadoop and relational databases. Sqoop can import 
been transformed using MapReduce. Sqoop also has the ability to import data into HBase and Hive. Sqoop connects 
imported. Both import and export utilize MapReduce, which provides parallel operation as well as fault tolerance. 
During import, Sqoop reads the table, row by row, into HDFS. Because import is performed in parallel, the output in 
HDFS is multiple files.  
Apache’s Pig is a major project, which is lying on top of Hadoop, and provides higher-level language to use 
Hadoop’s MapReduce library. Pig provides the scripting language to describe operations like the reading, filtering 
and transforming, joining, and writing data which are exactly the same operations that MapReduce was originally 
designed for. Instead of expressing these operations in thousands of lines of Java code which uses MapReduce 
directly, Apache Pig lets the users express them in a language that is not unlike a bash or Perl script.  
Pig was initially developed at Yahoo Research around 2006 but moved into the Apache Software Foundation in 
2007. Unlike SQL, Pig does not require that the data must have a schema, so it is well suited to process the 
unstructured data. But, Pig can still leverage the value of a schema if you want to supply one. PigLatin is relationally 
complete like SQL, which means it is at least as powerful as a relational algebra. Turing completeness requires 
conditional constructs, an infinite memory model, and looping constructs.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
84 
SQL for querying. Being a SQL dialect, HIVEQL is a declarative language. In PigLatin, you specify the data flow, 
but in Hive we describe the result we want and hive figures out how to build a data flow to achieve that result. 
Unlike Pig, in Hive a schema is required, but you are not limited to only one schema. Like PigLatin and SQL, 
HiveQL itself is a relationally complete language but it is not a Turing complete language.  
Apache Zoo Keeper is an effort to develop and maintain an open-source server, which enables highly reliable 
distributed coordination. It provides a distributed configuration service, a synchronization service and a naming 
registry for distributed systems. Distributed applications use ZooKeeper to store and mediate updates to import 
configuration information. ZooKeeper is especially fast with workloads where reads to the data are more common 
than writes. The ideal read/write ratio is about 10:1. ZooKeeper is replicated over a set of hosts (called an ensemble) 
and the servers are aware of each other and there is no single point of failure.   
Figure 1. Intel Manager for Hadoop [3] 
MongoDB is an open source, document-oriented NoSQL database that has lately attained some space in the data 
industry. It is considered as one of the most popular NoSQL databases, competing today and favors master-slave 
replication. The role of master is to perform reads and writes whereas the slave confines to copy the data received 
from master, to perform the read operation, and backup the data. The slaves do not participate in write operations 
but may select an alternate master in case of the current master failure. MongoDB uses binary format of JSON-like 
documents underneath and believes in dynamic schemas, unlike the traditional relational databases. The query 
system of MongoDB can return particular fields and query set compass search by fields, range queries, regular 
expression search, etc. and may include the user-defined complex JavaScript functions. As hinted already, 
MongoDB practice flexible schema and the document structure in a grouping, called Collection, may vary and 
common fields of various documents in a collection can have disparate types of the data. 
The MongoDB is equipped with the suitable drivers for most of the programming languages, which are used to 
develop the customized systems that use MongoDB as their backend player. There is an increasingly demand of 
using MongoDB as pure in-memory database; in such cases, the application dataset will always be small. Though, it 
is probably are easy for maintenance and can make a database developer happier; this can be a bottle neck for 
complex applications that require tremendous database management capabilities. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
85 
In order to efficiently address the challenges of Big Data, the leading vendor developed the Oracle NoSQL database. 
It was built by Oracle Berkeley DB team and the Berkeley DB Java Edition is the building block of Oracle NoSQL. 
Berkeley DB is a robust and scalable key-value store and used as the underlying storage for several popular data 
model such as Amazon Dynamo, GenieDB, MemcacheDB and Voldemort [28].  
scalability, throughput, and reliability with little tuning efforts. It is an efficient and a resilient transaction model that 
significantly eases the development process of applications, involving Big Data. It is a distributed, scalable yet 
simple key-value pair data model that fully supports the ACID transactions and JSON format and integrated with 
Oracle Database and Hadoop. It offers scalable throughput with bounded latency. The model very well 
accommodates the horizontal scaling with dynamic annexation of new capacity, citing high availability; the design 
architecture of Oracle NoSQL does not support single point of failure, and lucid load balancing. Actually, the goals 
of high availability, rapid failover in the event of a node failure, etc. are achieved by replicating the storage nodes. 
which is able to handle big data requirements. It is a highly scalable and high-performance distributed database 
management system that can handle real-time big data applications that drive key systems for modern and successful 
businesses. It has a built-for-scale architecture that can handle petabytes of information and thousands of concurrent 
users/operations per second as easily as it can manage much smaller amount of data and user traffic. It has a peer to 
peer design that offers no single point of failure for any database process or function, in addition to the location 
independence capabilities that equate to a true network-independent method of storing and accessing data, data can 
be read and written anywhere. Apache Cassandra is also equipped with flexible/dynamic schema design that 
accommodates all formats of big data applications, including structured, semi-structured, and unstructured data. 
online.  
clusters of computers. It is designed to scale up from single servers to thousands of machines, with each offering 
local computation and storage. The basic notion is to allow a single query to find and collect results from all the 
cluster members, and this model is clearly suitable for Google's model of search support. One of the largest 
technological challenges in software systems research today is to provide mechanisms for storage, manipulation, and 
information retrieval on large amount of data. Web services and social media produce together an impressive 
amount of data, reaching the scale of petabytes daily (Facebook, 2012). These data may contain valuable 
information, which sometimes is not properly explored by existing systems. Most of this data is stored in a non-
structured manner, using different languages and format, which, in many cases, are in compatible. 
large datasets. Over the last years, commodity hardware became part of clusters, since the x86 platform cope with 
the need of having an overall better cost/performance ratio, while decreasing maintenance cost. Apache Hadoop is a 
framework developed to take advantage of this approach, using such commodity clusters for storage, processing and 
manipulation of large amount of data. The framework was designed over the MapReduce paradigm and uses the 
HDFS as a storage file system. Hadoop presents key characteristics when performing parallel and distributed 
computing, such as data integrity, availability, scalability, exception handling, and failure recovery.   
Hadoop is a popular choice when you need to filter, sort, or pre-process large amounts of new data in place and 
distill it to generate denser data that theoretically contains more information. Pre-processing involves filtering new 
data sources to make them suitable for additional analysis in a data warehouse.  Hadoop is a top-level open source 
project of the Apache Software Foundation. Several suppliers, including Intel, offer their own commercial Hadoop 
distributions, packaging the basic software stack with other Hadoop software projects such as Apache Hive, Apache 
Pig, and Apache Sqoop. These distributions must integrate with data warehouses, databases, and other data 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
86 
process or query.	  
Figure 2. Data Architecture with Hadoop Integrated with existing data system [12] 
MapReduce is the original massively scalable, parallel processing framework commonly used with Hadoop and 
other components such as the Hadoop Distributed File System (HDFS) and YARN. YARN can be described as a 
large-scale, distributed operating system for big data implementations. As Hadoop has matured, the batch-oriented, 
disk-intensive MapReduce’ s limitations have become more apparent as Big Data analytics moves to more real-time, 
streaming processing and advanced implementations such as the aforementioned machine learning.  
MapReduce is the model of distributed data processing introduced by Google in 2004. The fundamental concept of 
MapReduce is to divide problems into two parts: a map function that processes source data into sufficient statistics 
and a reduce function that merges all sufficient statistics into a final answer. By definition, any number of 
concurrent map functions can be run at the same time without intercommunication. Once all the data has had the 
map function applied to it, the reduce function can be run to combine the results of the map phases.  For large scale 
batch processing and high speed data retrieval, common in Web search scenarios, MapReduce provides the fastest, 
most cost-effective and most scalable mechanism for returning results. Today, most of the leading technologies for 
managing "big data" are developed on MapReduce. With MapReduce there are few scalability limitations, but 
leveraging it directly does require writing and maintaining a lot of code. 
Splunk is a general-purpose search, analysis and reporting engine for time-series text data, typically machine data. 
Splunk software is deployed to address one or more core IT functions: application management, security, 
compliance, IT operations management and providing analytics for the business. The Splunk engine is optimized for 
quickly indexing and persisting unstructured data loaded into the system. Specifically, Splunk uses a minimal 
schema for persisted data – events consist only of the raw event text, implied timestamp, source (typically the 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
87 
filename for file based inputs), source type (an indication of the general type of data) and host (where the data 
originated).  
Once data enters the Splunk system, it quickly proceeds through processing, is persisted in its raw form and is 
indexed by the above fields along with all the keywords in the raw event text. Indexing is an essential element of the 
canonical “super-grep” use case for Splunk, but it also makes most retrieval tasks faster. Any more sophisticated 
processing on these raw events is deferred until search time. This serves four important goals: indexing speed is 
increased as minimal processing is performed, bringing new data into the system is a relatively low effort exercise as 
no schema planning is needed, the original data is persisted for easy inspection and the system is resilient to change 
as data parsing problems do not require reloading or re-indexing the data. 
Apache Spark an open source big data processing framework built around speed, ease of use, and sophisticated 
analytics. It was originally developed in 2009 in UC Berkeley’s AMP Lab, and open sourced in 2010 as an Apache 
project. Hadoop as a big data processing technology has been around for ten years and has proven to be the solution 
of choice for processing large data sets. MapReduce is a great solution for one-pass computations, but not very 
efficient for use cases that require multi-pass computations and algorithms. Each step in the data processing 
workflow has one Map phase and one Reduce phase and you'll need to convert any use case into MapReduce pattern 
to leverage this solution. Spark takes MapReduce to the next level with less expensive shuffles in the data 
processing. With capabilities like in-memory data storage and near real-time processing, the performance can be 
several times faster than other big data technologies.  
Spark also supports lazy evaluation of big data queries, which helps with optimization of the steps in data processing 
workflows. It provides a higher-level API to improve developer productivity and a consistent architect model for big 
data solutions. Spark holds intermediate results in memory rather than writing them to disk, which is very useful 
especially when you need to work on the same dataset multiple times. It’s designed to be an execution engine that 
works both in-memory and on-disk. Spark operators perform external operations when data does not fit in memory. 
Spark can be used for processing datasets that larger than the aggregate memory in a cluster. Spark will attempt to 
store as much as data in memory and then will spill to disk. It can store part of a data set in memory and the 
remaining data on the disk. You have to look at your data and use cases to assess the memory requirements. With 
this in-memory data storage, Spark comes with a great performance advantage. 
Spark is written in Scala Programing Language and runs on the Java Virtual machine. It currently supports 
programming languages like Scala, java, python, Clojure and R. Other than Spark Core API, there are additional 
libraries that are part of the Spark ecosystem and provide additional capabilities in Big Data analytics. Spark 
Streaming is one among the spark library that can be used for processing the real-time streaming data. This is based 
on micro based on micro batch style of computing and processing. Spark SQL provides the capabilities to expose the 
visualization tools. MLlib, GraphX are some other libraries from spark. 
Thomas H. Davenport was perhaps the first to observe in his Harvard Business Review article published in January 
2006 (“Competing on Analytics”) how companies who orientated themselves around fact based management 
approach and compete on their analytical abilities considerably out-performed their peers in the marketplace. The 
reality is that it takes continuous improvement to become an analytics-driven organization. In a presentation given at 
the Strata New York conference in September 2011, McKinsey & Company showed the eye opening; 10-year 
category growth rate differences (see Figure 7, below) between businesses that smartly use their big data and those 
that do not.  
Amazon uses Big Data to monitor, track and secure 1.5 billion items in its inventory that are laying around 200 
fulfillment centers around the world, and then relies on predictive analytics for its ‘anticipatory shipping’ to predict 
when a customer will purchase a product, and pre-ship it to a depot close to the final destination. Wal-Mart handles 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
88 
more than a million customer transactions each hour [23], imports information into databases to contain more than 
2.5 petabytes and asked their suppliers to tag shipments with radio frequency identification (RFID) systems [8] that 
can generate 100 to 1000 times the data of conventional bar code systems. UPS deployment of telematics in their 
freight segment helped in their global redesign of logistical networks [6]. Amazon is a big data giant and the largest 
online retail store. The company pioneered e-commerce in many different ways, but one of its biggest successes was 
the personalized recommendation system, which was built from the big data it gathers from its millions of 
customers’ transactions. 
The U.S. federal government collects more than 370,000 raw and geospatial datasets from 172 agencies and sub 
agencies.  It leverages that data to provide a portal to 230 citizen-developed apps, with the aim of increasing public 
access to information not deemed private or classified. Professional social network LinkedIn uses data from its more 
than 100 million users to build new social products based on users’ own definitions of their skill sets. Silver Spring 
Networks deploys smart, two-way power grids for its utility customers that utilize digital technology to deliver more 
help manage energy use and maximize efficiency. Jeffrey Brenner and the Camden Coalition mapped a city’s crime 
trends to identify problems with its healthcare system, revealing services that were both medically ineffective and 
expensive. 
Today’s technology landscape is changing fast. Organizations of all shapes and sizes are being pressured to be data-
driven and to do more with less. Even though big data technologies are still in a nascent stage, relatively speaking, 
the impact of the 3V’s of big data, which now is 5v’s cannot be ignored. The time is now for organizations to begin 
planning for and building out their Hadoop-based data lake. Organizations with the right infrastructures, talent and 
vision in place are well equipped to take their big data strategies to the next level and transform their businesses. 
They can use big data to unveil new patterns and trends, gain additional insights and begin to find answers to 
pressing business issues. The deeper organizations dig into big data and the more equipped they are to act upon 
what’s learned, the more likely they are to reveal answers that can add value to the top line of the business. This is 
where the returns on big data investments multiply and the transformation begins. Harnessing big data insight 
delivers more than cost cutting or productivity improvement but it definitely reveals new business opportunities. 
Data-driven decisions always tend to be better decisions. 
1. Apache Software Foundation. (2010). Apache ZooKeeper. Retrieved April 5, 2015 from 
https://zookeeper.apache.org 
2. Chae, B., Sheu, C., Yang, C. and Olson, D. (2014). The impact of advanced analytics and data accuracy on 
operational performance: A contingent resource based theory (RBT) perspective, Decision Support Systems, 59, 
119-126. 
3. Chambers, C., Raniwala, A., Adams, S., Henry, R., Bradshaw, R., and Weizenbaum, N. (2010). Flume Java: 
Easy, Efficient Data-Parallel Pipelines. Google, Inc. Retrieved April 1, 2015 from 
http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf 
4. Cisco Systems. Cisco UCS Common Platform Architecture Version 2 (CPA v2) for Big Data with 
Comprehensive Data Protection using Intel Distribution for Apache Hadoop. Retrieved March 15, 2015, from 
http://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/Cisco_UCS_CPA_for_Big_Data_wi
th_Intel.html 
5. DATASTAX Corporation. (2013, October). Big Data: Beyond the Hype - Why Big data Matters to you [White 
paper]. Retrieved March 15, 2015 from https://www.datastax.com/wp-content/uploads/2011/10/WP-DataStax-
BigData.pdf 
6. Davenport, T & Patil, D. (2012). Data Scientist: The Sexiest Job of the 21st Century. Harvard Business Review, 
90, 70-76. 
7. Dhawan, S & Rathee, S. (2013). Big Data Analytics using Hadoop Components like Pig and Hive. American 
International Journal of Research in Science, Technology, Engineering & Mathematics, 88, 13-131. Retrieved 
from http://iasir.net/AIJRSTEMpapers/AIJRSTEM13-131.pdf 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
89 
8. Edwards, P., Peters, M. and Sharman, G. (2001). The Effectiveness of Information Systems in Supporting the 
Extended Supply Chain, Journal of Business Logistics 22 (1), 1-27 
9. EMC Corporation. (2013, January). EMC Accelerates Journey to Big Data with Business Analytics-as-a-
Service [White paper]. Retrieved from http://www.emc.com/collateral/white-papers/h11259-emc-accelerates-
journey-big-data-ba-wp.pdf 
10. EMC Corporation. Big Data, Big Transformations [White paper]. Retrieved from 
http://www.emc.com/collateral/white-papers/idg-bigdata-umbrella-wp.pdf 
11. EMC Solutions Group. (2012, July). Big Data-as-a-Service [White paper]. Retrieved from 
https://www.emc.com/collateral/software/white-papers/h10839-big-data-as-a-service-perspt.pdf 
12. Enterprise Hadoop: The Ecosystem of Projects. Retrieved from http://hortonworks.com/hadoop/ 
13. George, L. (2014, September). Getting Started with Big Data Architecture. Retrieved April 5, 2015, from 
http://blog.cloudera.com/blog/2014/09/getting-started-with-big-data-architecture/ 
14. IBM Corporation. IBM Big Data Platform. Retrieved from http://www-
01.ibm.com/software/in/data/bigdata/enterprise.html 
15. Intel Corporation. Big Data Analytics - Extract, Transform, and Load Big data with Apache Hadoop [White 
paper]. Retrieved April 3, 2015 from https://software.intel.com/sites/default/files/article/402274/etl-big-data-
with-hadoop.pdf 
16. McClary, D. (2013, June). Acquiring Big Data Using Apache Flume. Retrieved March 3, 2015 from 
http://www.drdobbs.com/database/acquiring-big-data-using-apache-flume/240155029 
17. Millard, S. (2013). Big Data Brewing Value in Human Capital Management – Ventana Research. Retrieved 
April 2, 2015 from http://stephanmillard.ventanaresearch.com/2013/08/28/big-data-brewing-value-in-human-
capital-management 
18. Mosavi, A. and Vaezipour, A. (2013). Developing Effective Tools for Predictive Analytics and Informed 
Decisions. Technical Report. University of Tallinn.  
19. Oracle Corporation. (2013, March). Big Data Analytics - Advanced Analytics in Oracle Database [White 
paper]. Retrieved March 5, 2015 from http://www.oracle.com/technetwork/database/options/advanced-
analytics/advanced-analytics-wp-12c-1896138.pdf?ssSourceSiteId=ocomen 
20. Oracle Enterprise Architecture. (2015, April). An Enterprise Architect's Guide to Big Data - Reference 
Architecture Overview [White paper]. Retrieved from 
http://www.oracle.com/technetwork/topics/entarch/articles/oea-big-data-guide-1522052.pdf 
21. Penchikala, S. (2015, January). Big Data Processing with Apache Spark - Part 1: Introduction. Retrieved from 
http://www.infoq.com/articles/apache-spark-introduction 
22. Puri, R. (2013). How Online Retailers Use Predictive Analytics To Improve Your Shopping Experience. 
Retrieved April5, 2015 from http://blogs.sap.com/innovation/analytics/how-online-retailers-use-predictive-
analytics-to-improve-your-shopping-experience-0108060 
23. Sanders, N.R. (2014). Big Data Driven Supply Chain Management: A Framework for Implementing Analytics 
and Tuning Information into Intelligence, 1st Edition, Pearson, NJ 
24. Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. (2002). Recommendation systems for large e-commerce: 
Scalable neighborhood formation using clustering. In Proceedings of the fifth international conference on 
computer and information technology, 1.  
25. Shmueli, G. & Koppius, O. (2011). Predictive Analytics in Information Systems Research. MIS Quarterly, 
35(3), pp. 553-72. 
26. Sorkin, S. (2011). Splunk Technical Paper: Large-Scale, Unstructured Data Retrieval and Analysis Using 
Splunk. Retrieved April 15, 2015 from https://www.splunk.com/content/dam/splunk2/pdfs/technical-
briefs/splunk-and-mapreduce.pdf 
27. The Bloor Group. IBM and the Big Data Information Architecture. Retrieved April 3, 2015 from 
http://insideanalysis.com/wp-content/uploads/2014/08/BDIAVendor-IBMv01.pdf 
28. Tiwari, S. (2011). Using Oracle Berkeley DB as a NoSQL Data Store. Retrieved April 5 2015 from 
http://www.oracle.com/technetwork/articles/cloudcomp/berkeleydb-nosql-323570.htm 
29. Transparency Market Report. (May, 2015).Big Data Applications in Healthcare likely to Propel Market to 
US$48.3 Bn by 2018. Retrieved June 26, 2015, from 
http://www.transparencymarketresearch.com/pressrelease/big-data-market.htm 
30. Villars, R. L., Olofson, C. W., & Eastwood, M. (2011, June). Big data: What it is and why you should care. IDC 
White Paper. Framingham, MA: IDC. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
90 
31. Wolpe, T. (2015, March). How Facebook is speeding up the Pesto SQL query engine. Retrieved April 3, 2015, 
from http://www.zdnet.com/article/how-facebook-is-speeding-up-the-presto-sql-query-engine 
32. Zahari et al. (2010). Spark: Cluster Computing with Working Sets. Retrieved April 7, 2015, from 
http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf 

Removed lines from 48077-157-151840-1-10-20200520.pdf:
THÔNG TIN VÀ TƯ LIỆU - 2/2020 23
BIG DATA VÀ XU HƯỚNG ỨNG DỤNG TRONG HOẠT ĐỘNG THÔNG TIN - THƯ VIỆN
ThS Nguyễn Lê Phương Hoài
Viện Thông tin Khoa học xã hội 
● Tóm tắt: Big Data là một thuật ngữ được sử dụng để chỉ những bộ dữ liệu khổng lồ, chủ yếu không 
có cấu trúc, được thu thập từ nhiều nguồn khác nhau. Big Data có nhiều tác động, ứng dụng và được 
xem như một yếu tố quyết định đến việc phát triển, mang lại lợi thế cạnh tranh cho tổ chức. Bài viết 
tổng quan lược sử các quan điểm về Big Data, đồng thời nhấn mạnh vào các xu hướng ứng dụng 
trong hoạt động thông tin - thư viện.
● Từ khóa: Big Data; dữ liệu lớn; hoạt động thư viện.
BIG DATA APPLICATION IN LIBRARY AND INFORMATION ACTIVITIES
● Abstract: Big Data is a term used to refer to huge, mostly unstructured datasets, collected from 
a variety of sources. Big Data has many impacts, applications and is considered as a decisive 
factor in the development, bringing competitive advantages to the organization. The overview paper 
summarizes the views on Big Data and emphasizes application trends in library and information 
activities.
● Keywords: Big Data; library activities.
1. LƯỢC SỬ CÁC QUAN ĐIỂM VỀ BIG DATA
Hiện nay, chưa có một định nghĩa chính 
xác cho thuật ngữ Big Data. Big được ghi 
nhận lần đầu tiên trong báo cáo “Application-
controlled demand paging for out-of-core 
visualization” của Michael Cox và David 
thứ 8 (vào tháng 10 năm 1997). Ý tưởng đưa 
xử lý không còn đủ số lượng cần khảo sát, do 
thể phân tích được tất cả các thông tin [11].
Tháng 8 năm 1999, Steve Bryson, David 
Kenwright, Michael Cox, David Ellsworth, và 
Robert Haimes đăng bài “Visually exploring 
gigabyte data sets in real time” trên Tạp chí 
Communications of the ACM. Đây là bài viết 
đầu tiên sử dụng thuật ngữ “Big Data”. Các 
tác giả nhận định: “Những chiếc máy tính 
vực, cũng có thể là bất lợi; tính toán nhanh 
chóng tạo ra một lượng lớn dữ liệu. Nếu trước 
là lớn, thì bây giờ chúng ta có thể tìm thấy 
300 GB” [15]. 
Tháng 11 năm 2000, Francis X. Diebold 
Hiệp hội Kinh tế lượng bài viết “Big Data 
Measurement and Forecasting”. Trong bài 
viết này, tác giả khẳng định: “Gần đây, nhiều 
ngành khoa học như vật lý, sinh học, khoa 
học xã hội, vốn đang buộc phải đương đầu với 
khó khăn - đã thu được lợi từ hiện tượng Big 
Data và đã gặt hái được nhiều thành công. Big 
Data chỉ sự bùng nổ về số lượng (và đôi khi, 
chất lượng), khả năng liên kết cũng như độ 
sẵn sàng của dữ liệu, chủ yếu là kết quả của 
việc ghi lại dữ liệu và công nghệ lưu trữ” [4].
Tháng 2 năm 2001, Doug Laney - nhà 
phân tích của Tập đoàn Meta, công bố nghiên 
cứu “3D Data Managment: Controlling Data 
Volume, Velocity, and Variety”. Laney cho 
rằng, những thách thức và cơ hội nằm trong 
bằng mô hình “3Vs”: tăng về số lượng lưu trữ 
(Volume), tăng về tốc độ xử lý (Velocity) và 
tăng về chủng loại (Variety) [3]. Một thập kỷ 
sau, mô hình “3Vs” đã trở thành thuật ngữ 
dữ liệu lớn ba chiều. Nhiều công ty và tổ chức 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
24 THÔNG TIN VÀ TƯ LIỆU - 1/2020
24
dụng mô hình “3Vs” này để định nghĩa Big 
Data.
Tháng 12 năm 2008, Randal E. Bryant, 
Randy H. Katz, và Edward D. Lazowska 
công bố bài viết “Big-Data Computing: 
Commerce, Science and Society”, trong đó 
miêu tả: “Cũng như công cụ tìm kiếm đã làm 
thay đổi cách chúng ta tiếp cận thông tin, các 
ty, các nhà nghiên cứu khoa học, các học 
viên y tế, quốc phòng và tình báo,... Sử dụng 
nghệ máy tính suốt một thập kỷ qua. Chúng 
nó trong việc thu thập, sắp xếp và xử lý dữ 
liệu của tất cả các tầng lớp xã hội. Một khoản 
sẽ thúc đẩy phát triển và mở rộng nó” [13].
Tháng 2 năm 2010, Kenneth Cukier đăng 
“Data, data everywhere”. Cukier viết: “... thế 
mức không tưởng, và càng ngày càng được 
nhân rộng với tốc độ nhanh hơn bao giờ hết... 
Hiệu quả đã được thể hiện ở khắp mọi nơi, từ 
kinh doanh đến khoa học, từ chính phủ đến 
nghệ thuật. Các nhà khoa học và kỹ sư máy 
tượng này: Big Data” [8].
Tháng 5 năm 2012, Danah Boyd và Kate 
bài “Critical Question for Big Data” trên tờ 
Information, Communications and Society. 
Các tác giả định nghĩa Big Data như là “một 
hiện tượng văn hóa, công nghệ và học thuật 
dựa trên sự tương tác của: 1) Công nghệ tối 
thuật toán để thu thập, phân tích, liên kết, và 
so sánh các tập dữ liệu lớn; 2) Phân tích: tạo 
tuyên bố kinh tế, xã hội, kỹ thuật và pháp lý; 3) 
Thần thoại: niềm tin phổ biến rằng dữ liệu lớn 
biết mà trước đây không thể, với hào quang 
của sự thật, khách quan, chính xác” [2].
Sau đó, Gartner - công ty nghiên cứu và 
tư vấn công nghệ thông tin - bổ sung thêm 
rằng “Big Data ngoài 3 tính chất (số lượng, 
tốc độ xử lý và chủng loại) thì còn phải cần 
khám phá sâu vào sự vật/sự việc và tối ưu 
hóa các quy trình làm việc” [5]. Cùng quan 
điểm đó, Tan Jee Toon cho rằng Big Data 
mọi thứ xung quanh chúng ta, từ các thiết bị 
kỹ thuật số như di động, video, hình ảnh, tin 
nhắn tới các thiết bị cảm biến, các máy móc 
hội. Big Data có đặc điểm là được sinh ra với 
khối lượng (volume), tốc độ (velocity), độ đa 
dạng (variety) và tính xác thực (veracity) rất 
lớn [16].
Năm 2014, Gartner đưa ra khái niệm 
mới về Big Data bằng mô hình “5Vs”, gồm: 
Volume (khối lượng), Velocity (tốc độ), 
Variety (tính đa dạng), Veracity (tính xác 
thực) và Value (giá trị). Trong đó: Volume là 
khối lượng Big Data được tạo ra mỗi ngày. 
phân tán, nơi mà dữ liệu chỉ được lưu trữ một 
bởi phần mềm. Velocity là tốc độ dữ liệu mới 
được tạo ra và tốc độ dữ liệu chuyển động. 
giữ chúng trong các cơ sở dữ liệu. Variety là 
các kiểu khác nhau của dữ liệu. Công nghệ 
có cấu trúc truyền thống (được lưu trữ trong 
các bảng hoặc các cơ sở dữ liệu quan hệ) và 
phi cấu trúc (bao gồm các thông điệp, trao 
đổi của mạng xã hội, các hình ảnh, dữ liệu 
cảm biến, video, tiếng nói...). Veracity là tính 
hỗn độn hoặc tính tin cậy của dữ liệu. Công 
kiểm soát những loại dữ liệu này. Value là giá 
trị của dữ liệu. Việc tiếp cận Big Data sẽ chỉ 
thành những thứ có giá trị. Đây là khái niệm 
đầy đủ về 5 tính chất của Big Data [5].
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 25
THÔNG TIN VÀ TƯ LIỆU - 1/2020 25
2. XU HƯỚNG ỨNG DỤNG BIG DATA TRONG HOẠT 
ĐỘNG THÔNG TIN - THƯ VIỆN
Ngày nay, một lượng lớn dữ liệu số có thể 
các mạng xã hội. Theo Howe D. (2008): “Chỉ 
riêng trong các lĩnh vực nghiên cứu khoa học, 
trưởng theo cấp số nhân” [7]. Số lượng dữ 
nhiều lĩnh vực khác nhau và dữ liệu lớn (Big 
Data) được sử dụng rộng rãi trong các lĩnh 
vực, tổ chức với nhiều mục đích khác nhau. 
hành vi tiêu dùng của khách hàng, để đề xuất 
trên thông tin thu thập được (Ebay, Facebook, 
Google...). Các cơ sở nghiên cứu khoa học sử 
khoa học mới, ví dụ như xây dựng bản đồ 
gene của con người,... Việc sử dụng Big Data 
trong hoạt động thông tin - thư viện đã bắt 
đầu được quan tâm nghiên cứu. Campbell 
D. Grant, Cowan Scott R. (2016) phân tích 
và dữ liệu liên kết [1]. Kim Young Seok (2017) 
khuôn mặt Chernoff [9]. Gerrard D., Mooney 
J. và Thompson D. (2017) xem xét kiến trúc 
phân tích dữ liệu, các bộ tài nguyên được bảo 
thời gian tới [6]. Waqar Ahmed và Kanwal 
Ameen (2017) tổng quan các khái niệm về 
thư viện [17]. Ye Chunlei (2017) nghiên cứu 
trong thư viện đại học [18]. Zhan Ming, Widén 
Gunilla (2018) nghiên cứu vai trò của thư 
viện công cộng trong thời đại Big Data [20]. 
Li Shuqing; Jiao Fusen; Zhang Yong; Xu Xia 
(2019) nghiên cứu các vấn đề và thay đổi của 
dịch vụ người dùng tin [10],... Các nghiên cứu 
trong thời đại Big Data. Bàn về xu hướng ứng 
tiếp nào, nhưng có thể tổng hợp thành các xu 
hướng chính như sau:
Một là, tổ chức lưu trữ, bảo quản dữ liệu
Marydee Ojala nhận định: “Các thư viện 
ngày nay. Bộ sưu tập các tài nguyên số được 
các thư viện. Khối lượng và tính đa dạng dữ 
thư viện phải có phương pháp tổ chức lưu trữ, 
bảo quản dữ liệu hợp lý” [12]. Nguồn dữ liệu 
thư viện bao gồm: nguồn dữ liệu mô tả tài liệu 
thư viện, nguồn tài nguyên số hóa tài liệu thư 
viện, nguồn tài liệu số thư viện bổ sung qua 
việc mua hay sử dụng chung, nguồn dữ liệu 
khảo sát thư viện, dữ liệu định tính, dữ liệu 
tương tác xã hội,... Trước đây, các thư viện 
băng, đặt trong các cơ sở lưu trữ. Trước tác 
mạng công nghiệp lần thứ tư, các thư viện 
chi phí hiệu quả. Dữ liệu được lưu trữ theo 
hai cách, cả trên các thiết bị ngoại tuyến (thẻ 
nhớ SD, ổ cứng ngoài, ổ đĩa flash) và lưu trữ 
trực tuyến trên đám mây. Với phương thức 
kết hợp sử dụng băng từ để bảo quản lưu trữ, 
được yêu cầu, và sử dụng lưu trữ đám mây 
cho các Big Data. Các thư viện hướng đến 
thư viện (bao gồm cả tài nguyên vật chất và 
dữ liệu), xác định nhu cầu của người dùng 
thư viện. Trong thời gian tới, khi các yêu cầu 
mới thúc đẩy việc sử dụng Big Data, các thư 
viện hướng tới việc thu nhận, tổ chức lưu trữ 
dữ liệu (lưu trữ vật lý trong các máy chủ hoặc 
trong các cơ sở dữ liệu), bảo tồn dữ liệu và 
phổ biến dữ liệu, làm cho dữ liệu có sẵn trong 
qua các sản phẩm trực quan. Các thư viện 
tiến tới xây dựng, tạo lập hệ thống bảo quản 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
26
kỹ thuật số (bảo tồn cả tài nguyên số và siêu 
dữ liệu mô tả) có thể phát triển trong nhiều 
năm tới để đáp ứng với các yêu cầu mới. 
Hai là, cung cấp sản phẩm, dịch vụ thư 
viện mang tính cá nhân hóa, tùy chỉnh 
 Hiện nay, các thư viện đang có xu 
vụ trực tuyến. Nhiều thư viện đang sử dụng 
facebook, instagram để quảng bá các sản 
phẩm và dịch vụ thư viện. Các phương tiện 
thư viện. Cùng với các dữ liệu khảo sát thư 
viện, dữ liệu định tính (thông qua phỏng vấn, 
bảng trò chuyện...), dữ liệu tương tác xã hội 
(từ các trang truyền thông xã hội)... các thư 
thư viện. Từ đó, thư viện cung cấp các sản 
cầu của người dùng. Tuy nhiên, trong thời 
gian tới, “sự gia tăng của Big Data làm cho 
nhanh hơn, cho phép các thư viện vượt ra 
học tập và phân tích hiệu suất nghiên cứu” 
[19]. “Trong kỷ nguyên Big Data, không chỉ 
Big Data để đổi mới.” [10]. “Big Data có thể 
cũng sẽ thay đổi cho phù hợp” [17]. Các nhà 
có thể tác động đến hoạt động thông tin - thư 
viện, chuyển đổi phương thức cung cấp dịch 
vụ và tích hợp các hệ thống xử lý. Các hỗ trợ 
cạnh tranh để thư viện thu hút người dùng tin. 
Theo Li Shuqing, Jiao Fusen, Zhang Yong, 
Xu Xia: “Các vấn đề và tiềm năng của các thư 
dữ liệu, công nghệ, dịch vụ và người dùng 
tin. Sử dụng Big Data hiện có và xem xét các 
tại theo quan điểm của người dùng tin, thư 
viện có thể đưa ra các ý tưởng, phương pháp 
có trong các thư viện số” [10]. Đồng thời, nhu 
dùng tin. Kim Young Seok cho rằng: “Bằng 
thực, các thư viện có thể thiết kế các dịch 
tin. Big Data cung cấp thông tin chuyên sâu 
dùng tin, từ đó tạo ra trải nghiệm cá nhân 
hóa” [9]. Ví dụ, người dùng tin tìm kiếm trên 
gì người dùng tin gõ ở mục tìm kiếm, tần suất 
tìm kiếm, số lần tham khảo danh mục tài liệu, 
số lần xem mô tả tài liệu,... được thu thập và 
phân tích để tối ưu trải nghiệm, tạo cơ hội lớn 
hóa. Đặc biệt, với các công cụ phân tích dự 
báo của Big Data, thư viện sẽ nắm được thị 
hiếu, nhu cầu chính xác để cung cấp các sản 
phẩm, dịch vụ phù hợp với người dùng tin 
trong thời gian thực.
Ba là, ứng dụng dịch vụ phân tích dự báo
Giống như hầu hết các ngành khác, phân 
tích dự báo sẽ là một sự thay đổi lớn, quan 
trọng trong các cơ quan thông tin - thư viện. 
hoạt động hiệu quả hơn, đồng thời làm thay 
người dùng tin. Theo cách truyền thống, mối 
khá đơn giản. Người dùng thư viện nộp tiền, 
làm thẻ thư viện và đổi lại, họ được phục vụ 
trong các dịch vụ khác nhau của thư viện. Tuy 
nhiên, mối quan hệ này đang dần thay đổi 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 27
viện. Người dùng thư viện cung cấp dữ liệu 
hành vi người dùng. Thông qua các dữ liệu cá 
nhân như lịch sử sử dụng tài liệu thư viện, lịch 
sử tìm kiếm, cách thức, thói quen tìm kiếm,... 
các công cụ Big Data phân tích dữ liệu, đưa 
ra thông tin chi tiết, xác định khuynh hướng, 
nhu cầu sử dụng thư viện, nhu cầu tài liệu 
người dùng thư viện trong tương lai; các công 
cầu,... Simovic Aleksandar (2018) nhận định: 
“Các công cụ Big Data kết hợp với các thuật 
toán khoa học cho phép các thư viện dự đoán 
lai, giúp dự báo tốt hơn các vấn đề phát sinh 
tin tốt nhất cho người dùng tin” [14]. Về phía 
thư viện, việc sử dụng tài nguyên Big Data 
của người dùng tin, có thể đưa ra các ý tưởng 
các dịch vụ hiện có trong thư viện kỹ thuật số. 
Đồng thời, căn cứ vào các kết quả phân tích, 
dự báo, thư viện có thể xác định thời gian, 
và dịch vụ thư viện đến người dùng thư viện. 
Về phía người dùng thư viện, dựa vào các 
kết quả dự báo về hành vi tìm kiếm, tra cứu, 
sử dụng thư viện, các hệ thống khuyến nghị 
(Recommendation Engine) sẽ gửi đến người 
dùng tin các tài liệu có thể họ quan tâm.
Bốn là, mở rộng dịch vụ chăm sóc 
thư viện, đặc biệt trong môi trường thư viện 
điện tử, thư viện số. Các thư viện đang cố 
gắng để hiểu được người dùng thư viện, giúp 
các thắc mắc, nhu cầu của mình. Big Data 
hoạt, tạo ra giá trị từ quá trình xây dựng mối 
quan hệ thân thiết với người dùng thư viện. 
Cùng với Big Data, hệ thống trả lời tự động 
(như Chatbot) không cần sự trợ giúp của 
con người phát triển tương ứng, giúp tăng 
hiệu quả phân tích dữ liệu Big Data. Hiện 
nay, nhiều thư viện sử dụng Chatbot để giao 
tiếp, trao đổi với người dùng thư viện, tiếp 
các nhu cầu của người dùng. Khi người dùng 
viện, họ có rất nhiều thắc mắc và muốn được 
giải đáp. Chatbot sẽ đưa ra các gợi ý, hỗ trợ 
từng bước một, cung cấp thông tin về các 
sản phẩm, dịch vụ của thư viện cho người 
dùng. Chatbot được thiết kế và phát triển để 
đối thoại. Qua những dữ liệu người dùng thu 
thập được, công cụ phân tích dữ liệu Big Data 
tiến hành phân tích, xác định những nhu cầu, 
dùng thư viện. Bên cạnh đó, Chatbot nhắc 
viện như thời hạn trả tài liệu, thời hạn đổi thẻ 
sử dụng,... Đặc biệt, Chatbot giúp thư viện 
chủ động hỗ trợ 24/7, tăng trải nghiệm tối 
đa cho người dùng thư viện mọi lúc. Chatbot 
lưu lại lịch sử đối thoại, thông tin người dùng 
trong chính thư viện. Chatbot hỗ trợ các thư 
viện khai thác Big Data phục vụ người dùng. 
Trong tương lai, số thư viện sử dụng Chatbot 
tính năng và lợi ích mà Chatbot mang lại. 
Cùng với đó, thông qua dữ liệu người dùng, 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
28
các thư viện có thể phân tích, dự đoán các 
các sản phẩm, dịch vụ thông qua phân tích 
hiện các giải pháp kịp thời. 
Có thể thấy, Big Data giúp tối ưu hóa hoạt 
động thư viện bằng việc thu thập, phân tích 
thông tin, tăng trải nghiệm của người dùng 
tin bằng cách cá nhân hóa thư viện số. Cùng 
với đó, Big Data có thể giúp các thư viện tiến 
hành phân tích dự báo, tìm ra các đặc điểm 
chung dự báo thị hiếu đọc, tình trạng sử dụng 
các cơ sở dữ liệu. Không chỉ vậy, Big Data tạo 
dùng tin trong quá trình sử dụng thư viện.
1. Campbell D. Grant, Cowan Scott R. (2016). “The 
Paradox of privacy: revisiting a core library value in 
an age of big data and linked data”, Library trends, 
Vol. 64, No. 3, P. 492-811.
2. 
Boyd, 
Kate 
(2012). 
Critical Question for Big Data, Information, 
Communications and Society.
3. Doug Laney (2001). “3D Data Managment: 
controlling Data Volume, Velocity, and Variety”, 
Application Delivery Strategies, Meta Group. 
File:949.
4. Francis X. Diebold (2000). “Big Data Dynamic 
and Forecasting”, Discussion of Reichlin and 
Watson paper, in Economics and Econometrics, 
Eighth World Congress of the Econometric Society. 
5. Gartner (2013). Survey Analysis: Big Data Adoption 
in 2013 shows substance behind the hype. 
6. Gerrard, D., Mooney, J. , Thompson, D. (2017). 
“Digital Preservation at Big data scale: proposing a 
step - change in preservation system architectures”, 
Library Hi Tech, http://doi.org/10.1108/LHT-06-
2017-0122, truy cập ngày 17/10/2019.
7. Howe D. (2008). “The future of biocuration”, 
Nature 455, P. 47-50.
8. Kenneth Cukier (2010). “Data, data everywhere”, A 
special report on managing information, Economist 
Newspaper, Volume 394.
9. Kim Young Seok (2017). “Big data analysis of 
chernoff face method”, Journal of Documentation, 
Vol. 73, No. 3, P. 466-480.
10. Li Shuqing; Jiao Fúen; Zhang Yong; Xu Xia 
(2019). “Problems and Changes in Digital Libraries 
services”, Journal of Academic Librarianship, Vol, 
45.
11. Michael Cox, David Ellsworth (1997). “Application 
- Controlled Demand Paging for Out - of - Core 
Visualization”, Report NAS-97-010, NASA Ames 
Research Center.
12. Marydee Ojala (2018). “Big Data and AI: 
technology, transparency, and trust”, http://www.
infotoday.com, truy cập ngày 14/11/2019.
13. Randal E. Bryant, Randy H. Katz, và Edward D. 
Lazowska (2008). “Big - Data Computing: Creating 
in 
Commerce, 
Science and Society”, Computing Community 
Consortium, http://www.cra.org/ccc/initiatives, truy 
cập ngày 14/11/2019.
14. Simovic Aleksandar (2018). “A Big Data smart 
institution”, Library Hi Tech, Bradford, Vol. 36, Iss. 
3, tr.498-523
15. Steve Bryson, David Kenwright, Michael 
Cox, David Ellsworth, Robert Haimes (1999). 
“Visually exploring gigabyte data sets in real”, 
Communications of the ACM, Vol. 42, No.8, tr.83-
90.
16. Tan Jee Toon (2014). “Dữ liệu lớn, nhân 
tố thay đổi “cuộc chơi” của doanh nghiệp”, 
http://vneconomy.vn/cuoc-song-so/du-lieu-
lon-nhan-to-thay-doi-cuoc-choi-cua-doanh-
nghiep-20140422025542917.htm, truy cập ngày 
14/11/2019.
17. Waqar Ahmed, Kanwal Ameen (2017). “Defining 
the field of information and library management”, 
Library Hi Tech News, p. 21-24.
18. Ye Chunlei (2017). “Research on the key 
technology of big data service in university library”, 
the Institude of Electrical and Electronics Engineers, 
Inc. Conference Proceedings, Piscataway.
19. Yu Jen Chien (2016). “Library Data, Big Data or 
Better Data: Challenges from the Field”, ASIST 
Meeting, Proceeding of ASIST annual meeting, 
Vol 53, No. 1.
20. Zhan Ming, Widén Gunilla (2018). “Public 
libraries: roles in big data”, The Electronic library, 
Vol. 36, No.1, P. 133-145.
(Ngày Tòa soạn nhận được bài: 26-12-2019; 
Ngày phản biện đánh giá: 10-03-2020; Ngày chấp 
nhận đăng: 15-03-2020).
NGHIÊN CỨU - TRAO ĐỔI

Removed lines from big-data.pdf:
Adding Value to Manufacturing, Retail, Supply Chain, and 
Oklahoma State University, Stillwater, OK 74078 
The concept of big data has been around for many years.  Only in the last few years have organizations 
started to understand how they can use big data to gain insightful knowledge about their business operations, 
which is enabling them to make better business decisions. While there is no single definition, big data 
usually works on the principles of four Vs - Volume, Velocity, Variety, and Veracity. As the name suggests, 
big data is really big, meaning a huge amount of data is being generated daily, reaching the scale of 
petabytes. This data comes in all forms -  structured, semi-structured, and unstructured and is pouring in 
from all directions and generated by many systems and devices, such as transactional systems, log files, 
GPS devices, smartphones, RFID readers, surveillance cameras, sensor networks, Internet of Things (IoT), 
and social media. Finally, as big data becomes an important asset for enterprises, the focus is also on the 
trustworthiness of data and its sources. 
According to Gartner, Inc., “Big data is high-volume, high-velocity and high-variety information assets that 
demand cost-effective, innovative forms of information processing for enhanced insight and decision 
making.”a In this article, we first elaborate on the big data concept and present the storage and processing 
technologies that have been developed to deal with big data.  We then briefly discuss the evolution of 
traditional analytical processing to today’s big data analytics.   Through several applications and use cases, 
we illustrate how big data analytics is adding value to manufacturing, retail, supply chain, and logistics 
operations.  Finally, we conclude by discussing key challenges that businesses have to face as the use of 
big data analytics becomes more widespread. 
Regardless of the decision to be made - optimized production/work schedules, accurate forecasts, customer 
preferences - data nowadays has the potential to help businesses succeed more than ever before.  From an 
organizational perspective, big data is a holistic approach of obtaining actionable insight to create a 
competitive advantage over others.1 There are two distinct approaches to applying big data - improve the 
propositions. A challenge that organizations increasingly face is finding and working with trusted data. 
Working with inaccurate and untrusted data can be worse than having no data at all. As data requirements 
and regulations become more complex, organizations must be aware of where all their data is coming from, 
where it is getting stored, and who is interacting with this data as conclusions are drawn.2 
a https://www.gartner.com/it-glossary/big-data/ 
2 
What is Big Data? 
of data that needs to be handled and tracked, the speed at 
which the information is flowing into online systems, and 
regular basis. Because of the changes happening in the Web 
environment, new definitions for big data have been 
proposed, with a focus on technologies that handle this data. 
O’Reilly defines big data as “Big data is data that exceeds 
the processing capacity of conventional database systems. 
The data is too big, moves too fast, and doesn’t fit the 
structures of traditional database architectures. To gain 
value from this data, organizations must choose an 
alternative way to process it.”3 
To understand how big data is transforming businesses, we 
focus on the size of data in storage.4 Size is important but 
there are other aspects to big data namely variety, volume, 
and more recently, veracity.2 Together they are called the 4 
Vs of big data: Volume, Velocity, Variety, and Veracity. 
databases, data warehouses, and data marts.6 Here, the data 
is uploaded to operational data stores using Extract, 
Transform, and Load (ETL) tools which extract data from 
internal and external sources, transform the data to fit the 
operational needs, and finally load the data into the data 
warehouse. The key point is that the data is getting cleaned, 
transformed, and cataloged before being made available for 
data mining and online analytical functions. This traditional 
data warehouse approach discourages the incorporation of 
new data sources until they are cleansed and integrated.  
Since data is ubiquitous these days, big data storage 
environments need to be “magnetic” in nature, attracting 
data from all sources. Hence, big data calls for Magnetic, 
Agile, and Deep (MAD) analysis skills, which differs from 
the traditional data warehousing approach. Given the growing number of data sources and the sophisticated 
tools for data analysis, big data storage should allow analysts to easily process and use data rapidly. 
Solutions like distributed file systems and Massive Parallel Processing (MPP) databases are available 
nowadays for providing high query performance and platform scalability. Non-relational databases such as 
Not Only SQL (NoSQL) were developed for storing and managing unstructured data.7 These newer 
technologies aim for scalability, data model flexibility, and simplified application development and 
deployment. They separate data management and data storage and focus on high performance scalable data 
Volume. The ability to process a large 
social media, from Internet of Things to 
system logs, etc. 
Velocity. The rate at which data is 
getting created every second of the day. 
contributor, 
more 
data 
is 
generated and logged than ever before.5 
Also, the rapid adoption of social 
created a deluge of data. Advances in 
useful now. 
Variety.  It is the diversity of data 
which organizations are witnessing. 
processing a limited set of data, such as 
and 
logs. 
includes images, voice recordings, 
videos, and texts generated from 
media to deliver new insight. 
Veracity.  It is not just the quality of 
data, but also the trustworthiness of 
data sources.  Basic issues are the 
accuracy and applicability of data.  
uncertainty due to inconsistencies, 
incompleteness, ambiguities, etc. 
3 
storage, allowing management tasks to be written in the application layer instead of having it written in 
database specific languages.   
Why Big Data? 
When organizations adopt big data as a part of their business model, the first tangible question is usually 
what value this big data will provide to the company.7 Data must be used to make better decisions, to 
optimize resource consumption, and improve process quality and performance. It should also aim to 
perform precise customer segmentation, optimize customer satisfaction, and increase customer loyalty. 
from existing products and create additional revenue from new products. 
Newer Data Sources, Newer Opportunities 
The new sources of big data include industries which are taking a big step step towards digitization, and as 
a result, data growth in the past few years has been phenomenal. Some of the areas where data is coming 
from include social media, internet browsing pattern data, advertising response data, financial forecasts, 
location information, driving patterns, vehicle diagnostics, and traffic and weather data from sensors, 
monitors, and forecast systems. Other sources of data include data from healthcare, where the healthcare 
industry is implementing electronic medical records and digital imaging, which is used for short-term public 
health monitoring and long-term research programs.  Similarly, low cost gene sequencing can generate 
effectiveness in life sciences.8 Another area is data from video surveillance which is transitioning from 
patterns for security and service enhancement. Transportation and logistics industry has been generating 
and storing enormous amount of data coming from sensors, GPS transceivers, RFID tag readers, smart 
meters, cell phones, material handling equipment enabled with sensors, etc. This data can be used to 
opportunities. 
information contained therein.9 It involves applying algorithmic processes to derive insights. Analytics is 
used to extract previously unknown, useful, valid, and hidden patterns and information from large data 
sets.6 While the focus of analytics has been on inference, it can also provide prescriptive insights as 
explained later in this section.  Hence, analytics has a significant impact on research and technology, as 
businesses recognize its great potential in helping them gain competitive advantage. 
“Big data analytics is the use of advanced analytic techniques against very large and diverse data sets that 
include structured, semi-structured, and unstructured data from different sources, and in different sizes from 
terabytes to zettabytes.”b It helps in uncovering hidden patterns, unknown correlations, market trends, 
customer preferences and other useful information. Advanced analytics can help organizations discover 
what has changed and how they should react.  Analytics is the best way to discover new customer segments, 
identify the best suppliers, associate products of affinity, understand sales seasonality and so on.4 
Organizations are implementing specific forms of analytics tools and techniques which include data mining, 
statistical analysis, data visualization, artificial intelligence, machine learning, and other data capabilities 
b https://www.ibm.com/analytics/hadoop/big-data-analytics 
4 
which support analytics4. Though these techniques have been around for many years, organizations are 
using them now as most of these techniques adapt well to very large, multi-petabyte data sets. 
Big data’s worth is only realized when businesses can indulge in decision making using this data. To enable 
such data-driven decision making, organizations must use efficient processes to turn the high volume of 
fast moving and diverse data into meaningful insights. Analyzing big data allows researchers and businesses 
harness their data and use it to identify new opportunities which in turn leads to better and smarter business 
moves, more efficient operations, higher profits and satisfied customers and an overall competitive 
advantage.6 Big data analytics could be viewed as a sub-process in the complete process of knowledge 
extraction from big data.  
As organization began to adopt data analytics in the late 1990s and early 2000s, they faced many hurdles.  
professionals. Analysts used to spend more time collecting and preparing data than analyzing it.  They 
focused on finding more accurate and reliable solutions to business problems, while keeping the solutions 
simple at the same time so that business users could understand it.  Some examples of tools used during 
this time period are SAS, a tool for building backend data inference and modeling; Oracle and Teradata, 
detailed solution suites for easy development of solutions; IBM CPLEX, a tool for solving large 
optimization problems; and Cognos and MicroStrategy, tools for visualization, mostly in the form of 
reports. 
In late 2000s, social media giants like Google and Facebook and other internet-based companies in general 
started uncovering, collecting, and analyzing newer types of data which later evolved into big data. In 
addition to the data generated by companies in their internal operations and transactions, newer data was 
brought in from external sources including public data sources, social media, and mobile devices. Analysts 
realized this new data was qualitatively different (e.g., unstructured text, pictures, audio, and video) along 
with the much larger volumes as compared to internal company data.  This led to the development of newer 
tools and technologies, examples of which are Hadoop, a pioneer in distributed data storage and processing 
with low cost, flexibility, and scalability; Python and R, open source programming languages with vast and 
ever-evolving libraries for statistical data analysis; Tableau, Looker, and Microsoft Power BI, popular 
visualization products to develop, customize, and build visually appealing and interactive web dashboards. 
Descriptive, Predictive, and Prescriptive Analytics 
Analyzing data is not limited to deriving insights from the past, but it can also help businesses in predicting 
future outcomes and optimizing business performance. Currently organizations use three types of analytics 
at different stages in their decision-making process - Descriptive, Predictive, and Prescriptive analytics as 
shown in Figure 1. The latter two are also referred to collectively as advanced analytics. 
Descriptive analytics does exactly as the name suggests, ‘describe’ or summarize the data and convert it 
into something useful. It is the most basic type of analytics and almost 90% of the organizations today use 
this technique. Descriptive analytics is the analysis of historical data using data aggregation or data mining 
and lies at the bottom of the big data analytics value chain. However, it is extremely valuable because it 
organization’s future.  
Descriptive analytics is an important step to make raw data understandable to its users, and it helps in 
answering questions like “What is happening?” Consider for example, a metric that companies get from 
5 
web servers using Google Analytics tools, namely page views.  It can be used to determine if a strategy was 
a success or not. The main objective in descriptive analytics is to find the reasons behind the previous 
it can help the organization in strategizing.  
The majority of the statistics we use comes from descriptive analytics – e.g., calculations as simple as 
averages and standard deviations. Descriptive models use basic mathematical and statistical techniques to 
derive key performance indicators that can highlight the historical trends in data. STATA, MS Excel, and 
SPSS represent the older generation of descriptive analytics tools, while R and Python are quickly becoming 
the preferred tools in industry because of vast open-source libraries and the ease of development and 
deployment. Descriptive analytics can yield historical insights into an organization’s production, inventory 
levels, sales, operations, financials, and customer behavior.  
Figure 1. Analytics Framework by Tom Davenport26 
Predictive analytics can be defined as the ability to “predict” what might happen and a better understanding 
of future outcomes. It is one of the more sophisticated types of analytics techniques and employs statistical 
techniques and machine learning. It is used to detect clusters, tendencies, and exceptions, and to predict 
future trends, making it a valuable tool for forecasting. The foundation of predictive analytics is probability. 
It takes the data which the user has and tries to fill in the missing data values with best guesses. It helps in 
finding the answer to ‘What could happen?’ With properly tuned models, predictive analytics can support 
complex forecasting in marketing and sales. This helps an organization to set realistic goals for business, 
restrain expectations, and do effective planning. 
Tools used to apply predictive modeling vary by the nature of model’s complexity, but some commonly 
used tools are SAS, MATLAB, R, Python, among others. The common functionality of these tools is that 
they combine historical data found in POS, ERP, CRM, and HR systems to identify patterns in the data and 
apply algorithms such as random forest and Generalized Linear Model (GLM) for prediction, and K-means 
clustering for identifying clusters. Finally, simulation can be employed to statistically predict the outcomes 
of specific decision scenarios.   
6 
An application of predictive analytics is to produce credit scores, which are used by financial services to 
determine the probability of customer making timely payments. Other business uses include, how sales 
might close at the end of a year, inventory level forecasts, predicting what items a customer might purchase 
together and other customer purchasing patterns. Despite all the advantages that predictive analytics brings 
to the table, it is important to understand that forecasting is just an estimation, and its accuracy depends on 
the quality and stability of data.  
Prescriptive analytics is the most sophisticated analytics approach which makes use of optimization 
techniques to explore a given set of options and prescribe the best possible solution for a given scenario. 
As the name suggests, it “prescribes” a solution to a specific problem. One approach is machine learning 
neurons using training data sets.  Once trained, the neural network model can suggest the optimal course of 
action supporting the business objective for a given set of business inputs. Simulation, a predictive analytics 
tool at its core, can also be part of a powerful prescriptive analytics approach when combined with 
appropriate search or optimization techniques.  Prescriptive analytics not only predicts ‘What will happen?’, 
but also determines “What the company should do?” It provides recommendations for the actions to be 
taken to achieve optimal business performance. Because it has power to suggest optimal solutions, 
prescriptive analytics is the ultimate frontier for advanced analytics. 
Prescriptive analytical models are complex in nature. However, when implemented efficiently, prescriptive 
analytics can have a significant impact on the decision-making effectiveness of the organization. Technical 
advancements such as cloud computing have made deployment of these complex models much easier. 
prescriptive analytics to optimize production and inventory decisions in supply chains, optimize customer 
experience, and to make sure that the right product is being delivered at the right time.  Airline systems use 
travel factors, demand levels, purchasing patterns, timings, etc., in order to maximize the revenue generated.  
hence, they are ensuring to choose the right kind of analytics solutions to reduce operational cost, enhance 
service quality, and increase ROI. 
Big Data Analytics Applications and Use Cases 
Supply chain activities produce a huge amount of data, which is being continuously generated by systems 
and devices such as POS, ERP, SCM, RFID, GPS, blogs, and wiki entries, not to mention the unlimited 
data generated from sources like CCTVs, digital clickstreams, imagery, social media posts, and discussions 
on various forum platforms. Advanced connected devices and technologies which support today’s supply 
chain such as sensors, smart devices, and tags are continuously gathering real-time data and providing an 
end-to-end visibility in the supply chain. It becomes the task of supply chain managers to tap and process 
this data to make insightful decisions which could help boost productivity and reduce costs. 
7 
Application 
Area
Technique/Technology/
Supplier Insight Program Greater insight into suppliers' financial stability, 
performance, and ability to provide services.
Achieved product excellence, reduction in time-to-
market through co-development and co-production. 
Better alignment between engineers, suppliers, and 
customers.
Better service level; accurate prediction of customer 
needs and demand; automated planning and 
forecasting operations.
Inventory management with streaming analytics, real-
time data delivery and updates every few hours, and 
accurate performance analysis of each store.
Machine learning-based 
accuracy and greater profit margins.
application
Accurate forecasts, reduction in delivery time by 
upto 50%, and better service levels.
& Co.
Analytics application 
using Intel's Trusted 
Better tracking of in-store items using RFID tags; 
updating item location and inventory; helping 
salesperson track misplaced item to avoid lost sales.
Data-intensive 
Increase in forcasting accuracy; reduced inventory, 
stockouts, and obsolescence; better access to 
company's logistics needs.
Anticipatory shipping
before actual customer orders.
and packing area.
Drone-based delivery
Goods delivered to locations less than 30 minutes 
Cloud-based 3D 
Optimize picking accuracy, inventory turns, and 
warehouse productivity in real-time using inputs 
from sensors, such as  shelf weight and weight on 
forklift.
Quality early-warning 
Reduced rework, increased productivity and cost 
savings, higher quality standards, and improved 
service levels, by detecting and prioritizing quality 
related issues much sooner in the supply chain.
Greater demand and supply visibility, better 
distribution channel management, better service level, 
and improved inventory management.
collection.
Co.
Real-time monitoring and 
Greater visibility for customers, better pallet 
management, optimized space utilization, greater 
labor productivity, inventory accuracy of 99.9%, and 
improved customer satisfaction.
UPS
Optimized 55,000 delivery routes in North America, 
saving close to $400 million annually. Reducton in 
transportation.
shipment information, reduction in mileage and cost, 
and improved CO2 efficiency.
Resilience 360
Accuracy in risk detection, prevent production 
inefficiencies and revenue losses, maintain service 
levels, and reduce emergency cost by efficiently re-
routing shipments in case of unforeseen events.
analyze potential business opportunities. Real-time 
in a given location.
quality of address information is poor. Real-time 
ddress verification to optimize route planning.
DHL
Applications of Big Data Analytics
IBM
Supply Chain 
and 
8 
Applications and Use Cases in Manufacturing 
Raytheon, a major U.S. based defense contractor and industrial corporation, made use of data analytics to 
reduce costs within their supply chain and production operations. They developed a Supplier Insight 
program, which integrated structured and unstructured data from internal and external sources.10 With more 
than 10,000 suppliers, they needed a platform that could provide rapid, data-driven decision making 
capability. With this new system, they could track suppliers’ financial stability, performance, and their 
ability to provide services in the face of disruptive events. Raytheon was able to immediately identify if a 
supplier could provide what they needed, and quickly made decisions that reduced any adverse impact on 
their customers. Supplier Insight has allowed them to negotiate the cost better, by engaging in long-term 
contracts with suppliers for multiple programs.10 They now have an ability to look across all their suppliers 
and programs to achieve cost reductions. Raytheon has also developed smart factories which have the 
capacity to handle big data coming from different sources like sensors, instruments, CAD models, internet 
transactions, simulations, and digital records in the company, which equips them with real-time control of 
various elements of the production processes. For example, their Immersive Design Center (IDC) makes 
use of a 3-D immersive environment to achieve product excellence and decrease time-to-market through 
co-development and co-production of products by immersive data visualization and interaction.10 This also 
resulted in better alignment between their engineers, suppliers, and customers. They work together to refine 
the design and detect potential problems without the work and rework associated with expensive prototypes, 
resulting in reduced costs.10  
Lennox International, a U.S. based cooling and heating devices manufacturing company, integrated 
their expansion throughout North America.11 With the help of machine learning algorithms, they accurately 
predicted customer needs, while understanding customer demand better. It also helped the company to 
automate its planning and forecasting operations. 
Many companies gather data on supplier information and purchasing volumes for annual supplier 
performance review, spend analysis, and cost savings analysis functions to support strategic decisions.  For 
example, a pharmaceutical company created a database of all the bids submitted for packaging.12 This data 
was then evaluated to understand the cost structure of suppliers and to create detailed cost models for 
different packaging options. Such models can help in the selection of the most cost-effective supplier for 
new packaging.12 Another example is how IoT with its network of sensors embedded in millions of devices 
can enable new opportunities in manufacturing. For example, real-time information on a machine’s 
condition can initiate a production order for a spare part, which then can be shipped using a drone to the 
plant engineer for replacing the faulty or near faulty part.12 It also helps in determining when and how 
critical maintenance is required by a specific machine, thereby avoiding costly equipment breakdowns and 
improving the overall production efficiency.  
Daily production needs to be monitored to maintain the efficiency and output of a company. Big data 
analytics uses the data collected from operational machines, employee records, and data logs of the number 
of units produced, to provide insights to the operations manager, helping him/her to make changes that are 
profitable for the company. Manufacturers are also exploring predictive analytics to realize significant 
savings in product testing and improving product quality. Since different products and parts require 
different tests, instead of performing numerous quality tests on each part, data mining and pattern 
recognition can be used to determine the type and number of tests truly needed for each part or product.13  
9 
Applications and Use Cases in Retail 
Walmart, the number one fortune 500 company, has the world’s largest private cloud, which helps support 
real-time data feeds to its decision makers. Walmart’s Data Café based at their Bentonville, Arkansas 
headquarters takes care of most of this cloud architecture.14 Their original data infrastructure only enabled 
managers to get weekly reports, which prevented them from making decisions based on real-time market 
conditions. Also, the reports were standardized with little room for customization. Data café, which was 
built on SAP’s HANA in-memory analytics engine, enabled inventory management with streaming 
analytics, and provided an enterprise view of timely information flow for a large cross-sectional staff 
looking to resolve every-day business issues.14 The data delivered through this system is almost real-time 
and updated every few hours.  Furthermore, the system was designed to be responsive to providing reports 
and queries required by managers in the given time frame, which helped them gain timely insight and make 
better decisions. These insights are derived from “200 streams of internal and external data which includes 
40 petabytes of recent transactional data, and can be manipulated, modeled, and visualized.”14 The 
importance of near real-time insights is crucial since it helps managers respond to challenges in real-time 
as they arise. For example, on Black Friday, Walmart’s Data Café provides near real-time insights on the 
performance of east-coast stores, which enables Walmart to make pricing adjustments for west-coast stores 
before they open.14 During a recent Halloween, sales analysts were able to see that two stores were not 
selling a novelty cookie that was very popular in most stores. Using near real-time data from Data Café, it 
was discovered that simple stocking oversight led to the cookies not being put on shelves in these stores.14 
The company was able to react in real-time to avoid additional lost sales. Data Café also provides automated 
alerts to managers when a metric falls below a threshold in a department. This tool has reduced the problem-
solving time from weeks to minutes using reliable internal and external sources of data. 
Levi Strauss & Co, a leading American clothing company, provides better in-store shopping experience for 
items using IoT technology coupled with advanced analytics. Levi’s in collaboration with Intel® 
implemented a solution using Intel’s Trusted Analytics Platform (TAP), which helped salespersons to 
quickly find misplaced items in the store.15 This application made use of RFID tags woven into clothing 
items, in-store antenna sensors installed in the ceiling of the store to continuously track the RFID tags, and 
cloud-based analytical tool built on TAP for detailed analysis. This technology helped determine when 
items are no longer in their correct place or no longer available at that time. TAP algorithms use data 
collected overnight to determine the exact location of various groups of items, and during store hours 
sensors track the location of items and an algorithm determines if an item is in its assigned location. If an 
item is placed in its assigned group location, no action is generated by the algorithm.  Suppose a pair of 
jeans is lying in the T-shirt section or left in the fitting room, the TAP algorithm will generate an alert on 
the mobile application instructing the salesperson to put the item back in its assigned location.15 This helps 
the salesperson to keep the item where it belongs and avoid lost sales. Levi’s also aims to generate customer 
insight using big data analytics with the data collected from sensors tracking customers’ in-store behavior 
to better understand their preferences.15 
Groupe Danone, a French multinational food-product corporation, found itself making accurate predictions 
only 30 percent of the time for responses to promotional offers, which was resulting in significant losses to 
the company.11 When they implemented machine learning in their planning architecture, they saw 
significant improvement in both sales and forecasting. Similarly, Granarolo, an Italian dairy company, used 
machine learning to increase its forecasting accuracy by 5 percent, decreased delivery times by up to 50 
percent of the original time, which resulted in better service levels.11 Morrisons, one of UK’s largest food 
10 
retailers, was able to dramatically improve same store sales and achieve a 30% reduction in shelf gap and 
from Blue Yonder, which uses AI technology to “improve demand planning and reinvigorate replenishment 
based on customer behavior in every store.”16 Blue Yonder’s data-intensive forecasting methods deployed 
as cloud-based services is making such advanced capabilities accessible to other retailer’s as well.12 
Applications and Use Cases in Supply Chains and Warehouses 
In supply chain operations, planning and forecasting are among the most data-driven operations, which use 
an array of supply chain planning tools supported by ERP systems. With the use of supply chain analytics, 
it is now possible to re-envision the planning processes by using external and internal data sources to make 
real-time decisions based on market trends, uncertainty, seasonality, and other fluctuations.  
IBM understood the value of big data analytics early and employed it in optimizing their supply chain 
operations. They have used various analytical tools to solve a range of problems, and a few of them are 
discussed here.17 IBM’s Quality Early-Warning System (QEWS) was typically deployed upstream at 
suppliers, IBM’s operations, and in the field.  QEWS detects and prioritizes quality related issues much 
sooner than the traditional quality control processes. Analyzing big data coming from across their supply 
chain, IBM was able to reduce rework, increase productivity, ensure higher quality standards, and improve 
customer satisfaction, leading to significant cost savings. For a company like IBM, ensuring correct 
inventory levels with so many business partners was challenging. They made use of IBM Buying Analysis 
Tool, which not only provided demand and supply visibility, but ensured better distribution channel 
management, delivery of the right product at the right time to meet customer demand, while maintaining 
proper inventory levels. IBM also used a tool named Accounts Receivable, which uses advanced analytics 
to optimize the resources needed to collect revenues. They also make use of supply chain social listening, 
disrupt the supply chain.17 It also helps them obtain timely information and feedback on their products. As 
an early adopter, IBM has been using predictive and prescriptive analytics in its supply chain over the last 
several years. 
Warehousing is another area where big data analytics is creating new opportunities.  Logivations, a German 
supply chain solutions provider, offers a cloud-based 3D warehouse layout planning and optimization tool, 
camera-guided AGVs and tracking, and various other supply chain analytics solutions.18 Such technologies 
existing warehouse by simulating new configurations. Another example is the analysis of images and videos 
captured by AGVs, and sensor inputs including shelf weight and weight on the forklift, to monitor picking 
accuracy, inventory turns, and warehouse productivity in real-time.12 Also, forklift drive picking 
productivity and route optimization can be achieved by analyzing the route choices and driving behaviors.12  
A leading forklift provider is looking into all these opportunities, and figuring out how a forklift truck can 
be used as a big data hub - collecting  real-time data to identify additional sources of waste in the warehouse 
operations, using a hybrid of analytics and ERP and WMS data. Amazon is another warehouse automation 
pioneer, deploying Kiva robots that bring the items (racks) to the picking and packing area in their 
fulfillment centers. With increasing pressure to reduce order-to-delivery times, warehouses are turning to a 
flexible automation strategy by using autonomous technologies such as Amazon’s Kivac robots and 
GreyOrange’s Butlerd system to increase their picking efficiency.  Amazon has also tried to deliver goods 
c https://www.amazonrobotics.com/#/  
d https://www.greyorange.com/butler-goods-to-person-system  
11 
to people living less than 30 minutes away from an Amazon warehouse or distribution center via a drone. 
Amazon has also patented an “Anticipatory Shipping” technology to identify which orders should be 
packed and pushed into the logistics network before the actual customer orders are placed.12 
Merchandise Warehouse Co. (MW), a logistics provider of multi-temperature warehouse services in the US 
mid-west, provides services such as tempering, inspection, blast freezing, temperature monitoring, labeling, 
import/export, and packaging.19 With such operations there is little room for error, since clients’ food 
products could get spoiled if they are not maintained at correct temperatures.19 MW needed real-time 
enable quality assurance with comprehensive traceability.  They wanted this for all operations including 
inspections and holds.19 Technologies such as CCTV, WMS, electronic data interchange, mobile 
computers, and scanners were employed to help track and analyze data to get real-time information in the 
warehouse and manage inventory. It helped MW’s customers gain visibility by having on-line access to 
temperatures, activity reports, and information about inventory levels. MW’s solution also includes tools 
for pallet management for tracing every pallet from the time it arrives in the warehouse to until it leaves. 
inventory to marked temperature zones were provided by the new system. It also ensures greater labor 
productivity and accuracy using workflow-based warehouse management and could automate processes 
designed for specific customer needs. MW reaped various other benefits from this initiative like accurately 
capturing billing events in real-time resulting in reduced labor used for billing and paperwork. The system 
helped the company deal with the issue of “catch weight”, where the actual weight of the product,  especially 
meat, varies when it hits the retail shelves, a common problem in cold storage warehouses and food 
industry.19  Increased customer satisfaction levels were also achieved, since clients had real-time access to 
information and reports when needed. The solution helped MW achieve an inventory accuracy of 99.9 
percent from a previous 98.6 percent.19 
Applications and Use Cases in Logistics 
Logistics companies need to keep the goods moving at all times, even in the face of disruptions such as 
storms, cargos getting stranded due to ship crashes, and geopolitical events in order to keep the businesses 
running.  A Netherlands based logistics management company uses big data analytics on Microsoft’s Azure 
cloud to keep its customers informed about the number of goods in each container, their location at a given 
time, and expected delivery times.20 Purchase orders are tracked using mobile applications to identify 
challenges which could delay the delivery of an order. Tariff calculations and fees related to the movement 
of shipping containers are calculated by another application which can be accessed by the client, giving 
them a greater insight into financial risks.20 These mobile applications make use of big data analytics in 
internal supply chain operations to provide actionable business insights. Previously the time it took to 
identify a challenge and develop a solution to address it could be anywhere from 3 to 9 months. With the 
use of big data technologies, this time has been brought down to a couple of weeks depending upon how 
complex the problem is.20  
Companies managing their own supply chains and those outsourcing to third-party logistics providers 
manage a massive flow of freight, goods, and products daily while at the same time creating vast data sets. 
Millions of shipments are tracked daily from origins to destinations, generating information such as the 
content, weight, size, location, and route of each individual shipment, across a large number of networks. 
Companies are exploiting and analyzing these large data sets to improve their operational efficiencies, 
effectiveness, and customer service. A study by the Council of Supply Chain Management Professionals 
shows that 93 percent of shippers and 98 percent of 3PL providers feel that data-driven decision making is 
12 
a crucial supply chain activity.21 Also, 71 percent of these believe big data improves performance and 
quality. Logistics companies can utilize big data analytics to consolidate, interpret, and store the data 
coming from various sources for immediate or future use based on their requirements.  
Courier and delivery companies like UPS use real-time routing of deliveries using the trucks’ geo-location 
and traffic information data. UPS spent almost 10 years developing its On-Road Integrated Optimization 
and Navigation system (ORION) to optimize close to 55,000 routes in North America in its delivery 
network.22 This system saves the company $300 million to $400 million annually by saving about 100 
million miles per year, which is a reduction of 10 million gallons of fuel consumed and reducing CO2 
emissions by almost 100,000 metric tons.22  Data mining techniques also help logistics companies deliver 
services with fewer delivery attempts, by using predictive analytics to predict when a customer is more 
likely to be available at home.12 Costs and carbon emissions can also be reduced by selecting the right mode 
of transportation for deliveries. An example is the use of supply chain analytics to understand the priority 
and can be delivered by rail.12  
Better transportation planning can be achieved with the use of Transportation Management System (TMS) 
which can help identify future shipping patterns, optimize routes, carrier selection, or loads, and secure 
necessary capacity. This is achieved by tracking shipment frequency and identifying the endpoints of supply 
chains by studying precise inbound and outbound statistics. Direct application of predictive analytics is 
helping logistics providers make real-time decisions which result in reduced costs, greater reliability, and 
improved customer satisfaction. For example, data streams produced by sensors on delivery trucks, beacons 
which broadcast their presence to nearby devices such as computers and smartphones, radar devices, and 
employing simulation models.23 When a shipment is going to be late, a carrier can make real-time 
adjustments to prevent bottlenecks further down the supply chain.23 
DHL, a global logistics provider, has extensively explored big data analytics in their supply chain activities 
and is currently employing several smart systems around their services. Increasing the last mile efficiencies 
is often the most expensive step in the supply chain.24 Last mile optimization is an extensively studied area 
and researchers have found promising applications of big data analytics here. Data analytics is applied to 
achieve real-time optimization of delivery routes, where streams of data are processed to maximize the 
performance of the delivery fleet. Rapid processing of real-time information supports the goal of route 
optimization on the last mile, saving time in the delivery process. When the vehicles are loaded and 
unloaded, manual sequencing of shipments is eliminated by the use of sensors, and dynamic calculations 
are used to find the optimal delivery sequence. Based on real-time traffic conditions on the road, telematic 
databases are used to change the delivery route automatically. DHL’s SmartTruck uses data mining, 
machine learning, and other data analytics techniques to optimize the initial tour planning based on 
incoming shipment on a daily basis.24 Dynamic routing system recalculates the routes depending on the 
traffic situations and delivery times. This also results in cost reduction and improved CO2 efficiency by 
reducing the miles travelled. 
It is vital for robust supply chains to be able to cope with unforeseen events in today’s rapidly changing 
world. Apart from being flexible and resilient, businesses need accurate risk detection systems to keep 
running smoothly. Big data analytics and complex event processing algorithms are used to alert businesses 
when a pattern falls in the set of critical conditions such as tornadoes or floods in an area, or breakdown of 
fleet. These alert systems send a report on the probability and impact of the risk and provide suitable 
actionable insight to alleviate potential interruption. With this information on hand, customers can re-route 
their shipments or manage supplies from other distribution locations. DHL’s Resilience 360 risk 
13 
management solutions aims to provide such functionalities.24 It is equipped with two components, a risk 
assessment portion and supply chain monitoring instruments, both operating in real time. This improves the 
resilience of the supply chain and prevents production inefficiencies and revenue losses. Resilience 360 is 
designed to maintain prescribed service levels, protect sales and operations, and reduce emergency costs, 
creating a competitive advantage for the company.24 
Future economic development is often modeled on global transportation of goods and services. The type of 
goods shipped indicate the local demand and supply preferences. Logistics providers make use of big data 
shipments by their distribution networks. These shipment records are a valuable resource for market 
intelligence research, and logistics providers refine this data to substantiate existing market research. 
Regression analysis techniques are used to produce demand and supply forecasts with the use of the 
shipment records and market research outcomes. The primary target group for these advanced data analytics 
services are small and medium-sized enterprises, which lack capacity to conduct their own market research. 
The results from regression-based analytics have high predictive value, which can help these enterprises 
serve a larger customer base, and generate accurate forecasts based on industry, geography, and product 
category. DHL Geovista is one such online geo-marketing tool available for small and medium-sized 
enterprises to analyze potential business opportunities.24  
shipments more accurately.24 Customer’s delivery address verification is a fundamental requirement for 
any logistics provider. This can be troublesome in developing countries and other remote areas, where the 
area. Address Management uses daily freight and parcel delivery data and matches this data with reference 
data and returns the incorrect incoming data with validated data from the database, in order to verify the 
address in real-time and optimize route planning for retailers and public sector entities.  
Other Applications 
There are several other applications of big data analytics which a company can encounter on a regular basis. 
Locating a new store is a strategic decision for a company, and big data analytics could play an important 
role here. Extensive data analysis is performed by the analysts in exploring customer data, demographic 
factors, retailer network, location of other competitors in the area, and market potential. A recent example 
of this is the location for Amazon’s HQ2. Visualizing the growth of a company has become easier with the 
use of data analytics, since it is now possible to quickly compare the performance matrix of different sites 
and identify the reasons behind such results. Predictive analytics comes in handy in analyzing the market 
and gaining insight on questions related to global growth strategy, site relocation, new product introduction, 
and supplier selection. 
business profitable.25 Data analytics tools simplify the process of price formation, which not only accounts 
for the cost of production of an item, but also the spending capacity of the customers and presence of 
competitors in the market. Price flexibility, buying patterns of the customers, competitors’ prices, and 
seasonality are analyzed using the data coming from various sources. Machine learning algorithms help 
to changes in prices. Furthermore, using real-time price optimization techniques, retailers can attract new 
customers and retain existing customers by adjusting the price as per market trends. Recommendation 
engines is another great way of predicting customers’ behavior, since they give a retailer insight into 
customers’ reviews and opinions. It also helps the retailers to increase sales and stay abreast with trends. 
14 
Based on machine learning algorithms, recommendation engines make adjustments depending on customer 
preferences, previous shopping and browsing experience, demographic data, need, and usefulness. 
Collaborative or content-based data filtering is used in this process to gain useful insight which gives 
leverage to retailers on customers’ opinions.  
Companies often fail to understand what big data is, its benefits, and more importantly the computing and 
the human infrastructure required to realize its true potential. Without a clear understanding of the concept 
of big data, adopting and implementing a project using big data tools can seriously challenge its success. 
Having discussed various applications and use cases of implementing big data technologies in 
manufacturing, retail, supply chain, and logistics, it is important to understand the associated challenges. 
they decide to invest in technologies using big data.  
reducing their dependence on legacy systems. Even though the industry is shifting its focus to the digital 
age with adoption of IoT and artificial intelligence, it is still a long way before the full potential of big data 
is realized. Industry has to develop an awareness of the various elements of the big data landscape, which 
include sensors to social media that collect data, in-memory to cloud for data storage, data mining to deep 
learning to convert data into useful business insights or actions.  Any new business solution will involve a 
significantly. Most people are resistant to change, and it shows in companies when workers stick to to an 
old way of thinking and doing work. An example is the use of Excel, which to the present day remains one 
of the popular tools in many companies, despite having  many limitations when compared to newer tools.27 
While there is a need to educate industry to change this legacy mentality, there is no need for an abrupt or 
complete shift to newer tools. A viable option is to slowly augment existing systems with big data analytics 
tools and capabilities.   
With the phenomenal increase in the size of data, the problem of storage space for big data has become a 
real problem for many companies. Cloud storage is soon becoming the only viable alternative with the ever-
increasing need for storage space.  With the maturity of the cloud computing infrastructure, which includes 
storage, applications, and computing platforms, companies are beginning to consider shifting to the cloud 
infrastructure for most of their computing needs. But transitioning from the traditional in-house computing 
infrastructure to the cloud infrastructure has its own challenges.  According to McAfee, “Most organizations 
that have been around awhile have a hodgepodge of hardware, operating systems, and applications, often 
described as ‘legacy spaghetti’.”28 First, companies have to address legacy system issues and simplify their 
system before moving to the cloud. For the most part, cloud is cost-effective compared to building and 
running an IT infrastructure.  However, a company needs to carefully evaluate the cost factor based on their 
specific needs, for example, in-house applications requiring continuous transfer of large data sets. 
analytics with specialized MS degrees in Data Science.  These degree programs are housed mostly in 
business schools or computer science departments.  Engineering schools to a large extent are still lagging 
in providing adequate training in data science to their graduates.   Data science professionals can manage 
and analyze large volumes of real-time data coming from multiple sources and in different formats. With 
several new technologies such as the NoSQL data management framework, Hadoop, cloud computing, and 
in-memory analytics, their skills are vital for the rapidly changing computing landscape. Given that 
engineering schools are still looking for the right curriculum mix (e.g., minors, degree options, and 
15 
certificates) to train engineers in data science, training employees at entry level is a challenging and 
expensive proposition for companies dealing with these newer technologies. When industry hires data 
science professionals, akin to software developers and programmers, they need guidance from subject 
matter experts (SMEs) to build the right tools and techniques that can help industry harness the power of 
big data in the long-run.  Industry needs to quickly educate SMEs to understand the big data analytics 
professionals.   
As seen in recent times, data privacy has become one of the major concerns of organizations. With recent 
threats like hacking of personal data, individuals and companies have become apprehensive about linking 
data from multiple sources as it may compromise an individual’s privacy. Also, with an increase in the 
number of connected devices within the industry, data security has also become a big concern and presently 
this risk is greater than ever. Big data analysis uses huge amounts of data for analysis and mining purposes 
to reach some meaningful conclusion, and security of this big data can be enhanced by using techniques 
such as authentication, authorization, and encryption.   
Effective flow and sharing of information among supply chain partners is critical to the success of today’s 
digital supply chains.  Unauthorized disclosure and data leakage of information shared among supply chain 
partners have been identified as two main threats in today’s digital supply chains.29 Visibility needed within 
a supply chain and consumers’ demand for transparency seem to be at odds with security requirements.  
With newer, secure technologies such as blockchain and data cleanroom, it is possible to achieve both 
visibility and transparency.30  Data cleanroom is a shared environment between two or more supply chain 
visibility to their data. Blockchain, a decentralized, distributed database is one of the most secure options 
available for supply chain partners for real-time information tracking.  Another important, but often 
overlooked challenge is the ethical use of data.  The legal infrastructure has not kept up with the rapid 
development in technology, which is able to collect and store vast amounts of consumer data with or without 
their knowledge.  While it may be legal, certain use of the data may be considered unethical.  Such actions 
may have a negative impact on a company as today’s consumers are more educated and have experienced 
negative consequences of such unethical usage. 
In a recent survey of supply chain professionals conducted by APQC, “lack of people with the needed 
skills” was identified as the biggest barrier to advanced analytics applications in industry.31  In addition, 
these employees need “a good understanding of the business to provide solid advice.”29  Resistance to 
change and lack of access to data across disparate systems were the second and third biggest barriers, 
respectively.  In addition to lack of access to data, issues such as inconsistent and unorganized data are also 
issues in some cases as different companies record their data in different formats, platforms, and systems.27 
useful insights.  
As companies make a push for big data analytics applications, they should first establish a clear business 
need such as “solving a problem or seizing an opportunity.”7  According to Watson, “big data initiatives 
should start with a specific or narrowly defined set of objectives rather than a ‘build it and they will come’ 
approach.”7 Pilot schemes are a good way to demonstrate the value of big data analytics.32  It is common to 
focus the initial business case for big data analytics on customer-centric objectives.7 The various 
applications and uses cases discussed earlier cover many different areas that have benefited from big data 
analytics. Whatever be the area, it is desirable that the pilot project address a problem tied to a specific 
16 
business outcome.  The pilot project should not only help solve a business problem, but also demonstrate 
the effectiveness of big data analytics for the organization and its stakeholders. Finally, for successful big 
data initiatives it is essential to have strong, committed sponsorship and alignment between the business 
and analytics strategies.7 In the early stages of adoption, the sponsor could be the CIO and then shifting to 
function-specific executives as business opportunities are identified.   
To benefit from big data analytics companies must also establish a data-driven decision-making culture, 
which calls for acting on insights from data rather than on pure managerial intuition.32 Promotion of data-
sharing practices, increased availability of training in data analytics, and communication of the benefits of 
data-driven decision making are some of the strategies for promoting a data-drive culture.7 While workforce 
training needs to focus on improving technological and digital proficiency, the future work environment 
also demands training in certain soft skills.  The work environment is changing with the rapid introduction 
of AI, automation, and analytics-driven solutions.  Workers need to be open to new ways of working and 
have openness to agility, adaptability, and working in teams to cope with a constantly changing external 
environment.  In the long-run, big data needs to become an integral part of the organization’s operating 
model. There also needs to be clear ownership for big data in the organization with leadership positions 
such as a chief analytics officer.32 Data science should become another established skill in the organization.   
during the development of this white paper.  We would like convey our appreciation to Scott Wahl for his 
guidance and feedback during the formative stages of this effort.  We would also like to thank John 
Ashodian, John Hill, Ying Tat Leung, Juan Ma, Hari Padmanabhan, and John Paxton for carefully reading 
an earlier version of this white paper and providing several constructive suggestions and feedback, which 
have helped us greatly improve the quality of the white paper. 
17 
1. Morten Brinch, Jan Stentoft, and Jesper K. Jensen, “Big Data and its Applications in Supply Chain 
Management: Findings from a Delphi Study,” Proceedings of the 50th Hawaii International Conference 
on System Sciences, 2017: 1351-1360.  
2. IBM Corporation, “The Path to Data Veracity,” IBM Big Data and Analytics Hub, May 2018, 
https://www.ibmbigdatahub.com/whitepaper/path-data-veracity 
3. DataStax Corporation, “Big Data: Beyond the Hype,” October 2013, 
https://www.datastax.com/resources/whitepapers/bigdata 
4. Phillip Russom, “Big Data Analytics,” TDWI Research, 2011, 
https://tdwi.org/research/2011/09/best-practices-report-q4-big-data-
analytics.aspx?tc=page0&m=1  
5. DXC Technology Company, “Five Industries Where Big Data is Making a Difference,” November 
2015, https://assets1.dxc.technology/analytics/downloads/DXC-Analytics-
Five_Industries_Where_Big_Data_is_Making_a_Difference-4AA5-6292ENW.pdf 
6. Nada Elgendy and Ahmed Elragal, “Big Data Analytics: A Literature Review Paper,” In: Perner P. 
(eds) Advances in Data Mining. Applications and Theoretical Aspects. ICDM 2014. Lecture Notes in 
Computer Science, vol 8557, Springer, Cham., 2014, https://link.springer.com/chapter/10.1007/978-
3-319-08976-8_16  
7. Hugh J. Watson, "Tutorial: Big Data Analytics: Concepts, Technologies, and Applications," 
Communications of the Association for Information Systems, 34 (2014), Article 65. 
http://aisel.aisnet.org/cais/vol34/iss1/65  
8. Richard L. Villars, Carl W. Olofson, and Matthew Eastwood, “Big Data: What It Is and Why You 
Should Care,” International Data Corporation, 2011. 
http://www.tracemyflows.com/uploads/big_data/idc_amd_big_data_whitepaper.pdf 
9. Sunil Tiwari, H.M. Wee, and Yosef Daryanto, “Big Data Analytics in Supply Chain Management 
Between 2010 and 2016: Insights to Industries,” Computers and Industrial Engineering, 115 (2017): 319-
330.  
10. Bob Trebilcock, “Supply Chain, Data Analytics, and Big Data,” Logistics Management, August 2015.  
https://www.logisticsmgmt.com/article/supply_chain_data_analytics_and_big_data  
11. Kaushik Pal, “How Machine Learning Can Improve Supply Chain Efficiency,” Techopedia, February-
2018.  https://www.techopedia.com/2/31846/trends/big-data/how-machine-learning-can-improve-
supply-chain-efficiency  
12.  McKinsey & Company, “Big Data and the Supply Chain: The Big-Supply-Chain Analytics 
Landscape: Part 1,” February 2016,  https://www.mckinsey.com/business-functions/operations/our-
insights/big-data-and-the-supply-chain-the-big-supply-chain-analytics-landscape-part-1#  
13. Lorenzo Romano, “Big Data Analytics: A Key Ingredient for Agility in Manufacturing,” May 2019, 
https://www.orange-business.com/en/blogs/big-data-analytics-key-ingredient-agility-manufacturing  
18 
14. Joe McKendrick, “Walmart’s Gigantic Private Cloud for Real-Time Inventory Control,” RT 
Insights.com, January 2017. https://www.rtinsights.com/walmart-cloud-inventory-management-
real-time-data/  
15. RT Insights team, “Levi’s Real-Time Tracking of Jeans: RFID in Retail,” RT Insights.com, April 
2016. https://www.rtinsights.com/rfid-in-retail-customer-experience-levis/ 
16.  JDA, “Store Replenishment at Morrisons,” 2017, https://jda.com/knowledge-center/collateral/by-
morrisons-case-study  
17.  Hans W. Ittmann, “The Impact of Big Data and Business Analytics on Supply Chain Management,” 
Journal of Transport and Supply Chain Management, 9, no. 1 (2015). 
https://jtscm.co.za/index.php/jtscm/article/view/165/331  
18. Logivation, https://www.logivations.com/en/solutions/plan/design_efficiency.php  
19. RT Insights team, “Using Mobile Device for a Real-Time Warehouse,” 2016, 
https://www.rtinsights.com/zebra-omnii-xt15-datek-real-time-warehouse/  
 20. Motifworks, “How Big Data Analytics Can Benefit Supply Chain and Logistics Industry,” 2017. 
https://motifworks.com/2017/02/23/how-big-data-analytics-can-benefit-supply-chain-logistics-
industry/  
 21. “2017 Third-Party Logistics Study,” https://jda.com/-/media/jda/knowledge-center/thought-
leadership/2017stateoflogisticsreport_new.ashx  
 22. UPS, “ORION Backgrounder,” 2019, 
https://www.pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=Factsheet
s&id=1426321616277-282  
23. “Data-Driven Logistics: The Growing Use of Predictive Analytics,” July 2018, https://www.smith-
howard.com/data-driven-logistics-the-growing-use-of-predictive-analytics/ 
 24. Martin Jeske, Moritz Grüner, and Frank Weiẞ, “Big Data in Logistics – A DHL Perspective on How 
to Move Beyond the Hype,” December 2013. 
http://www.dhl.com/content/dam/downloads/g0/about_us/innovation/CSI_Studie_BIG_DATA.pdf  
25.  McKinsey & Company, “Big Data, Analytics, and the Future of Marketing and Sales,” March 2015, 
https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/Marketing%20and%20Sales/Our
%20Insights/EBook%20Big%20data%20analytics%20and%20the%20future%20of%20marketing%20sal
es/Big-Data-eBook.ashx 
26. Gurobi Optimization, “The Power of Analytics,” accessed September 8, 2019. 
http://www.gurobi.com/resources/prescriptive-analytics,  
27.  Transmetrics, “ Big Data and Big Roadblocks:  How the Logistics Industry can Overcome its Big 
Data Challenges,” March 2018, https://www.youredi.com/blog/logistics-industry-can-overcome-big-data-
28. Andrew McAfee, “What Every CEO Needs to Know About the Cloud,” Harvard Business Review, 
Nov. 2011: 124-132. 
19 
29. Bharat Bhargava, Rohit Ranchal, and Lotfi Ben Othmane, “Secure Information Sharing in Digital 
Supply Chains,” 3rd IEEE International Advanced Computing Conference, May 2013, 
https://www.cs.purdue.edu/homes/bb/Bhargava-Supply_Chain-Feb2013-india.pdf  
30. Megan Ray Nicholas, “How to Share Data Safely Across your Supply Chain,” 
https://www.smartdatacollective.com/share-data-safely-across-supply-chain/  
31. APQC, “APQC Quick Poll:  The Current State of Big Data & Advanced Analytics in Supply Chain,” 
May 2019, 
https://www.scmr.com/article/apqc_quick_poll_the_current_state_of_big_data_advanced_analytics_in_su
pply  
32. David Meer, “A Call to Action on Big Data,” Forbes, October 2014, 
https://www.forbes.com/sites/strategyand/2014/10/28/a-call-to-action-on-big-data/#6a4b6c22314  

Removed lines from KimAnh-HTKhoa.pdf:
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381804857
Tích Hợp Big Data Và Điện Toán Đám Mây: Động Lực Thúc Đẩy Thay Đổi Cho
Doanh Nghiệp.
Conference Paper · June 2024
0
376
1 author:
Vo Thi Kim Anh
Ton Duc Thang University
28 PUBLICATIONS   2 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Vo Thi Kim Anh on 29 June 2024.
The user has requested enhancement of the downloaded file.
KỶ YẾU
HỘI THẢO KHOA HỌC
KHOA CÔNG NGHỆ THÔNG TIN
LẦN 6
2024
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
i 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
KHOA CÔNG NGHỆ THÔNG TIN 
-------------- 
KỶ YẾU HỘI THẢO 
KHOA HỌC CÔNG NGHỆ LẦN 6 
Thành Phố Hồ Chí Minh, tháng 06 năm 2024 
(Lưu hành nội bộ) 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
ii 
BAN BIÊN TẬP 
1. TS. Nguyễn Hà Giang - Trưởng Khoa CNTT 
2. TS. Văn Thị Thiên Trang - Phó Trưởng Khoa CNTT 
3. ThS. Nguyễn Minh Tuấn - Phó Trưởng Khoa CNTT 
4. ThS. Trần Thành Công - Trợ lý Trưởng Khoa, Trưởng Ngành TMĐT 
5. ThS. Hoàng Văn Hiếu - Trưởng Ngành CNTT 
6. ThS. Võ Đình Ngà - Trưởng Ngành TKĐH 
7. ThS. Nguyễn Thị Hoài Linh - Trưởng Ngành KHDL 
8. ThS. Ngô Văn Công Bằng - Trưởng Bộ môn THUD 
9. ThS. Trương Nhã Bình - Trưởng Bộ môn Toán 
THƯ KÝ 
1. KS. Phạm Hữu Kỳ – Giảng viên Khoa CNTT 
2. Trần Thị Phương Anh – Thư ký Khoa CNTT 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iii 
LỜI GIỚI THIỆU 
Công nghệ thông tin đã và đang là yếu tố cốt lõi thúc đẩy nền kinh tế - xã hội phát triển 
mạnh mẽ, đặc biệt trong thời đại kỹ thuật số ngày nay. Sự bùng nổ của các công nghệ mới 
và ứng dụng tiên tiến đã thay đổi cách chúng ta sống, làm việc và tương tác. Với mục đích 
tạo ra một diễn đàn để các nhà nghiên cứu, học giả, giảng viên, cũng như các chuyên gia, 
trao đổi kết quả nghiên cứu, chia sẻ kiến thức, thảo luận quan điểm, ý tưởng về các xu 
hướng mới nhất trong lĩnh vực công nghệ thông tin và ứng dụng, Khoa Công nghệ thông 
tin, Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh (UEF) tổ chức hội thảo 
với chủ đề “Hội thảo khoa học công nghệ Khoa CNTT lần 6 năm 2024”.  
Hội thảo không chỉ nhằm mục đích nâng cao năng lực nghiên cứu mà còn thúc đẩy các 
phát minh, đổi mới và chuyển giao công nghệ trong lĩnh vực công nghệ thông tin. Đây là 
cơ hội để các chuyên gia đầu ngành, nhà nghiên cứu, giảng viên và sinh viên gặp gỡ, học 
hỏi và hợp tác, cùng nhau phát triển và ứng dụng các thành tựu khoa học kỹ thuật vào thực 
tiễn. Qua đó, hội thảo mong muốn góp phần nâng cao chất lượng giáo dục, nghiên cứu và 
thực hành trong lĩnh vực công nghệ thông tin. 
Do thời gian chuẩn bị có hạn, việc biên tập Kỷ yếu này không tránh khỏi những thiếu 
sót. Ban biên tập rất mong ý kiến đóng góp cũng như sự lượng thứ từ quý độc giả để các 
kỳ hội thảo sau được tổ chức ngày một tốt hơn, hiệu quả hơn 
Trân trọng! 
Tp. Hồ Chí Minh, tháng 6 năm 2024 
BAN BIÊN TẬP 
KHOA CÔNG NGHỆ THÔNG TIN 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
141-145 ĐIỆN BIÊN PHỦ, P.15, Q.BÌNH THẠNH, TP.HCM 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iv 
TỐI ƯU HÓA TRUY VẤN TRONG SQL SERVER: PHƯƠNG PHÁP VÀ ỨNG 
DỤNG..........................................................................................Trang 1 
Nguyễn Minh Tuấn - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KÊ CỦA CÔNG CỤ CHATGPT.......................................................Trang 14 
Nguyễn Văn Vinh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
RANSOMWARE: MỐI ĐE DỌA TRONG THỜI ĐẠI SỐ........................Trang 24 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
CHỮ KÝ.....................................................................................Trang 29 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC THÚC ĐẨY 
THAY ĐỔI CHO DOANH NGHIỆP.................................................Trang 35 
Võ Thị Kim Anh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
NHÂN CHO SINH VIÊN NGÀNH THIẾT KẾ ĐỒ HỌA.........................Trang 44 
Võ Đình Ngà - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
ỨNG DỤNG BÀI TOÁN VẬN TẢI: TỐI ƯU CHI PHÍ THU GOM RÁC SINH 
HOẠT CỦA CÁC BỆNH VIỆN..........................................................Trang 59 
Trương Nhã Bình - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
Ngô Thuận Dủ - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
DOANH CỦA DOANH NGHIỆP.......................................................Trang 70 
Hoàng Văn Hiếu - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
SÁNG TẠO NỘI DUNG AI: CÁCH MẠNG HÓA TƯƠNG LAI CỦA TIẾP THỊ NỘI 
DUNG .......................................................................................Trang 85 
Trần Thành Công - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 35 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC 
THÚC ĐẨY THAY ĐỔI CHO DOANH NGHIỆP. 
INTEGRATION OF BIG DATA AND CLOUD COMPUTING: A 
1Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh, anhvtk@uef.edu.vn  
Tóm tắt: Kỷ nguyên số mang đến sự bùng nổ dữ liệu, tạo ra cả thách thức và cơ hội cho doanh nghiệp. 
Sự hội tụ của Big Data và điện toán đám mây nổi lên như giải pháp mạnh mẽ, cách mạng hóa cách 
thức xử lý và khai thác dữ liệu. Bài viết này khám phá tác động biến đổi của sự kết hợp này, đồng thời 
đề xuất những cân nhắc thực tế cho doanh nghiệp bắt đầu áp dụng Big Data trên nền tảng đám mây. 
Từ khóa: Kỷ nguyên số, Big Data, điện toán đám mây, biến đổi, doanh nghiệp. 
Abstract: The digital era has ushered in an unprecedented surge of data, presenting both challenges 
and opportunities for businesses. The convergence of big data and cloud computing has emerged as a 
powerful solution, revolutionizing the way data is processed and harnessed. This paper delves into the 
embarking on their big data on cloud journey. 
Key words: Digital Era, Big Data, Cloud Computing, Transformation, Business 
1. Sự kết hợp mạnh mẽ giữa Big Data và 
liệu, mang đến cả thách thức và cơ hội cho 
doanh nghiệp. Khái niệm Big Data, với đặc 
trưng khối lượng, tốc độ và sự đa dạng, lần 
đầu tiên được giới thiệu bởi Laney (2001) [1] 
và khai thác thông tin. Tuy nhiên, việc quản 
minh là rất phức tạp. 
Sự xuất hiện của điện toán đám mây [2] 
Data, cung cấp giải pháp mạnh mẽ để giải 
quyết thách thức này. Điện toán đám mây 
internet, giúp doanh nghiệp tận dụng tối đa 
linh hoạt. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 36 
Dikaiakos et al. (2009) [3] nhấn mạnh về khả 
năng mở rộng, hiệu quả chi phí và khả năng 
truy cập. 
Đối với khả năng mở rộng: cơ sở hạ tầng 
trên nhu cầu xử lý, loại bỏ nhu cầu đầu tư ban 
đầu tốn kém vào phần cứng. Doanh nghiệp chỉ 
cần trả tiền cho các tài nguyên họ sử dụng, tối 
tức đầu tư [3]. 
Về hiệu quả về chi phí: doanh nghiệp chỉ 
trả tiền cho các tài nguyên họ sử dụng, tối ưu 
đầu tư [3]. 
Còn đối với khả năng truy cập: các giải 
năng truy cập mọi lúc, mọi nơi, thúc đẩy cộng 
tác và sự linh hoạt. 
động, phát triển sản phẩm mới, gia tăng lợi thế 
doanh đầy biến động (xem thống kê Bảng 1).
Bảng 1: Lợi ích của Big Data và Điện toán đám mây  
Tự động mở rộng/thu hẹp tài nguyên, tối ưu hóa chi phí. 
Chỉ trả tiền cho tài nguyên sử dụng. 
Truy cập mọi lúc, mọi nơi, thúc đẩy cộng tác. 
Tự động hóa quy trình, cải thiện ra quyết định, tối ưu hóa 
chuỗi cung ứng. 
Xác định xu hướng thị trường và nhu cầu khách hàng. 
Đưa ra quyết định sáng suốt và nhanh chóng dựa trên dữ liệu. 
Phân tích dữ liệu để dự đoán rủi ro và nắm bắt cơ hội mới. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 37 
2. Ứng dụng thực tiễn của Big Data  
giới thực. Các doanh nghiệp đang tận dụng 
động, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Dưới đây là một số ví dụ cụ 
thể:  
Trước tiên, đó là ở ngành bán lẻ: Các gã 
nền tảng đám mây để quản lý hàng tồn kho, 
thông tin chi tiết về khách hàng [4]. Ví dụ, câu 
chuyện về Amazon retail (Amazon.com). 
Ngày 10 tháng 11 năm 2010 là ngày 
Amazon.com tắt máy chủ web vật lý cuối 
cùng 
trong 
trung 
tâm 
dữ 
liệu 
của 
Amazon.com ([17]). Amazon.com là nhà bán 
lẻ trực tuyến lớn nhất thế giới. Do 
Amazon.com tạo ra rất nhiều dữ liệu, trước 
trữ dữ liệu đó. Nhưng khi Amazon.com phát 
triển lớn hơn, kích thước cơ sở dữ liệu Oracle 
cùng khó khăn. Điều này khiến họ phải cân 
AWS. Bằng cách chuyển sang AWS, họ đã 
trải nghiệm cải thiện hiệu suất gấp 12 lần và 
giảm thời gian khôi phục từ khoảng 15 giờ 
xuống 2,5 giờ ([18]). Amazon.com đã vượt 
qua chi phí cao, hiệu suất chậm và quản lý tốn 
AWS. Họ tận dụng Amazon S3 vì tính tiết 
kiệm chi phí, khả năng mở rộng, bảo mật và 
lưu trữ bền vững, giúp sao lưu và khôi phục 
nhanh hơn đáng kể. Ngoài ra, việc di chuyển 
mạch. Nhìn chung, việc chuyển sang AWS 
giúp giảm chi phí, cải thiện hiệu quả và cung 
phát triển của Amazon (Bảng 3, 4). 
Trong ngành chăm sóc sức khỏe: Ngành 
Data. Nghiên cứu của [5] cho thấy các tổ chức 
mây để phân tích dữ liệu bệnh nhân, từ đó cải 
sáng kiến nghiên cứu. Ví dụ, Mayo Clinic sử 
điều trị mới, chẩn đoán bệnh chính xác hơn và 
cải thiện hiệu quả chăm sóc.  
Và trong ngành dịch vụ tài chính: Phân 
chính. Các nghiên cứu điển hình của [6] cho 
để xác định các giao dịch gian lận, đánh giá 
rủi ro tín dụng và quản lý danh mục đầu tư. Ví 
dụ, JPMorgan Chase sử dụng Big Data để 
phát hiện các trường hợp rửa tiền, ngăn chặn 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 38 
đầu tư.  
vực. Các giải pháp Big Data dựa trên nền tảng 
doanh đầy biến động. Bảng 2 sau đây thống 
kê các ứng dụng:
Bảng 2: Ứng dụng thực tiễn của Big Data 
Quản lý hàng tồn kho, cá nhân hóa 
chiến dịch tiếp thị, thu thập thông tin 
khách hàng, đề xuất sản 
[4, 18] 
Phân tích dữ liệu bệnh nhân, cải thiện 
chất lượng chăm sóc, thúc đẩy nghiên 
cứu 
phương pháp điều trị mới, 
[5] 
Quản lý rủi ro, phát hiện gian lận, 
đánh giá rủi ro tín dụng, quản lý danh 
rửa tiền, ngăn chặn gian lận 
thẻ tín dụng, tối ưu hóa danh 
[6] 
Bảng 3: Bảng so sánh Lưu trữ truyền thống vs Lưu trữ đám mây Amazon S3 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Chi phí trả trước cao cho phần cứng băng, dung 
lượng trung tâm dữ liệu và giấy phép phần mềm. 
Mô hình trả tiền theo nhu cầu, loại 
bỏ chi phí trả trước. 
liệu ngày càng tăng. 
của Amazon. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 39 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Sao lưu và phục hồi chậm do thời gian đọc băng. 
kể so với băng. 
liệu, dễ bị lỗi phần cứng. 
11 số chín (99.999999999%). 
tầng băng. 
cầu quản lý tối thiểu. 
Bảng 4: Bảng so sánh Máy chủ cục bộ vs AWS EC2 đám mây 
Máy chủ On-premises 
trung tâm dữ liệu cục bộ. 
chuyên dụng để giao tiếp liền mạch. 
máy chủ web, cơ sở dữ liệu và 
các công cụ. 
AWS. 
3. Giải quyết thách thức và triển khai hiệu 
toán đám mây mang lại nhiều lợi ích, nó cũng 
thận. Bảo mật dữ liệu là một trong những 
mối quan tâm hàng đầu. Pearson (2013) [7] 
vệ dữ liệu nhạy cảm trên đám mây. Các biện 
pháp này bao gồm: mã hóa dữ liệu, kiểm soát 
quyền truy cập, và tuân thủ các quy định. 
truy cập trái phép. Kiểm soát quyền truy cập 
vào dữ liệu và mức độ truy cập của dữ liệu đó. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 40 
Đối với việc tuân thủ các quy định, như là, 
bảo mật dữ liệu, chẳng hạn như quy định về 
bảo vệ dữ liệu (GDPR) của Liên minh châu 
Âu. 
pháp Big Data trên nền tảng đám mây. 
Achanta (2023) [8] và Setiyawan & Patel 
(2019) [9] đã nêu bật tầm quan trọng của 
việc: chất lượng dữ liệu, và tuân thủ quy định. 
việc xem xét dữ liệu tính chính xác, đầy đủ 
và nhất quán để có thể phân tích hiệu quả. 
quản lý dữ liệu, chẳng hạn như Sarbanes-
Oxley Act (SOX) của Hoa Kỳ.  
Ngoài ra, còn có một số thách thức khác 
Big Data trên nền tảng đám mây, bao gồm: sự 
tương tác, kỹ năng nhân sự, và chi phí triển 
khai – vận hành. Về khả năng tương tác, thì 
doanh nghiệp. Đối với các kỹ năng, thì doanh 
đám mây. Còn lại, đối với quản lý chi phí, thì 
toán đám mây. 
thức và cân nhắc này, các doanh nghiệp có thể 
tranh (Bảng 3). 
Bảng 5: Giải quyết thách thức và triển khai hiệu quả Big Data dựa trên điện toán đám mây 
Mã hóa mạnh mẽ, kiểm soát quyền truy 
cập, tuân thủ quy định 
[7] 
Đảm bảo chất lượng dữ liệu, tuân thủ 
[8, 9] 
[10] 
[11, 12] 
[13, 14] 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 41 
4. Tương lai của việc ra quyết định dựa 
mây. Các xu hướng mới nổi như điện toán 
lý và sử dụng dữ liệu. 
Theo Banjanovic & Husaković (2023) 
[15], điện toán biên tích hợp phân tích Big 
liệu thời gian thực tại ranh giới của mạng. 
nguồn khác nhanh chóng và hiệu quả hơn. 
McGrath & Brenner (2017) [16] cho rằng 
liệu. Nhờ vậy, doanh nghiệp có thể thúc đẩy 
đổi mới và tăng trưởng nhanh hơn. 
Sự kết hợp của Big Data, điện toán đám 
tâm trong việc ra quyết định, đổi mới và tăng 
trưởng (Bảng 4). Doanh nghiệp cần nắm bắt 
đại dữ liệu. 
Bảng 6: Tương lai của việc ra quyết định dựa trên dữ liệu 
biên 
mạng 
liệu nhanh chóng, hiệu quả 
[15] 
chủ 
[16] 
năng lưu trữ, xử lý và phân tích dữ liệu mạnh 
mẽ. Nhờ đó, doanh nghiệp có thể nâng cao 
hiệu quả hoạt động, hiểu rõ hơn về khách 
hàng, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Việc nắm bắt sức mạnh của Big 
số. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 42 
mây hiệu quả, doanh nghiệp cần xác định rõ 
mục tiêu, lựa chọn giải pháp phù hợp, xây 
dựng chiến lược quản trị dữ liệu, đầu tư đào 
án nhỏ đến mở rộng dần. Việc áp dụng thành 
doanh nghiệp thành công trong kỷ nguyên số. 
hiệu quả, doanh nghiệp có thể khai thác sức 
đạt được những lợi ích to lớn. 
[1] Laney, D. (2001) 3D Data Management: 
Controlling Data Volume, Velocity and Variety. 
META Group Research Note, 6. 
[2] Armbrust, M., Griffith, R., Joseph, A. D., Katz, 
R., Konwinski, A., Lee, G., Patterson, D., et al. 
(2010). 
A 
view 
of 
computing. 
Communications of the ACM, 53(4), 50-58. 
ACM. 
[3] Dikaiakos, M., Katsaros, D., Mehra, P., Pallis, G., 
& Vakali, A. (2009). “Cloud computing: 
scientific research”. IEEE Internet Computing, 
13(5), 10-13. 
[4] Chen, W., Li, J., & Jin, X. J. (2016). The 
replenishment policy of agri-products with 
stochastic demand in integrated agricultural 
supply chains. Expert Systems with Applications, 
48, 55-66. 
[5] Halamka, J. (2014). The Argonaut Project 
Charter. Life as a Healthcare CIO. 
[6] Rizvi, S. (2021). Role of big data in financial 
institutions for financial fraud. SSRN Electronic 
Journal, 4, 35. 
[7] Pearson, S. (2013). Privacy, Security and Trust in 
Cloud Computing. In: Pearson, S., Yee, G. (eds) 
Privacy and Security for Cloud Computing. 
and 
Networks. 
Springer, London. https://doi.org/10.1007/978-1-
4471-4189-1_1  
[8]  Achanta, M. (2023). Data governance in the age of 
cloud computing: Strategies and considerations. 
(IJSR), 12, 1338-1343. 
[9]  Setiyawan, D., & Patel, C. (2019). A proposed 
and data management in higher education. SSRN 
Electronic Journal, 6, 19-25. 
[10] Agrawal, D. & Das, S. & Abbadi, A. (2011). Big 
Data and Cloud Computing: Current State and 
Opportunities. 
ACM 
Series. 
530-533. 
10.1145/1951365.1951432. 
[11] Ghaleb, E.A.A.; Dominic, P.D.D.; Fati, S.M.; 
Muneer, A.; Ali, R.F. 2021. The Assessment of Big 
Data Adoption Readiness with a Technology–
Organization–Environment 
Framework: 
A 
Employees. 
2021, 
13, 
8379. 
https://doi.org/10.3390/su13158379 
[12] Shamim, S., Zeng, J., Choksy, U.S. & Shariq, S. 
M. 2020. Connecting big data management 
employee level, International Business Review, 
Volume 29, Issue 6, 101604, ISSN 0969-5931, 
https://doi.org/10.1016/j.ibusrev.2019.101604. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 43 
[13] Muniswamaiah, M., Agerwala, T. & Tappert, C. 
(2019). Big data in cloud computing review and 
opportunities.  International Journal of Computer 
Science & Information Technology (IJCSIT) Vol 
11, 
No 
4. 
https://arxiv.org/ftp/arxiv/papers/1912/1912.108
21.pdf 
[14] El-Seoud, S. A., El-Sofany, H. F., Abdelfattah, 
M. A. F., & Mohamed, R. (2017). Big Data and 
Cloud Computing: Trends and Challenges. 
(iJIM), 
11(2), 
pp. 
34–52. 
https://doi.org/10.3991/ijim.v11i2.6561 
[15] Banjanovic, M. L., & Husaković, A. (2023). Edge 
AI: Reshaping the Future of Edge Computing 
with 
Intelligence. 
10.5644/PI2023.209.07. 
[16] McGrath, G., & Brenner, P. R. (2017). 
"Serverless 
Computing: 
Design, 
Implementation, and Performance," 2017 IEEE 
Computing Systems Workshops (ICDCSW), 
Atlanta, GA, USA, 2017, pp. 405-410, doi: 
10.1109/ICDCSW.2017.36. 
[17] [Amazon Web Services]. (2012, December 10). 
AWS re: Invent ENT 205- Drinking Our Own 
[Video]. 
Https://www.Youtube.com/User/AmazonWebSe
rvices/Cloud 
. 
https://www.youtube.com/watch?v=f45Uo5rw6
YY  
[18] Chavan, A. (2020, September 21). How Amazon 
retail (Amazon.Com) uses the AWS cloud. 
Medium. 
June 
7, 
2024, 
from 
https://ankush-chavan.medium.com/how-
amazon-retail-amazon-com-uses-the-aws-cloud-
View publication stats

Removed lines from sybca-bigdata-ppt.pdf:
Introduction to Big Data
What is Data?
The quantities, characters, or symbols on which operations are performed by a computer, 
magnetic, optical, or mechanical recording media.
What is Big Data?
Big Data is also data but with a huge size. Big Data is a term used to describe a 
collection of data that is huge in volume and yet growing exponentially with time. In 
tools are able to store it or process it efficiently.
“Extremely large data sets that may be analyzed computationally to reveal patterns , 
trends and association, especially relating to human behavior and interaction are 
known as Big Data.”

Following are some the examples of Big Data-
The New York Stock Exchange generates about one terabyte of new trade data per day.
Social Media
The statistic shows that 500+terabytes of new data get ingested into the databases of social 
media site Facebook, every day. This data is mainly generated in terms of photo and video 
uploads, message exchanges, putting comments etc.
A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many 
thousand flights per day, generation of data reaches up to many Petabytes.
Name
Size(In Bytes)
Bit
1/8
1/2 (rare)
Byte
1
1024 bytes
1024
1, 024kilobytes
1, 048, 576
1, 024 megabytes
1, 073, 741, 824
1, 024 gigabytes
1, 099, 511, 627, 776
1, 024 terrabytes
1, 125, 899, 906, 842, 624
1, 024 petabytes
1, 152, 921, 504, 606, 846, 976
1, 024 exabytes
1, 180, 591, 620, 717, 411, 303, 424
1, 024 zettabytes
1, 208, 925, 819, 614, 629, 174, 706, 176
Characteristics Of Big Data
•
The following are known as “Big Data Characteristics”.
1. Volume
2. Velocity
3. Variety
4. Veracity
1. Volume:
Volume means “How much Data is generated”. Now-a-days, 
very vast amount of Data say TB(Tera Bytes) to PB(Peta Bytes) to Exa
Byte(EB) and more.
2. Velocity:
Velocity means “How fast produce Data”. Now-a-days, Organizations or 
fast rate.
3. Variety:
Variety means “Different forms of Data”. Now-a-days, Organizations or 
rate in different formats. We will discuss in details about different formats of 
Data soon.
4. Veracity
Veracity means “The Quality or Correctness or Accuracy of Captured Data”. 
Out of 4Vs, it is most important V for any Big Data Solutions. Because without 
Correct Information or Data, there is no use of storing large amount of data at 
fast rate and different formats. That data should give correct business value.
Types of Digital Data
1. Structured
2. Unstructured
3. Semi-structured
Structured

Any data that can be stored, accessed and processed in the form of fixed format is 
termed as a 'structured' data. 

Over the period of time, talent in computer science has achieved greater success in 
developing techniques for working with such kind of data (where the format is well 
known in advance) and also deriving value out of it.

However, nowadays, we are foreseeing issues when a size of such data grows to a huge 
extent, typical sizes are being in the range of multiple zettabytes.
Do you know? 1021 bytes equal to 1 zettabyte or one billion terabytes forms a zettabyte.
given and imagine the challenges involved in its storage and processing.
Do you know? Data stored in a relational database management system is one 
example of a 'structured' data.
• Examples Of Structured Data
An 'Employee' table in a database is an example of Structured Data
2365
Male
650000
3398
650000
7465
Male
500000
7500
Male
500000
7699
550000
Unstructured

Any data with unknown form or the structure is classified as unstructured data.

In addition to the size being huge, un-structured data poses multiple challenges in terms 
of its processing for deriving value out of it.

combination of simple text files, images, videos etc. 

Now day organizations have wealth of data available with them but unfortunately, they 
don't know how to derive value out of it since this data is in its raw form or unstructured 
format.
• Examples Of Un-structured Data
The output returned by 'Google Search'
Semi-structured

Semi-structured data can contain both the forms of data. 

We can see semi-structured data as a structured in form but it is actually not defined 
with e.g. a table definition in relational DBMS.

Example of semi-structured data is a data represented in an XML file.
Examples Of Semi-structured Data
Personal data stored in an XML file-
<rec><name>Prashant Rao</name><sex>Male</sex><age>35</age></rec>
<rec><name>Seema R.</name><sex>Female</sex><age>41</age></rec>
<rec><name>Satish Mane</name><sex>Male</sex><age>29</age></rec>
<rec><name>Subrato Roy</name><sex>Male</sex><age>26</age></rec>
<rec><name>Jeremiah J.</name><sex>Male</sex><age>35</age></rec>
Big Data Analytics
Big Data Analytics: 

Big Data analytics is the process of collecting, organizing and analyzing 
large sets of data (called Big Data) to discover patterns and other useful 
information.

that is most important to the business and future business decisions. 
from analyzing the data.
High-Performance Analytics Required: 

To analyze such a large volume of data, Big Data analytics is typically 
performed using specialized software tools and applications for predictive 
analytics, data mining, text mining, forecasting and data optimization. 

high-performance analytics.

large volumes of data that a business has collected to determine which data is 
relevant and can be analyzed to drive better business decisions in the future.
The Challenges:

For most organizations, Big Data analysis is a challenge. Consider the sheer 
volume of data and the different formats of the  
data(both structured and unstructured data) that is collected across the entire 
combined, contrasted and analyzed to find patterns and other useful business 
information.

organization stores in different places and often in different systems. 

easily as structured data. 

This massive volume of data is typically so large that it's difficult to process 
using traditional database and software methods.
How Big Data Analytics is Used Today:

data improves, business can be transformed in all sorts of ways. 

Today's advances in analyzing big data allow researchers to decode human DNA in 
minutes, predict where terrorists plan to attack, determine which gene is mostly likely 
to be responsible for certain diseases and, of course, which ads you are most likely to 
respond to on Facebook.

Another example comes from one of the biggest mobile carriers in the world.

France's Orange launched its Data for Development project by releasing subscriber 
data for customers in the Ivory Coast.

The 2.5 billion records, which were made anonymous, included details on calls and 
text messages exchanged between 5 million users.

as the foundation for development projects to improve public health and safety.

cell phone data to map where people went after emergencies; another showed how to 
use cellular data for disease containment. (source)
The Benefits of Big Data Analytics:

data. Many big data projects originate from the need to answer specific 
business questions. With the right big data analytics platforms in place, an 
enterprise can boost sales, increase efficiency, and improve operations, 
customer service and risk management.

Webopedia parent company, QuinStreet, surveyed 540 enterprise decision-
companies plan to use Big Data analytics to improve operations. About half 
of all respondents said they were applying big data analytics to improve 
customer retention, help with product development and gain a competitive 
advantage.

Notably, the business area getting the most attention relates to increasing 
efficiency and optimizing operations. Specifically, 62 percent of respondents 
said that they use big data analytics to improve speed and reduce complexity.
Application of Big Data 
Here is the list of top Big Data applications in today’s world:
•
•
•
Big Data in E-commerce
•
•
•
•
•
Let’s discuss the applications of Big Data in detail.
1. Big Data in Retail

The retail industry is the one that faces the most fierce competition of all. Retailers 
constantly hunt for ways that will give them a competitive edge over others. 
Customers are the real king sounds legit for the retail industry in particular.

For retailers to thrive in this competitive world, they need to understand their 
customers in a better way. If they are aware of their customers’ needs and how to 
fulfill those needs in the best possible way, then they know everything.

– Big Data in Retail.

Through advanced analysis of their customer’s data, retailers are now able to 
understand them from every angle possible. They gather this data from various 
sources such as social media, loyalty programs, etc.

Even a minute detail about any customer has now become significant for them. They are 
now closer to their customers than they have ever been. This empowers them to provide 
customers with more personalized services and predict their demands in advance.

This helps them in building a loyal customer base. Some of the biggest names in the retail 
world like Walmart, Sears and Holdings, Costco, Walgreens, and many more now have Big 
Data as an integral part of their organizations.

are responsible for as much as 30% of retail annual sales.
2. Big Data in Healthcare

Big Data and healthcare are an ideal match. It complements the healthcare industry better 
than anything ever will. The amount of data the healthcare industry has to deal with is 
unimaginable.
Gone are the days when healthcare practitioners were incapable of harnessing this data. 
From finding a cure to cancer to detecting Ebola and much more, Big Data has got it all 
under its belt and researchers have seen some life-saving outcomes through it.

medications. Data analysts are harnessing this data to develop more and more effective 
treatments. Identifying unusual patterns of certain medicines to discover ways for 
developing more economical solutions is a common practice these days.

Explore how Big Data helps to speed up the treatment process – Big Data in 
Healthcare.

people of all age groups. This generates massive amounts of real-time data in the 
form of alerts which helps in saving the lives of the people.
3. Big Data in Education

When you ask people about the use of the data that an educational institute gathers, the 
need it for future references.

Even you had the same perception about this data, didn’t you? But the fact is, this data 
holds enormous importance. Big Data is the key to shaping the future of the people and 
has the power to transform the education system for better.

curriculum. Additionally, universities can even track the dropout rates of the students 
and are taking the required measures to reduce this rate as much as possible.
4. Big Data in E-commerce

One of the greatest revolutions this generation has seen is that of E-commerce. It is now part 
and parcel of our routine life. Whenever we need to buy something, the first thought that 
provokes our mind is E-commerce. And not your surprise, Big Data has been the face of it.

Some of the biggest E-commerce companies of the world like Amazon, Flipkart, Alibaba, and 
popularity Big Data has gained in recent times.

Big Data is now as important as anyone else in these organizations. Amazon, the biggest E-
commerce firm in the world and one of the pioneers of Big Data and analytics, has Big Data as 
the backbone of its system. Flipkart, the biggest E-commerce firm in India, has one of the most 
robust data platforms in the country.

See how Flipkart used Big Data to have one of the most robust data platforms.

Big Data’s recommendation engine is one of the most amazing applications the Big Data world 
has ever witnessed. It furnishes the companies with a 360-degree view of its customers.

Companies then suggest customers accordingly. Customers now experience more personalized 
services than they have ever had. Big Data has completely redefined people’s online shopping 
experiences.
5. Big Data in Media and Entertainment

sheer piece of art. Art and science are often considered to be the two completely 
contrasting domains but when employed together, they do make a deadly duo and Big 
Data’s endeavors in the media industry are a perfect example of it.

Viewers these days need content according to their choices only. Content that is 
relatively new to what they saw the previous time. Earlier the companies 
broadcasted the Ads randomly without any kind of analysis.

But after the advent of Big Data analytics in the industry, companies now are 
aware of the kind of Ads that attracts a customer and the most appropriate time to 
broadcast it for seeking maximum attention.

Customers are now the real heroes of the Media and entertainment industry -
courtesy to Big Data and Analytics.
6. Big Data in Finance

data is one of the toughest challenges any financial firm faces. Data has been the second most 
important commodity for them after money.

Even before Big Data gained popularity, the finance industry was already conquering the 
technical field. In addition to it, financial firms were among the earliest adopters of Big Data 
and Analytics.

has been at the heart of it. Big Data is bossing the key areas of financial firms such as fraud 
detection, risk analysis, algorithmic trading, and customer contentment.

This has brought much-needed fluency in their systems. They are now empowered to focus 
more on providing better services to their customers rather than focussing on security issues. 
Big Data has now enhanced the financial system with answers to its hardest of the challenges.
7. Big Data in Travel Industry

with it, the travel industry was a bit late to realize its worth. Better late than never though. 
Having a stress-free traveling experience is still like a daydream for many.

And now Big Data’s arrival is like a ray of hope, that will mark the departure of all the 
hindrances in our smooth traveling experience.
See how Big Data is revolutionizing the travel & tourism sector.

Through Big Data and analytics, travel companies are now able to offer more 
customized traveling experience. They are now able to understand their customer’s 
requirements in a much-enhanced way.

From providing them with the best offers to be able to make suggestions in real-time, 
Big Data is certainly a perfect guide for any traveler. Big Data is gradually taking the 
window seat in the travel industry.
8. Big Data in Telecom

The telecom industry is the soul of every digital revolution that takes place around the world. 
With the ever-increasing popularity of smartphones, it has flooded the telecom industry with 
massive amounts of data.

And this data is like a goldmine, telecom companies just need to know how to dig it properly. 
Through Big Data and analytics, companies are able to provide the customers with smooth 
connectivity, thus eradicating all the network barriers that the customers have to deal with.
Companies now with the help of Big Data and analytics can track the areas with the lowest as 
well as the highest network traffics and thus doing the needful to ensure hassle-free network 
connectivity.
Big Data alike other industries have helped the telecom industry to understand its customers 
pretty well. 
Telecom industries now provide customers with offers as customized as possible.

Big Data has been behind the data revolution we are currently experiencing.
9. Big Data in Automobile

“A business like an automobile, has to be driven, in order to get results.” B.C. Forbes

smoothly. Big Data is driving the automobile industry towards some unbelievable and never 
before results.

wings to it. Big Data has helped the automobile industry achieve things that were beyond our 

From analyzing the trends to understanding the supply chain management, from taking care 
of its customers to turning our wildest dream of connected cars a reality, Big Data is well 
and truly driving the automobile industry crazy.

Removed lines from TNG_QUAN_V_D_LIU_LN_BIGDATA.pdf:
1 
TỔNG QUAN Vӄ DӲ LIӊU LӞN (BIGDATA) 
Ks. Nguyễn Công Hoan 
Trung Tâm Thông tin Khoa học thống kê (Viện KHTK) 
Trước đây, chúng ta mới chỉ biết đến dữ liệu có cấu trúc (structure data), ngày 
nay, với sự kết hợp của dữ liệu và internet, đã xuất hiện một dạng khác của dữ liệu -  Big 
data (dịch là “dữ liệu lớn”). Dữ liệu này có thể từ các nguồn như: hồ sơ hành chính,giao 
dịch điện tử, dòng trạng thái (status), chia sẻ hình ảnh, bình luận, nhắn tin...của chính 
chúng ta, nói cách khác chúng là dữ liệu được sản sinh qua quá trình chia sẻ thông tin 
trực tuyến liên tục của người sử dụng. Để cung cấp cái nhìn tổng quan, chúng tôi xin giới 
liệu lớn mang lại.    
1.  Khái niӋm, đặc trưng của dӳ liӋu lӟn và sự khác biӋt vӟi dӳ liӋu truyӅn thống 
1.1. Khái niệm về dữ liệu lớn 
- Theo wikipedia: Big data là một thuật ngữ chỉ bộ dữ liệu lớn hoặc phức tạp mà các 
phương pháp truyền thống không đӫ các ứng dөng để xử lỦ dữ liệu này. 
-  Theo Gartner: Dữ liệu lớn là những nguồn thông tin có đặc điểm chung khối lượng lớn,  
tốc độ nhanh và dữ liệu định dạng dưới nhiều hình thức khác nhau, do đó muốn khai thác 
được  đòi hỏi phải có hình thức xử lỦ mới để đưa ra quyết định, khám phá và tối ưu hóa 
quy trình.  
1.2. Nguồn hình thành dữ liệu và phương pháp khai thác và quản lý dữ liệu lớn. 
Qua thống kê và tổng hợp, nguồn dữ liệu lớn được hình thành chӫ yếu từ 6 nguồn: 
(1) Dữ liệu hành chính (phát sinh từ chương trình cӫa một tổ chức, có thể là chính phӫ 
hay phi chính phӫ). Ví dө, hồ sơ y tế điện tử ở bệnh viện, hồ sơ bảo hiểm, hồ sơ ngân 
hàng...; (2) Dữ liệu từ hoạt động thương mại (phát sinh từ các giao dịch giữa hai thực 
thể). Ví dө, các giao dịch thẻ tín dөng, giao dịch trên mạng, bao gồm cả từ các thiết bị di 
động; (3) Dữ liệu từ các thiết bị cảm biến như thiết bị chөp hình ảnh vệ tinh, cảm biến 
đường, cảm biến khí hậu; (4) Dữ liệu từ các thiết bị theo dõi, ví dө theo dõi dữ liệu từ 
điện thoại di động, GPS; (5) Dữ liệu từ các hành vi, ví dө như tìm kiếm trực tuyến về 
(một sản phẩm, một dịch vө hay thông tin khác), đọc các trang mạng trực tuyến...; (6) Dữ 
liệu từ các thông tin về  Ủ kiến, quan điểm cӫa các cá nhân, tổ chức, trên các phương tiện 
thông tin xã hội. 
theo các nguồn hình thành dữ liệu lớn. Mỗi nguồn dữ liệu lớn khác nhau sẽ có phương 
pháp khai thác và quản lỦ dữ liệu lớn khác nhau. Tuy nhiên, hiện nay phần lớn các tổ 
dữ liệu lớn. 
1.3. Đặc trưng 5V cͯa dữ liệu lớn 
Dữ liệu lớn có 5 đặc trưng cơ bản như sau (mô hình 5V):   
(1) Khối lượng dữ liệu (Volume)  
2 
Đây là đặc điểm tiêu biểu nhất cӫa dữ liệu lớn, khối lượng dữ liệu rất lớn. Kích cỡ 
cӫa Big Data đang từng ngày tăng lên, và tính đến năm 2012 thì nó có thể nằm trong 
khoảng vài chөc terabyte cho đến nhiều petabyte (1 petabyte = 1024 terabyte) chỉ cho 
một tập hợp dữ liệu. Dữ liệu truyền thống có thể lưu trữ trên các thiết bị đĩa mềm, đĩa 
cứng. Nhưng với dữ liệu lớn chúng ta sẽ sử dөng công nghệ “đám mây” mới đáp ứng khả 
năng lưu trữ được dữ liệu lớn. 
 (2) Tốc độ (Velocity) 
Tốc độ có thể hiểu theo 2 khía cạnh: (a) Khối lượng dữ liệu gia tăng rất nhanh (mỗi 
giây có tới 72.9 triệu các yêu cầu truy cập tìm kiếm trên web bán hàng cӫa Amazon); (b) 
Xử lỦ dữ liệu nhanh ở mức thời gian thực (real-time), có nghĩa dữ liệu được xử lỦ ngay 
tức thời ngay sau khi chúng phát sinh (tính đến bằng mili giây). Các ứng dөng phổ biến 
trên lĩnh vực Internet, Tài chính, Ngân hàng, Hàng không, Quân sự, Y tế – Sức khỏe như 
hiện nay phần lớn dữ liệu lớn được xử lỦ real-time. Công nghệ xử lỦ dữ liệu lớn ngày nay 
đã cho phép chúng ta xử lỦ tức thì trước khi chúng được lưu trữ vào cơ sở dữ liệu. 
(3) Đa dạng (Variety) 
Đối với dữ liệu truyền thống chúng ta hay nói đến dữ liệu có cấu trúc, thì ngày nay 
hơn 80% dữ liệu được sinh ra là phi cấu trúc (tài liệu, blog, hình ảnh, vi deo, bài hát, dữ 
liệu từ thiết bị cảm biến vật lỦ, thiết bị chăm sóc sức khỏe...). Big Data cho phép liên kết 
và phân tích nhiều dạng dữ liệu khác nhau. Ví dө, với các bình luận cӫa một nhóm người 
dùng nào đó trên Facebook với thông tin video được chia sẻ từ Youtube và Twitter. 
(4) Độ tin cậy/chính xác (Veracity) 
Một trong những tính chất phức tạp nhất cӫa Dữ liệu lớn là độ tin cậy/chính xác cӫa 
dữ liệu. Với xu hướng phương tiện truyền thông xã hội (Social Media) và mạng xã hội 
(Social Network) ngày nay và sự gia tăng mạnh mẽ tính tương tác và chia sẻ cӫa người 
dùng Mobile làm cho bức tranh xác định về độ tin cậy & chính xác cӫa dữ liệu ngày một 
khó khăn hơn. Bài toán phân tích và loại bỏ dữ liệu thiếu chính xác và nhiễu đang là tính 
chất quan trọng cӫa BigData. 
 (5) Giá trị (Value) 
Giá trị là đặc điểm quan trọng nhất cӫa dữ liệu lớn, vì khi bắt đầu triển khai xây 
thông tin mang lại như thế nào, khi đó chúng ta mới có quyết định có nên triển khai dữ 
liệu lớn hay không. Nếu chúng ta có dữ liệu lớn mà chỉ nhận được 1% lợi ích từ nó, thì 
không nên đầu tư phát triển dữ liệu lớn. Kết quả dự báo chính xác thể hiện rõ nét nhất về 
giá trị cӫa dữ liệu lớn mang lại. Ví dө, từ khối dữ liệu phát sinh trong quá trình khám, 
chữa bệnh sẽ giúp dự báo về sức khỏe được chính xác hơn, sẽ giảm được chi phí điều trị 
và các chi phí liên quan đến y tế.  
1.4. Sự khác biệt giữa dữ liệu lớn với dữ liệu truyền thống  
3 
Dữ liệu lớn khác với dữ liệu truyền thống (ví dө, kho dữ liệu - Data Warehouse) ở 4 
điểm cơ bản: Dữ liệu đa dạng hơn; lưu trữ dữ liệu lớn hơn; truy vấn nhanh hơn; độ chính 
xác cao hơn. 
(1) Dữ liệu đa dạng hơn: Khi khai thác dữ liệu truyền thống (Dữ liệu có cấu trúc), 
chúng ta thường phải trả lời các câu hỏi: Dữ liệu lấy ra kiểu gì? định dạng dữ liệu như thế 
nào? Đối với dữ liệu lớn, không phải trả lời các câu hỏi trên. Hay nói khác, khi khai thác, 
chúng; điều quan tâm là giá trị mà dữ liệu mang lại có đáp ứng được cho công việc hiện 
tại và tương lai hay không. 
(2) Lưu trữ dữ liệu lớn hơn: Lưu trữ dữ liệu truyền thống vô cùng phức tạp và luôn 
đặt ra câu hỏi lưu như thế nào? dung lượng kho lưu trữ bao nhiêu là đӫ? gắn kèm với câu 
hỏi đó là chi phí đầu tư tương ứng. Công nghệ lưu trữ  dữ liệu lớn hiện nay đã phần nào 
có thể giải quyết được vấn đề trên nhờ những công nghệ lưu trữ đám mây, phân phối lưu 
xác và xử lỦ nhanh trong thời gian thực. 
(3) Truy vấn dữ liệu nhanh hơn: Dữ liệu lớn được cập nhật liên tөc, trong khi đó 
tin đáp ứng theo yêu cầu. 
(4) Độ chính xác cao hơn: Dữ liệu lớn khi đưa vào sử dөng thường được kiểm định 
lại dữ liệu với những điều kiện chặt chẽ, số lượng thông tin được kiểm tra thông thường 
rất lớn, và đảm bảo về nguồn lấy dữ liệu không có sự tác động cӫa con người vào thay 
đổi số liệu thu thập. 
2. Bͱc tranh tổng thể ͱng dụng dữ liệu lớn  
 Dữ liệu lớn đã được ứng dөng trong nhiều lĩnh vực như: hoạt động chính trị; giao 
thông; y tế; thể thao; tài chính; thương mại; thống kê... dưới đây là một số ví dө về ứng 
dөng dữ liệu lớn. 
2.1. Ͱng dụng dữ liệu lớn trong hoạt động chính trị 
cӫa mình. Ông xây dựng một đội ngũ nhân viên chuyên đi 
triển khai về dữ liệu lớn. Đội ngũ nhân viên này thu thập tất 
cả thông tin về người dân ở các khu vực, sau đó phân tích và 
chỉ ra một số thông tin quan trọng về người dân Mỹ như: 
Thích đọc sách gì, thích mua loại thuốc gì, thích sử dөng phương tiện gì... Thậm chí còn 
biết được cả thông tin về mẹ cӫa cử tri đó đã bỏ phiếu tín nhiệm ai ở lần bầu cử trước. 
Trên cơ sở những thông tin này, Tổng thống Obama đã đưa ra kế hoạch vận động phù 
hợp, giúp ông tái đắc cử Tổng thống nước Mỹ lần thứ 2. 
4 
dөng như: Hệ thống chính phӫ điện tử; phân tích quy định và việc tuân thӫ quy định; 
phân tích, giám sát, theo dõi và phát hiện gian lận, mối đe dọa, an ninh mạng. 
2.2. Ͱng dụng dữ liệu lớn trong giao thông 
dòng giao thông trong thành phố vào các giờ cao điểm, từ đó có 
những kế hoạch phân luồng giao thông chi tiết, hợp lỦ giúp giảm 
thiểu kẹt xe. Ngoài ra còn đưa ra thông tin cho người tham gia 
đi vào giờ nào để tránh kẹt xe, hoặc đi đường nào là ngắn nhất.v.v. Ngoài ra dữ liệu lớn 
còn giúp phân tích định vị người dùng thiết bị di động, ghi nhận chi tiết cuộc gọi trong 
thời gian thực; và giảm thiểu tình trạng ùn tắc giao thông. 
2.3. Ͱng dụng dữ liệu lớn trong y tế 
để đưa ra dự đoán về nguy cơ mắc bệnh. Đồng thời cũng đưa ra 
được xu hướng lây lan cӫa bệnh. Ví dө, ứng dөng Google Flu 
dөng này dựa trên từ khóa tìm kiếm ở một khu vực nào đó, sau đó 
kiếm đó, sau cùng là đưa ra dự báo về xu hướng dịch cúm tại khu 
vực đó. Qua đó cho biết tình hình cúm tại khu vực đó sẽ diễn ra như thế nào để đưa ra các 
giải pháp phòng tránh. Những kết quả mà Google Flu Trend đưa ra, hoàn toàn phù hợp 
với báo cáo cӫa Tổ chức y tế thế giới WHO về tình hình bệnh cúm tại các khu vực đó. 
2.4. Ͱng dụng dữ liệu lớn trong thể thao 
cӫa đội tuyển Đức (hình bên) đã đưa ra những điểm bất hợp lỦ 
trong cấu trúc cӫa đội tuyển Đức, từ đó giúp cho đội tuyển Đức 
khắc phөc được điểm yếu và đã dành được World cup 2014. 
2.5. Ͱng dụng dữ liệu lớn trong tài chính 
Từ những dữ liệu chính xác, kịp thời thu thập được thông qua các giao dịch cӫa 
khách hàng, tiến hành phân tích, xếp hạng và quản lỦ các rӫi ro trong đầu tư tài chính, tín 
dөng. 
2.6. Ͱng dụng dữ liệu lớn trong thương mại 
sau: Phân khúc thị trường và khách hàng; phân tích hành vi khách hàng tại cửa hàng; tiếp 
thị trên nền tảng định vị; phân tích tiếp thị chéo kênh, tiếp thị đa kênh; quản lỦ các chiến 
dịch tiếp thị và khách hàng thân thiết; So sánh giá; Phân tích và quản lỦ chuỗi cung ứng; 
Phân tích hành vi, thói quen người tiêu dùng. 
2.7. Ͱng dụng dữ liệu lớn trong thống kê 
5 
thức, Ӫy ban Thống kê Liên hợp quốc cũng như các tổ chức thống kê khu vực và cơ quan 
thống kê quốc gia cӫa nhiều nước đã triển khai hàng loạt các hoạt động về Bigdata như: 
Hàn Quốc sử dөng ảnh vệ tinh để thống kê nông nghiệp và một số lĩnhvực khác;Australia 
sử dөng ảnh vệ tinh để thống kê diện tích đất nông nghiệp và năng suất; Italia sử dөng dữ 
liệu điện thoại di động để thống kê di cư; Bhutan dùng thiết bị di động để tính toán chỉ số 
giá tiêu dùng; Estonia dùng điện thoại di động định vị vệ tinh để thống kê du lịch; 
3. Nhӳng cơ hội và thách thͱc khi ͱng dụng Big data trong thống kê chính thͱc 
3.1 Cơ hội  
(1) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp cho chúng ta có thêm phương án 
giải quyết, xử lỦ và đối phó với những thách thức đối sản xuất số liệu thống kê chính thức 
trong hiện tại và tương lai. Những nghiên cứu thực nghiệm cần phải được tiến hành để 
khám phá những ứng dөng tiềm năng cӫa dữ liệu lớn trong số liệu thống kê chính thức, 
và nghiên cứu thực nghiệm đó phải là một phần trong quy trình sản xuất số liệu thống kê. 
(2) Nghiên cứu về dữ liệu lớn cần phải có cơ sở hạ tầng công nghệ thông tin hiện 
đại, đáp ứng các yêu cầu xử lỦ khối lượng lớn dữ liệu và nhanh, đồng thời có thể tập hợp 
dữ liệu từ nhiều nguồn khác nhau. Thực hiện được điều này chúng ta có được đội ngũ 
qua kinh nghiệm thực tế. 
(3) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp chúng ta có được những văn bản 
được khai thác dữ liệu thông qua hồ sơ hành chính, ngoài ra dữ liệu cũng được bảo đảm 
và giữ bí mật nhờ những văn bản pháp lỦ bổ sung này. 
(4) Sử dөng dữ liệu lớn đem lại niềm tin cӫa cộng đồng với thống kê chính thức do 
tác động chӫ Ủ cӫa con người. 
3.2 Thách thͱc  
(1)Tài chính 
Nhiều đơn vị, tổ chức không đo lường được vấn đề sẽ phát sinh trong quá trình triển 
khai thực hiện, dự toán kinh phí chưa chính xác, do vậy dự án không thực hiện được. Để 
triển khai được thành công, yếu tố tài chính có Ủ nghĩa rất quan trọng, một số tập đoàn 
Big data như IBM, website bán hàng thương mại điện tử Amazon ... 
(2) Chính sách, quy định Luật pháp về truy cập và sử dụng dữ liệu 
Việc sử dөng và khai thác dữ liệu lớn phө thuộc vào luật quy định cӫa mỗi quốc gia. 
1 Xem Báo cáo “Thống kê chính thức với Big data: Kinh nghiệm quốc tế và định hướng của Thống kê Việt Nam. 
6 
  Ví dө: ở Canada người dùng có thể được tiếp cận dữ liệu từ cả hai tổ chức chính 
phӫ và phi chính phӫ, nhưng ở những nước khác như Ireland thì phải được sự cho phép 
từ các cơ quan chính phӫ. Điều này có thể dẫn đến những hạn chế để truy cập vào một số 
loại dữ liệu lớn. 
 (3) Trình độ khai thác và quản lý dữ liệu  
quản lỦ là cũng khác nhau tuy nhiên, Một vấn đề liên quan đến quản lỦ thông tin hiện nay 
là nguồn nhân lực. Khoa học dữ liệu lớn đang phát triển mạnh trong những tổ chức tư 
nhân, trong khi đó bộ phận này chưa được liên kết với những tổ chức cӫa chính phӫ một 
cách chặt chẽ dẫn đến việc quản lỦ vẫn còn nhiều vướng mắc.. 
(4) Hạ tầng Công nghệ thông tin 
sử dөng giao diện ứng dөng cӫa Chương trình chuyên sâu tiêu chuẩn (API) để truy cập 
dữ liệu. Bằng cách này, nó có thể kết nối các ứng dөng cho dữ liệu thu về và xử lỦ dữ liệu 
trực tiếp với dữ liệu hành chính. Ngoài ra hệ thống khai thác dữ liệu lớn cũng cần phải 
được tính toán để có thể kết nối vào được kho cơ sở dữ liệu truyền thống, đó cũng là một 
trong những thách thức lớn cần được giải quyết.  
data, những lợi ích mà Big data mang lại cho chúng ta. Bên cạnh đó cũng chỉ ra những 
thách thức khi triển khai áp dөng khai thác Big data. 
cung cấpthông tin để chung ta xử lỦ được tình huống nhanh nhất, chính xác nhất và giá trị 
cӫa Big data mang lại luôn có tính định hướng đến tương  lai ? giải đáp những câu hỏi tại 
sao việc ấy lại xảy ra?;  Sau chuyện đó thì điều gì sẽ sảy ra? và chúng ta nên ứng phó như 
thế nào trong hoàn cảnh đó? 
1. Tài liệu cơ hội và thách thức với bigdata –E cӫa Liên Hợp Quốc: 
http://unstats.un.org/unsd/statcom/doc14/2014-11-BigData-E.pdf 
2. Báo cáo Hội thảo về tương lai cӫa Thống kê học London: 
https://statistics.stanford.edu/statistics-and-science-london-workshop-report 
3. Tài liệu về các khái niệm và đặc trưng cӫa Big data: 
https://viblo.asia/dovv/posts/3OEqGjWwv9bL 

Removed lines from what-is-big-data-ebook-4421383.pdf:
What is Big Data? 
04 
  08 
Big Data Use Cases                                                                      10 
                                                             13 
15 
18 
4 
5 
What exactly is big data? 
To put it simply: big data is larger, more 
complex data sets, especially from new data 
sources. These data sets are so voluminous that 
traditional data processing software just can’t 
manage them. But these massive volumes of 
you wouldn’t have been able to tackle before. 
To really understand big data, it’s helpful to have 
some historical background. Here’s Gartner’s 
defnition, circa 2001(which is still the go-to 
defnition): 
“Big data is data that contains greater variety 
arriving in increasing volumes and with ever 
higher velocity. This is known as the three Vs.” 
• Volume.The amount of data matters. With 
big data, you’ll have to process high volumes 
of low-density, unstructured data. This can be 
data of unknown value, such as Twitter data 
feeds, clickstreams on a webpage or a mobile 
app, or sensor-enabled equipment. For some 
organizations, this might be tens of 
terabytes of data. For others, it may be 
hundreds of petabytes. 
Velocity. Velocity is the fast rate at which data 
is received and (perhaps) acted on. Normally, 
memory versus being written to disk.  Some 
internet-enabled smart products operate in real 
real-time evaluation and action. 
• Variety. In today’s big data world, data 
comes in new unstructured data types. 
Unstructured and semi-structured data types, 
such as text, audio, and video require addition 
support metadata. 
6 
Volume
1 
2 
3 
THE VALUE—AND TRUTH—OF 
Since 2001, two more Vs have become apparent: 
value and veracity. Data has intrinsic value. But 
it’s of no use until that value is discovered. 
Equally important: How truthful is your data—and 
how much can you rely on it? 
Today, big data has become capital. Think of 
some of the world’s biggest tech companies. A 
their data, which they’re constantly analyzing to 
new products. 
and compute, making it easier and less expensive 
to store more data than ever before. With an 
increased volume of big data now cheaper and 
more accessible, you can make more accurate 
and precise business decisions. 
Finding value in big data isn’t only about 
analyzing it (which is a whole other beneft). 
It’s an entire discovery process that requires 
insightful analysts, business users, and 
executives who ask the right questions, recognize 
patterns, make informed assumptions, and 
predict behavior. 
But how did we get here? 
7 
8 
Around 2005, people began to realize just how 
much data users generated through Facebook, 
YouTube, and other online services. Hadoop (an 
open-source framework created specifcally to 
store and analyze big data sets) was developed 
that same year. NoSQL also began to gain 
popularity during this time. 
The development of open-source frameworks, 
such as Hadoop (and more recently, Spark) was 
store. In the years since then, the volume of big 
data has skyrocketed. Users are still generating 
huge amounts of data—but it’s not just humans. 
With the advent of Internet of Things (IoT) , more 
objects and devices are connected to the internet, 
product performance. The emergence of machine 
learning has produced still more data. 
While big data has come far, its popularity is only 
just beginning. Cloud computing has expanded 
big data possibilities even further. 
The cloud offers a truly elastic scalability, where 
test around a subset of data. It’s an exciting time 
to see what’s going to happen next. 
THE VALUE OF BIG DATA COMES IS TWOFOLD: 
1. Big data makes it possible for you to gain 
2. More complete answers means more 
confdence in the data–which means 
a completely different approach to 
9 
10 
cases that you haven’t been able to fully delve 
into before. Here are just a few.  (More use cases 
are on our solutions page): 
Companies like Netfix and Procter & Gamble 
use big data to anticipate customer demand. 
products or services, and then modeling the 
commercial success of the offerings, they build 
predictive models for new products and services. 
In addition, P&G uses data and analytics from 
focus groups, social media, test markets, and 
early store rollouts to plan, produce, and launch 
new products. 
be deeply buried in structured data, such as the 
equipment year, make, and model, as well as 
entries, senor data, error messages, and engine 
temperature., By analyzing these indications of 
potential issues before the problems happen, 
equipment uptime. 
The race for customers is on. A clearer view of 
ever before. Big data enables you to gather data 
from social media, web visits, call logs, and 
experience and maximize the value delivered. 
Start delivering personalized offers, reduce 
customer churn, and handle issues proactively. 
When it comes to security, it’s not just a few 
rogue hackers; you’re up against entire expert 
teams. Security landscapes and compliance 
requirements are constantly evolving. Big 
indicate fraud and aggregate large volumes of 
much faster. 
now. And data—specifcally big data—is one of 
the reasons why. It’s only recently that we’ve 
them. And the availability of big data to train 
machine-learning models makes that happen. 
11 
news, but it’s an area in which big data is having 
the most impact. With big data, you can analyze 
and assess production, customer feedback and 
returns, and other factors to reduce outages and 
anticipate future demands. Big data can also be 
used to improve decision-making in line with 
current market demand. 
interdependencies between humans, institutions, 
entities, and process and then determining new 
ways to use those insights. Use data insights to 
considerations. Examine trends and what 
services. Implement dynamic pricing. There are 
endless possibilities. 
While big data holds a lot of promise, it is not 
without its challenges.  
First, big data is... big. Although new 
technologies have been developed to store data, 
data volumes are doubling in size around every 
two years. Organizations still struggle to keep 
store it. 
But it’s not enough to just store the data. Data 
must be used to be valuable, and that depends 
on curation. Clean data, or data that’s relevant 
meaningful analysis requires a lot of work. 
Data scientists spend 50 to 80 percent of their 
actually be used. 
Finally, big data technology is changing at a fast 
pace. A few years ago, Apache Hadoop was the 
popular technology used to handle big data. That 
is, until Apache Spark was introduced in 2014. 
Today, a combination of the two frameworks 
appears to be the best approach. Keeping up with 
big data technology is an ongoing challenge. 
12 
13 
Oracle Cloud for  Big  Data  Analytics 
Data 
Enterprise Apps 
Data 
new opportunities and business models. Getting 
started involves three key actions:   
disparate sources and applications. Traditional 
data integration mechanisms, such as ETL 
(extract, transform, and load) generally aren’t 
up to the task. It requires new strategies and 
technologies to analyze big data sets at terabyte, 
or even petabyte, scale. At the same time, big 
data has the same requirements for quality, 
governance, and confdence as traditional data 
sources. During integration, you need to bring in 
the data, process it, and make sure it’s formatted 
analysts can get started with. 
Big data requires storage. Your storage solution 
can be in the cloud, on-premises, or both. 
on an on-demand basis. Many people choose 
data is currently residing. The cloud is gradually 
gaining popularity because it supports your 
to spin up resources as needed. 
analyze and act on your data. Get new clarity with 
a visual analysis of your varied data sets. Explore 
the data further to make new discoveries. Share 
your fndings with others. Build data models with 
machine learning and artifcial intelligence. Put 
your data to work. 
To help you on your big data journey, we’ve put 
in mind. Here are our guidelines for building a 
successful big data foundation. 
14 
15 
#1: ALIGN BIG DATA WITH 
new discoveries. To that end, it is important to 
base new investments in skills, organization, 
or infrastructure with a strong business-
investments and funding. To determine if 
you are on the right track, ask how big data 
supports and enables your top business and IT 
priorities. Examples include understanding how 
behavior, deriving sentiment from social 
media and customer support interactions, and 
and their relevance for customer, product, 
manufacturing, and engineering data. 
#2: EASE SKILLS SHORTAGE 
shortage. You can mitigate this risk by ensuring 
that big data technologies, considerations, 
governance program. 
Standardizing your approach will allow you 
to manage costs and leverage resources. 
proactively identify any potential skill gaps. These 
can be addressed by training/cross-training 
existing resources, hiring new resources, and 
leveraging consulting frms. 
#3: OPTIMIZE KNOWLEDGE 
Use a Center of Excellence approach to share 
knowledge, control oversight, and manage 
project communications. Whether big data is a 
new or an expanding investment, the soft and 
hard costs can be shared across the enterprise. 
Leveraging this approach can help increase 
systematic way. 
#4:TOP PAYOFF IS ALIGNING 
own.But you can bring even greater business 
already using today. 
16 
Whether you are capturing customer, product, 
equipment, or environmental big data, the goal 
core master and analytical summaries, leading 
to better conclusions. For example, there is 
sentiment from that of only your best customers. 
capabilities, data warehousing platform, and 
information architecture. 
processes and models can be both human- and 
machine-based. Big data analytical capabilities 
include statistics, spatial analysis, semantics, 
interactive discovery, and visualization. Using 
analytical models, you can correlate different 
and meaningful discoveries. 
#5: PLAN YOUR DISCOVERY LAB 
straightforward. Sometimes we don’t even 
know what we’re looking for. That’s expected. 
Management and IT needs to support this “lack 
of direction” or “lack of clear requirement.” 
At the same time, it’s important for analysts and 
requirements. To accommodate the interactive 
statistical algorithms, you need high-performance 
work areas. Be sure that sandbox environments 
have the power they need—and are 
properly governed. 
#6: ALIGN WITH THE CLOUD 
jobs. A big data solution includes all data 
realms including transactions, master data, 
reference data, and summarized data. Analytical 
sandboxes should be created on demand. 
control of the entire data fow including pre- 
and post-processing, integration, in-database 
summarization, and analytical modeling. A well-
supporting these changing requirements. 
17 
18 
Clearly, big data has tremendous potential. 
customers, make more accurate decisions, and 
create new growth opportunities. Contact us to 
learn more. 
See how Oracle can help your big data journey. 
Start your free trial today. 
Contact us URL: 
https://www.oracle.com/marketingcloud/contact­
sales.html 
Free Trial URL: 
https://go.oracle.com/LP=50758/? 
19 
Oracle Corporation 
Copyright © 2019, Oracle and/or its affiliates. All rights reserved. This document is provided for information purposes only, and the contents hereof are subject 
to change without notice. This document is not warranted to be error-free, nor subject to any other warranties or conditions, whether expressed orally or 
implied in law, including implied warranties and conditions of merchantability or fitness for a particular purpose. We specifically disclaim any liability with 
500 Oracle Parkway 
respect to this document, and no contractual obligations are formed either directly or indirectly by this document. This document may not be reproduced or 
transmitted in any form or by any means, electronic or mechanical, for any purpose, without our prior written permission. 
CA 94065 
Oracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners. 
USA 
Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or 
registered trademarks of SPARC International, Inc. AMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks or registered trademarks of 
Advanced Micro Devices. UNIX is a registered trademark of The Open Group. 
Phone: +1.650.506.7000 
+1.800.ORACLE1 
Fax: 
+1.650.506.7200 
oracle.com 

Removed lines from 2_iis_2015_81-90.pdf:
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
81 
BIG DATA ANALYTICS 
Jasmine Zakir, Minot State University,	  jasminezakir@outlook.com 
Tom Seymour, Minot State University,	  tom.seymour@minotstateu.edu 
Kristi Berg, Minot State University,	  kristi.berg@minostatateu.edu 
ABSTRACT 
Today Big Data draws a lot of attention in the IT world. The rapid rise of the Internet and the digital economy has 
fuelled an exponential growth in demand for data storage and analytics, and IT department are facing tremendous 
challenge in protecting and analyzing these increased volumes of information. The reason organizations are 
collecting and storing more data than ever before is because their business depends on it. The type of information 
being created is no more traditional database-driven data referred to as structured data rather it is data that 
include documents, images, audio, video, and social media contents known as unstructured data or Big Data. Big 
Data Analytics is a way of extracting value from these huge volumes of information, and it drives new market 
opportunities and maximizes customer retention. This paper primarily focuses on discussing the various 
technologies that work together as a Big Data Analytics system that can help predict future volumes, gain insights, 
take proactive actions, and give way to better strategic decision-making. Further this paper analyzes the adoption, 
usage and impact of big data analytics to the business value of an enterprise to improve its competitive advantage 
using a set of data algorithms for large data sets such as Hadoop and MapReduce.  
Keywords: Big Data, Analytics, Hadoop, MapReduce 
Big Data is an important concept, which is applied to data, which does not conform to the normal structure of the 
traditional database. Big Data consists of different types of key technologies like Hadoop, HDFS, NoSQL, 
MapReduce, MongoDB, Cassandra, PIG, HIVE, and HBASE that work together to achieve the end goal like 
extracting value from data that would be previously considered dead. According to a recent market report published 
by Transparency Market Research, the total value of big data was estimated at $6.3 billion as of 2012, but by 2018, 
it’s expected to reach the staggering level of $48.3 billion that’s almost a 700 percent increase [29]. Forrester 
Research estimates that organizations effectively utilize less than 5 percent of their available data. This is because 
the rest is simply too expensive to deal with. Big Data is derived from multiple sources. It involves not just 
traditional relational data, but all paradigms of unstructured data sources that are growing at a significant rate. For 
instance, machine-derived data multiplies quickly and contains rich, diverse content that needs to be discovered. 
Another example, human-derived data from social media is more textual, but the valuable insights are often 
overloaded with many possible meanings.  
Big Data Analytics reflect the challenges of data that are too vast, too unstructured, and too fast moving to be 
managed by traditional methods. From businesses and research institutions to governments, organizations now 
routinely generate data of unprecedented scope and complexity. Gleaning meaningful information and competitive 
advantages from massive amounts of data has become increasingly important to organizations globally. Trying to 
efficiently extract the meaningful insights from such data sources quickly and easily is challenging. Thus, analytics 
increase their market share. The tools available to handle the volume, velocity, and variety of big data have 
improved greatly in recent years. In general, these technologies are not prohibitively expensive, and much of the 
software is open source. Hadoop, the most commonly used framework, combines commodity hardware with open-
source software. It takes incoming streams of data and distributes them onto cheap disks; it also provides tools for 
analyzing the data. However, these technologies do require a skill set that is new to most IT departments, which will 
need to work hard to integrate all the relevant internal and external sources of data. Although attention to technology 
isn’t sufficient, it is always a necessary component of a big data strategy. This paper discusses some of the most 
commonly used big data technologies mostly open source that work together as a big data analytics system for 
leveraging large quantities of unstructured data to make more informed decisions.  
https://doi.org/10.48009/2_iis_2015_81-90
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
82 
Big Data is a data analysis methodology enabled by recent advances in technologies that support high-velocity data 
capture, storage and analysis. Data sources extend beyond the traditional corporate database to include emails, 
mobile device outputs, and sensor-generated data where data is no longer restricted to structured database records 
but rather unstructured data having no standard formatting [30]. Since Big Data and Analytics is a relatively new 
and evolving phrase, there is no uniform definition; various stakeholders have provided diverse and sometimes 
contradictory definitions. One of the first widely quoted definitions of Big Data resulted from the Gartner report of 
2001. Gartner proposed that, Big Data is defined by three V’s volume, velocity, and variety. Gartner expanded its 
definition in 2012 to include veracity, representing requirements about trust and uncertainty pertaining to data and 
the outcome of data analysis. In a 2012 report, IDC defined the 4th V as value—highlighting that Big Data 
applications need to bring incremental value to businesses. Big Data Analytics is all about processing unstructured 
information from call logs, mobile-banking transactions, online user generated content such as blog posts and 
tweets, online searches, and images which can be transformed into valuable business information using 
computational techniques to unveil trends and patterns between datasets. 
Another dimension of the Big Data definition involves technology. Big Data is not only large and complex, but it 
requires innovative technology to analyze and process. In 2013, the National Institute of Standard and Technology 
(NIST) Big Data workgroup proposed the following definition of Big Data that emphasizes application of new 
technology; Big Data exceed the capacity or capability of current or conventional methods and systems, and enable 
novel approaches to frontier questions previously inaccessible or impractical using current or conventional methods. 
Business challenges rarely show up in the appearance of a perfect data problem, and even when data are abundant, 
practitioners have difficulties to incorporate it into their complex decision-making that adds business value. In 2012, 
McKinsey & Company conducted a survey of 1,469 executives across various regions, industries and company 
sizes, in which 49 percent of respondents said that their companies are focusing big data efforts on customer 
insights, segmentation and targeting to improve overall performance [10] An even higher number of respondents 60 
percent said their companies should focus efforts on using data and analytics to generate these insights. Yet, just 
one-fifth said that their organizations have fully deployed data and analytics to generate insights in one business unit 
or function, and only 13 percent use data to generate insights across the company. As these survey results show, the 
question is no longer whether big data can help business, but how can business derive maximum results from big 
data. 
Predictive Analytics is the use of historical data to forecast on consumer behavior and trends [18]. It is the use of 
past/historical data to predict future trends. This analysis makes use of the statistical models and machine learning 
algorithms to identify patterns and learn from historical data [25]. Predictive Analysis can also be defined as a 
process that uses machine learning to analyze data and make predictions [22].  
future, and 68% sight competitive advantage as the prime benefit of predictive analysis [17]. Broadly speaking, 
predictive analysis can be applied in ecommerce for product recommendation, price management, and predictive 
search. Typically a large e-commerce site offers thousands of product and services for sale. Navigating and 
searching for a product out of thousands on a website could be a major setback to consumers. However, with the 
invention of recommender system, an E-Commerce site/application can quickly identify/predict products that 
closely suit the consumer’s taste [24].  
Using a technology called Collaborative Filtering a database of historical user preferences is created. When a new 
customer access the ecommerce site, the customer is matched with the database of preferences, in order to discover a 
preference class that closely matches the customer taste. These products are then recommended to the customer [24]. 
Another technology that is used in ecommerce is the clustering algorithm. Clustering algorithm works by identifying 
groups of users that have similar preferences. These users are then clustered into a single group and are given a 
unique identifier.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
83 
New customers cluster are predicted by calculating the average similarities of the individual members in that cluster. 
Hence a user could be a partial member of more than one cluster depending of the weight of the user’s average 
opinion [24]. Advanced analytics is defined as the scientific process of transforming data into insight for making 
better decisions. As a formal discipline, advanced analytics have grown under the Operational Research domain. 
There are some fields that have considerable overlap with analytics, and also different accepted classifications for 
the types of analytics [2].  
Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large 
amounts of log data from many different sources to a centralized data store. Flume deploys as one or more agents, 
each contained within its own instance of the Java Virtual Machine (JVM). Agents consist of three pluggable 
components: sources, sinks, and channels. Flume agents ingest incoming streaming data from one or more sources. 
Data ingested by a Flume agent is passed to a sink, which is most commonly a distributed file system like Hadoop. 
agent to be the sink of another. Flume sources listen and consume events. Events can range from newline-terminated 
strings in stdout to HTTP POSTs and RPC calls — it all depends on what sources the agent is configured to use. 
Flume agents may have more than one source, but at the minimum they require one. Sources require a name and a 
type; the type then dictates additional configuration parameters. 
Channels are the mechanism by which Flume agents transfer events from their sources to their sinks. Events written 
to the channel by a source are not removed from the channel until a sink removes that event in a transaction. This 
allows Flume sinks to retry writes in the event of a failure in the external repository (such as HDFS or an outgoing 
network connection). For example, if the network between a Flume agent and a Hadoop cluster goes down, the 
channel. Sink is an interface implementation that can remove events from a channel and transmit them to the next 
agent in the flow, or to the event’s final destination and also sinks can remove events from the channel in 
transactions and write them to output. Transactions close when the event is successfully written, ensuring that all 
events are committed to their final destination.  
Apache Sqoop is a CLI tool designed to transfer data between Hadoop and relational databases. Sqoop can import 
been transformed using MapReduce. Sqoop also has the ability to import data into HBase and Hive. Sqoop connects 
imported. Both import and export utilize MapReduce, which provides parallel operation as well as fault tolerance. 
During import, Sqoop reads the table, row by row, into HDFS. Because import is performed in parallel, the output in 
HDFS is multiple files.  
Apache’s Pig is a major project, which is lying on top of Hadoop, and provides higher-level language to use 
Hadoop’s MapReduce library. Pig provides the scripting language to describe operations like the reading, filtering 
and transforming, joining, and writing data which are exactly the same operations that MapReduce was originally 
designed for. Instead of expressing these operations in thousands of lines of Java code which uses MapReduce 
directly, Apache Pig lets the users express them in a language that is not unlike a bash or Perl script.  
Pig was initially developed at Yahoo Research around 2006 but moved into the Apache Software Foundation in 
2007. Unlike SQL, Pig does not require that the data must have a schema, so it is well suited to process the 
unstructured data. But, Pig can still leverage the value of a schema if you want to supply one. PigLatin is relationally 
complete like SQL, which means it is at least as powerful as a relational algebra. Turing completeness requires 
conditional constructs, an infinite memory model, and looping constructs.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
84 
SQL for querying. Being a SQL dialect, HIVEQL is a declarative language. In PigLatin, you specify the data flow, 
but in Hive we describe the result we want and hive figures out how to build a data flow to achieve that result. 
Unlike Pig, in Hive a schema is required, but you are not limited to only one schema. Like PigLatin and SQL, 
HiveQL itself is a relationally complete language but it is not a Turing complete language.  
Apache Zoo Keeper is an effort to develop and maintain an open-source server, which enables highly reliable 
distributed coordination. It provides a distributed configuration service, a synchronization service and a naming 
registry for distributed systems. Distributed applications use ZooKeeper to store and mediate updates to import 
configuration information. ZooKeeper is especially fast with workloads where reads to the data are more common 
than writes. The ideal read/write ratio is about 10:1. ZooKeeper is replicated over a set of hosts (called an ensemble) 
and the servers are aware of each other and there is no single point of failure.   
Figure 1. Intel Manager for Hadoop [3] 
MongoDB is an open source, document-oriented NoSQL database that has lately attained some space in the data 
industry. It is considered as one of the most popular NoSQL databases, competing today and favors master-slave 
replication. The role of master is to perform reads and writes whereas the slave confines to copy the data received 
from master, to perform the read operation, and backup the data. The slaves do not participate in write operations 
but may select an alternate master in case of the current master failure. MongoDB uses binary format of JSON-like 
documents underneath and believes in dynamic schemas, unlike the traditional relational databases. The query 
system of MongoDB can return particular fields and query set compass search by fields, range queries, regular 
expression search, etc. and may include the user-defined complex JavaScript functions. As hinted already, 
MongoDB practice flexible schema and the document structure in a grouping, called Collection, may vary and 
common fields of various documents in a collection can have disparate types of the data. 
The MongoDB is equipped with the suitable drivers for most of the programming languages, which are used to 
develop the customized systems that use MongoDB as their backend player. There is an increasingly demand of 
using MongoDB as pure in-memory database; in such cases, the application dataset will always be small. Though, it 
is probably are easy for maintenance and can make a database developer happier; this can be a bottle neck for 
complex applications that require tremendous database management capabilities. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
85 
In order to efficiently address the challenges of Big Data, the leading vendor developed the Oracle NoSQL database. 
It was built by Oracle Berkeley DB team and the Berkeley DB Java Edition is the building block of Oracle NoSQL. 
Berkeley DB is a robust and scalable key-value store and used as the underlying storage for several popular data 
model such as Amazon Dynamo, GenieDB, MemcacheDB and Voldemort [28].  
scalability, throughput, and reliability with little tuning efforts. It is an efficient and a resilient transaction model that 
significantly eases the development process of applications, involving Big Data. It is a distributed, scalable yet 
simple key-value pair data model that fully supports the ACID transactions and JSON format and integrated with 
Oracle Database and Hadoop. It offers scalable throughput with bounded latency. The model very well 
accommodates the horizontal scaling with dynamic annexation of new capacity, citing high availability; the design 
architecture of Oracle NoSQL does not support single point of failure, and lucid load balancing. Actually, the goals 
of high availability, rapid failover in the event of a node failure, etc. are achieved by replicating the storage nodes. 
which is able to handle big data requirements. It is a highly scalable and high-performance distributed database 
management system that can handle real-time big data applications that drive key systems for modern and successful 
businesses. It has a built-for-scale architecture that can handle petabytes of information and thousands of concurrent 
users/operations per second as easily as it can manage much smaller amount of data and user traffic. It has a peer to 
peer design that offers no single point of failure for any database process or function, in addition to the location 
independence capabilities that equate to a true network-independent method of storing and accessing data, data can 
be read and written anywhere. Apache Cassandra is also equipped with flexible/dynamic schema design that 
accommodates all formats of big data applications, including structured, semi-structured, and unstructured data. 
online.  
clusters of computers. It is designed to scale up from single servers to thousands of machines, with each offering 
local computation and storage. The basic notion is to allow a single query to find and collect results from all the 
cluster members, and this model is clearly suitable for Google's model of search support. One of the largest 
technological challenges in software systems research today is to provide mechanisms for storage, manipulation, and 
information retrieval on large amount of data. Web services and social media produce together an impressive 
amount of data, reaching the scale of petabytes daily (Facebook, 2012). These data may contain valuable 
information, which sometimes is not properly explored by existing systems. Most of this data is stored in a non-
structured manner, using different languages and format, which, in many cases, are in compatible. 
large datasets. Over the last years, commodity hardware became part of clusters, since the x86 platform cope with 
the need of having an overall better cost/performance ratio, while decreasing maintenance cost. Apache Hadoop is a 
framework developed to take advantage of this approach, using such commodity clusters for storage, processing and 
manipulation of large amount of data. The framework was designed over the MapReduce paradigm and uses the 
HDFS as a storage file system. Hadoop presents key characteristics when performing parallel and distributed 
computing, such as data integrity, availability, scalability, exception handling, and failure recovery.   
Hadoop is a popular choice when you need to filter, sort, or pre-process large amounts of new data in place and 
distill it to generate denser data that theoretically contains more information. Pre-processing involves filtering new 
data sources to make them suitable for additional analysis in a data warehouse.  Hadoop is a top-level open source 
project of the Apache Software Foundation. Several suppliers, including Intel, offer their own commercial Hadoop 
distributions, packaging the basic software stack with other Hadoop software projects such as Apache Hive, Apache 
Pig, and Apache Sqoop. These distributions must integrate with data warehouses, databases, and other data 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
86 
process or query.	  
Figure 2. Data Architecture with Hadoop Integrated with existing data system [12] 
MapReduce is the original massively scalable, parallel processing framework commonly used with Hadoop and 
other components such as the Hadoop Distributed File System (HDFS) and YARN. YARN can be described as a 
large-scale, distributed operating system for big data implementations. As Hadoop has matured, the batch-oriented, 
disk-intensive MapReduce’ s limitations have become more apparent as Big Data analytics moves to more real-time, 
streaming processing and advanced implementations such as the aforementioned machine learning.  
MapReduce is the model of distributed data processing introduced by Google in 2004. The fundamental concept of 
MapReduce is to divide problems into two parts: a map function that processes source data into sufficient statistics 
and a reduce function that merges all sufficient statistics into a final answer. By definition, any number of 
concurrent map functions can be run at the same time without intercommunication. Once all the data has had the 
map function applied to it, the reduce function can be run to combine the results of the map phases.  For large scale 
batch processing and high speed data retrieval, common in Web search scenarios, MapReduce provides the fastest, 
most cost-effective and most scalable mechanism for returning results. Today, most of the leading technologies for 
managing "big data" are developed on MapReduce. With MapReduce there are few scalability limitations, but 
leveraging it directly does require writing and maintaining a lot of code. 
Splunk is a general-purpose search, analysis and reporting engine for time-series text data, typically machine data. 
Splunk software is deployed to address one or more core IT functions: application management, security, 
compliance, IT operations management and providing analytics for the business. The Splunk engine is optimized for 
quickly indexing and persisting unstructured data loaded into the system. Specifically, Splunk uses a minimal 
schema for persisted data – events consist only of the raw event text, implied timestamp, source (typically the 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
87 
filename for file based inputs), source type (an indication of the general type of data) and host (where the data 
originated).  
Once data enters the Splunk system, it quickly proceeds through processing, is persisted in its raw form and is 
indexed by the above fields along with all the keywords in the raw event text. Indexing is an essential element of the 
canonical “super-grep” use case for Splunk, but it also makes most retrieval tasks faster. Any more sophisticated 
processing on these raw events is deferred until search time. This serves four important goals: indexing speed is 
increased as minimal processing is performed, bringing new data into the system is a relatively low effort exercise as 
no schema planning is needed, the original data is persisted for easy inspection and the system is resilient to change 
as data parsing problems do not require reloading or re-indexing the data. 
Apache Spark an open source big data processing framework built around speed, ease of use, and sophisticated 
analytics. It was originally developed in 2009 in UC Berkeley’s AMP Lab, and open sourced in 2010 as an Apache 
project. Hadoop as a big data processing technology has been around for ten years and has proven to be the solution 
of choice for processing large data sets. MapReduce is a great solution for one-pass computations, but not very 
efficient for use cases that require multi-pass computations and algorithms. Each step in the data processing 
workflow has one Map phase and one Reduce phase and you'll need to convert any use case into MapReduce pattern 
to leverage this solution. Spark takes MapReduce to the next level with less expensive shuffles in the data 
processing. With capabilities like in-memory data storage and near real-time processing, the performance can be 
several times faster than other big data technologies.  
Spark also supports lazy evaluation of big data queries, which helps with optimization of the steps in data processing 
workflows. It provides a higher-level API to improve developer productivity and a consistent architect model for big 
data solutions. Spark holds intermediate results in memory rather than writing them to disk, which is very useful 
especially when you need to work on the same dataset multiple times. It’s designed to be an execution engine that 
works both in-memory and on-disk. Spark operators perform external operations when data does not fit in memory. 
Spark can be used for processing datasets that larger than the aggregate memory in a cluster. Spark will attempt to 
store as much as data in memory and then will spill to disk. It can store part of a data set in memory and the 
remaining data on the disk. You have to look at your data and use cases to assess the memory requirements. With 
this in-memory data storage, Spark comes with a great performance advantage. 
Spark is written in Scala Programing Language and runs on the Java Virtual machine. It currently supports 
programming languages like Scala, java, python, Clojure and R. Other than Spark Core API, there are additional 
libraries that are part of the Spark ecosystem and provide additional capabilities in Big Data analytics. Spark 
Streaming is one among the spark library that can be used for processing the real-time streaming data. This is based 
on micro based on micro batch style of computing and processing. Spark SQL provides the capabilities to expose the 
visualization tools. MLlib, GraphX are some other libraries from spark. 
Thomas H. Davenport was perhaps the first to observe in his Harvard Business Review article published in January 
2006 (“Competing on Analytics”) how companies who orientated themselves around fact based management 
approach and compete on their analytical abilities considerably out-performed their peers in the marketplace. The 
reality is that it takes continuous improvement to become an analytics-driven organization. In a presentation given at 
the Strata New York conference in September 2011, McKinsey & Company showed the eye opening; 10-year 
category growth rate differences (see Figure 7, below) between businesses that smartly use their big data and those 
that do not.  
Amazon uses Big Data to monitor, track and secure 1.5 billion items in its inventory that are laying around 200 
fulfillment centers around the world, and then relies on predictive analytics for its ‘anticipatory shipping’ to predict 
when a customer will purchase a product, and pre-ship it to a depot close to the final destination. Wal-Mart handles 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
88 
more than a million customer transactions each hour [23], imports information into databases to contain more than 
2.5 petabytes and asked their suppliers to tag shipments with radio frequency identification (RFID) systems [8] that 
can generate 100 to 1000 times the data of conventional bar code systems. UPS deployment of telematics in their 
freight segment helped in their global redesign of logistical networks [6]. Amazon is a big data giant and the largest 
online retail store. The company pioneered e-commerce in many different ways, but one of its biggest successes was 
the personalized recommendation system, which was built from the big data it gathers from its millions of 
customers’ transactions. 
The U.S. federal government collects more than 370,000 raw and geospatial datasets from 172 agencies and sub 
agencies.  It leverages that data to provide a portal to 230 citizen-developed apps, with the aim of increasing public 
access to information not deemed private or classified. Professional social network LinkedIn uses data from its more 
than 100 million users to build new social products based on users’ own definitions of their skill sets. Silver Spring 
Networks deploys smart, two-way power grids for its utility customers that utilize digital technology to deliver more 
help manage energy use and maximize efficiency. Jeffrey Brenner and the Camden Coalition mapped a city’s crime 
trends to identify problems with its healthcare system, revealing services that were both medically ineffective and 
expensive. 
Today’s technology landscape is changing fast. Organizations of all shapes and sizes are being pressured to be data-
driven and to do more with less. Even though big data technologies are still in a nascent stage, relatively speaking, 
the impact of the 3V’s of big data, which now is 5v’s cannot be ignored. The time is now for organizations to begin 
planning for and building out their Hadoop-based data lake. Organizations with the right infrastructures, talent and 
vision in place are well equipped to take their big data strategies to the next level and transform their businesses. 
They can use big data to unveil new patterns and trends, gain additional insights and begin to find answers to 
pressing business issues. The deeper organizations dig into big data and the more equipped they are to act upon 
what’s learned, the more likely they are to reveal answers that can add value to the top line of the business. This is 
where the returns on big data investments multiply and the transformation begins. Harnessing big data insight 
delivers more than cost cutting or productivity improvement but it definitely reveals new business opportunities. 
Data-driven decisions always tend to be better decisions. 
1. Apache Software Foundation. (2010). Apache ZooKeeper. Retrieved April 5, 2015 from 
https://zookeeper.apache.org 
2. Chae, B., Sheu, C., Yang, C. and Olson, D. (2014). The impact of advanced analytics and data accuracy on 
operational performance: A contingent resource based theory (RBT) perspective, Decision Support Systems, 59, 
119-126. 
3. Chambers, C., Raniwala, A., Adams, S., Henry, R., Bradshaw, R., and Weizenbaum, N. (2010). Flume Java: 
Easy, Efficient Data-Parallel Pipelines. Google, Inc. Retrieved April 1, 2015 from 
http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf 
4. Cisco Systems. Cisco UCS Common Platform Architecture Version 2 (CPA v2) for Big Data with 
Comprehensive Data Protection using Intel Distribution for Apache Hadoop. Retrieved March 15, 2015, from 
http://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/Cisco_UCS_CPA_for_Big_Data_wi
th_Intel.html 
5. DATASTAX Corporation. (2013, October). Big Data: Beyond the Hype - Why Big data Matters to you [White 
paper]. Retrieved March 15, 2015 from https://www.datastax.com/wp-content/uploads/2011/10/WP-DataStax-
BigData.pdf 
6. Davenport, T & Patil, D. (2012). Data Scientist: The Sexiest Job of the 21st Century. Harvard Business Review, 
90, 70-76. 
7. Dhawan, S & Rathee, S. (2013). Big Data Analytics using Hadoop Components like Pig and Hive. American 
International Journal of Research in Science, Technology, Engineering & Mathematics, 88, 13-131. Retrieved 
from http://iasir.net/AIJRSTEMpapers/AIJRSTEM13-131.pdf 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
89 
8. Edwards, P., Peters, M. and Sharman, G. (2001). The Effectiveness of Information Systems in Supporting the 
Extended Supply Chain, Journal of Business Logistics 22 (1), 1-27 
9. EMC Corporation. (2013, January). EMC Accelerates Journey to Big Data with Business Analytics-as-a-
Service [White paper]. Retrieved from http://www.emc.com/collateral/white-papers/h11259-emc-accelerates-
journey-big-data-ba-wp.pdf 
10. EMC Corporation. Big Data, Big Transformations [White paper]. Retrieved from 
http://www.emc.com/collateral/white-papers/idg-bigdata-umbrella-wp.pdf 
11. EMC Solutions Group. (2012, July). Big Data-as-a-Service [White paper]. Retrieved from 
https://www.emc.com/collateral/software/white-papers/h10839-big-data-as-a-service-perspt.pdf 
12. Enterprise Hadoop: The Ecosystem of Projects. Retrieved from http://hortonworks.com/hadoop/ 
13. George, L. (2014, September). Getting Started with Big Data Architecture. Retrieved April 5, 2015, from 
http://blog.cloudera.com/blog/2014/09/getting-started-with-big-data-architecture/ 
14. IBM Corporation. IBM Big Data Platform. Retrieved from http://www-
01.ibm.com/software/in/data/bigdata/enterprise.html 
15. Intel Corporation. Big Data Analytics - Extract, Transform, and Load Big data with Apache Hadoop [White 
paper]. Retrieved April 3, 2015 from https://software.intel.com/sites/default/files/article/402274/etl-big-data-
with-hadoop.pdf 
16. McClary, D. (2013, June). Acquiring Big Data Using Apache Flume. Retrieved March 3, 2015 from 
http://www.drdobbs.com/database/acquiring-big-data-using-apache-flume/240155029 
17. Millard, S. (2013). Big Data Brewing Value in Human Capital Management – Ventana Research. Retrieved 
April 2, 2015 from http://stephanmillard.ventanaresearch.com/2013/08/28/big-data-brewing-value-in-human-
capital-management 
18. Mosavi, A. and Vaezipour, A. (2013). Developing Effective Tools for Predictive Analytics and Informed 
Decisions. Technical Report. University of Tallinn.  
19. Oracle Corporation. (2013, March). Big Data Analytics - Advanced Analytics in Oracle Database [White 
paper]. Retrieved March 5, 2015 from http://www.oracle.com/technetwork/database/options/advanced-
analytics/advanced-analytics-wp-12c-1896138.pdf?ssSourceSiteId=ocomen 
20. Oracle Enterprise Architecture. (2015, April). An Enterprise Architect's Guide to Big Data - Reference 
Architecture Overview [White paper]. Retrieved from 
http://www.oracle.com/technetwork/topics/entarch/articles/oea-big-data-guide-1522052.pdf 
21. Penchikala, S. (2015, January). Big Data Processing with Apache Spark - Part 1: Introduction. Retrieved from 
http://www.infoq.com/articles/apache-spark-introduction 
22. Puri, R. (2013). How Online Retailers Use Predictive Analytics To Improve Your Shopping Experience. 
Retrieved April5, 2015 from http://blogs.sap.com/innovation/analytics/how-online-retailers-use-predictive-
analytics-to-improve-your-shopping-experience-0108060 
23. Sanders, N.R. (2014). Big Data Driven Supply Chain Management: A Framework for Implementing Analytics 
and Tuning Information into Intelligence, 1st Edition, Pearson, NJ 
24. Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. (2002). Recommendation systems for large e-commerce: 
Scalable neighborhood formation using clustering. In Proceedings of the fifth international conference on 
computer and information technology, 1.  
25. Shmueli, G. & Koppius, O. (2011). Predictive Analytics in Information Systems Research. MIS Quarterly, 
35(3), pp. 553-72. 
26. Sorkin, S. (2011). Splunk Technical Paper: Large-Scale, Unstructured Data Retrieval and Analysis Using 
Splunk. Retrieved April 15, 2015 from https://www.splunk.com/content/dam/splunk2/pdfs/technical-
briefs/splunk-and-mapreduce.pdf 
27. The Bloor Group. IBM and the Big Data Information Architecture. Retrieved April 3, 2015 from 
http://insideanalysis.com/wp-content/uploads/2014/08/BDIAVendor-IBMv01.pdf 
28. Tiwari, S. (2011). Using Oracle Berkeley DB as a NoSQL Data Store. Retrieved April 5 2015 from 
http://www.oracle.com/technetwork/articles/cloudcomp/berkeleydb-nosql-323570.htm 
29. Transparency Market Report. (May, 2015).Big Data Applications in Healthcare likely to Propel Market to 
US$48.3 Bn by 2018. Retrieved June 26, 2015, from 
http://www.transparencymarketresearch.com/pressrelease/big-data-market.htm 
30. Villars, R. L., Olofson, C. W., & Eastwood, M. (2011, June). Big data: What it is and why you should care. IDC 
White Paper. Framingham, MA: IDC. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
90 
31. Wolpe, T. (2015, March). How Facebook is speeding up the Pesto SQL query engine. Retrieved April 3, 2015, 
from http://www.zdnet.com/article/how-facebook-is-speeding-up-the-presto-sql-query-engine 
32. Zahari et al. (2010). Spark: Cluster Computing with Working Sets. Retrieved April 7, 2015, from 
http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf 

Removed lines from 48077-157-151840-1-10-20200520.pdf:
THÔNG TIN VÀ TƯ LIỆU - 2/2020 23
BIG DATA VÀ XU HƯỚNG ỨNG DỤNG TRONG HOẠT ĐỘNG THÔNG TIN - THƯ VIỆN
ThS Nguyễn Lê Phương Hoài
Viện Thông tin Khoa học xã hội 
● Tóm tắt: Big Data là một thuật ngữ được sử dụng để chỉ những bộ dữ liệu khổng lồ, chủ yếu không 
có cấu trúc, được thu thập từ nhiều nguồn khác nhau. Big Data có nhiều tác động, ứng dụng và được 
xem như một yếu tố quyết định đến việc phát triển, mang lại lợi thế cạnh tranh cho tổ chức. Bài viết 
tổng quan lược sử các quan điểm về Big Data, đồng thời nhấn mạnh vào các xu hướng ứng dụng 
trong hoạt động thông tin - thư viện.
● Từ khóa: Big Data; dữ liệu lớn; hoạt động thư viện.
BIG DATA APPLICATION IN LIBRARY AND INFORMATION ACTIVITIES
● Abstract: Big Data is a term used to refer to huge, mostly unstructured datasets, collected from 
a variety of sources. Big Data has many impacts, applications and is considered as a decisive 
factor in the development, bringing competitive advantages to the organization. The overview paper 
summarizes the views on Big Data and emphasizes application trends in library and information 
activities.
● Keywords: Big Data; library activities.
1. LƯỢC SỬ CÁC QUAN ĐIỂM VỀ BIG DATA
Hiện nay, chưa có một định nghĩa chính 
xác cho thuật ngữ Big Data. Big được ghi 
nhận lần đầu tiên trong báo cáo “Application-
controlled demand paging for out-of-core 
visualization” của Michael Cox và David 
thứ 8 (vào tháng 10 năm 1997). Ý tưởng đưa 
xử lý không còn đủ số lượng cần khảo sát, do 
thể phân tích được tất cả các thông tin [11].
Tháng 8 năm 1999, Steve Bryson, David 
Kenwright, Michael Cox, David Ellsworth, và 
Robert Haimes đăng bài “Visually exploring 
gigabyte data sets in real time” trên Tạp chí 
Communications of the ACM. Đây là bài viết 
đầu tiên sử dụng thuật ngữ “Big Data”. Các 
tác giả nhận định: “Những chiếc máy tính 
vực, cũng có thể là bất lợi; tính toán nhanh 
chóng tạo ra một lượng lớn dữ liệu. Nếu trước 
là lớn, thì bây giờ chúng ta có thể tìm thấy 
300 GB” [15]. 
Tháng 11 năm 2000, Francis X. Diebold 
Hiệp hội Kinh tế lượng bài viết “Big Data 
Measurement and Forecasting”. Trong bài 
viết này, tác giả khẳng định: “Gần đây, nhiều 
ngành khoa học như vật lý, sinh học, khoa 
học xã hội, vốn đang buộc phải đương đầu với 
khó khăn - đã thu được lợi từ hiện tượng Big 
Data và đã gặt hái được nhiều thành công. Big 
Data chỉ sự bùng nổ về số lượng (và đôi khi, 
chất lượng), khả năng liên kết cũng như độ 
sẵn sàng của dữ liệu, chủ yếu là kết quả của 
việc ghi lại dữ liệu và công nghệ lưu trữ” [4].
Tháng 2 năm 2001, Doug Laney - nhà 
phân tích của Tập đoàn Meta, công bố nghiên 
cứu “3D Data Managment: Controlling Data 
Volume, Velocity, and Variety”. Laney cho 
rằng, những thách thức và cơ hội nằm trong 
bằng mô hình “3Vs”: tăng về số lượng lưu trữ 
(Volume), tăng về tốc độ xử lý (Velocity) và 
tăng về chủng loại (Variety) [3]. Một thập kỷ 
sau, mô hình “3Vs” đã trở thành thuật ngữ 
dữ liệu lớn ba chiều. Nhiều công ty và tổ chức 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
24 THÔNG TIN VÀ TƯ LIỆU - 1/2020
24
dụng mô hình “3Vs” này để định nghĩa Big 
Data.
Tháng 12 năm 2008, Randal E. Bryant, 
Randy H. Katz, và Edward D. Lazowska 
công bố bài viết “Big-Data Computing: 
Commerce, Science and Society”, trong đó 
miêu tả: “Cũng như công cụ tìm kiếm đã làm 
thay đổi cách chúng ta tiếp cận thông tin, các 
ty, các nhà nghiên cứu khoa học, các học 
viên y tế, quốc phòng và tình báo,... Sử dụng 
nghệ máy tính suốt một thập kỷ qua. Chúng 
nó trong việc thu thập, sắp xếp và xử lý dữ 
liệu của tất cả các tầng lớp xã hội. Một khoản 
sẽ thúc đẩy phát triển và mở rộng nó” [13].
Tháng 2 năm 2010, Kenneth Cukier đăng 
“Data, data everywhere”. Cukier viết: “... thế 
mức không tưởng, và càng ngày càng được 
nhân rộng với tốc độ nhanh hơn bao giờ hết... 
Hiệu quả đã được thể hiện ở khắp mọi nơi, từ 
kinh doanh đến khoa học, từ chính phủ đến 
nghệ thuật. Các nhà khoa học và kỹ sư máy 
tượng này: Big Data” [8].
Tháng 5 năm 2012, Danah Boyd và Kate 
bài “Critical Question for Big Data” trên tờ 
Information, Communications and Society. 
Các tác giả định nghĩa Big Data như là “một 
hiện tượng văn hóa, công nghệ và học thuật 
dựa trên sự tương tác của: 1) Công nghệ tối 
thuật toán để thu thập, phân tích, liên kết, và 
so sánh các tập dữ liệu lớn; 2) Phân tích: tạo 
tuyên bố kinh tế, xã hội, kỹ thuật và pháp lý; 3) 
Thần thoại: niềm tin phổ biến rằng dữ liệu lớn 
biết mà trước đây không thể, với hào quang 
của sự thật, khách quan, chính xác” [2].
Sau đó, Gartner - công ty nghiên cứu và 
tư vấn công nghệ thông tin - bổ sung thêm 
rằng “Big Data ngoài 3 tính chất (số lượng, 
tốc độ xử lý và chủng loại) thì còn phải cần 
khám phá sâu vào sự vật/sự việc và tối ưu 
hóa các quy trình làm việc” [5]. Cùng quan 
điểm đó, Tan Jee Toon cho rằng Big Data 
mọi thứ xung quanh chúng ta, từ các thiết bị 
kỹ thuật số như di động, video, hình ảnh, tin 
nhắn tới các thiết bị cảm biến, các máy móc 
hội. Big Data có đặc điểm là được sinh ra với 
khối lượng (volume), tốc độ (velocity), độ đa 
dạng (variety) và tính xác thực (veracity) rất 
lớn [16].
Năm 2014, Gartner đưa ra khái niệm 
mới về Big Data bằng mô hình “5Vs”, gồm: 
Volume (khối lượng), Velocity (tốc độ), 
Variety (tính đa dạng), Veracity (tính xác 
thực) và Value (giá trị). Trong đó: Volume là 
khối lượng Big Data được tạo ra mỗi ngày. 
phân tán, nơi mà dữ liệu chỉ được lưu trữ một 
bởi phần mềm. Velocity là tốc độ dữ liệu mới 
được tạo ra và tốc độ dữ liệu chuyển động. 
giữ chúng trong các cơ sở dữ liệu. Variety là 
các kiểu khác nhau của dữ liệu. Công nghệ 
có cấu trúc truyền thống (được lưu trữ trong 
các bảng hoặc các cơ sở dữ liệu quan hệ) và 
phi cấu trúc (bao gồm các thông điệp, trao 
đổi của mạng xã hội, các hình ảnh, dữ liệu 
cảm biến, video, tiếng nói...). Veracity là tính 
hỗn độn hoặc tính tin cậy của dữ liệu. Công 
kiểm soát những loại dữ liệu này. Value là giá 
trị của dữ liệu. Việc tiếp cận Big Data sẽ chỉ 
thành những thứ có giá trị. Đây là khái niệm 
đầy đủ về 5 tính chất của Big Data [5].
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 25
THÔNG TIN VÀ TƯ LIỆU - 1/2020 25
2. XU HƯỚNG ỨNG DỤNG BIG DATA TRONG HOẠT 
ĐỘNG THÔNG TIN - THƯ VIỆN
Ngày nay, một lượng lớn dữ liệu số có thể 
các mạng xã hội. Theo Howe D. (2008): “Chỉ 
riêng trong các lĩnh vực nghiên cứu khoa học, 
trưởng theo cấp số nhân” [7]. Số lượng dữ 
nhiều lĩnh vực khác nhau và dữ liệu lớn (Big 
Data) được sử dụng rộng rãi trong các lĩnh 
vực, tổ chức với nhiều mục đích khác nhau. 
hành vi tiêu dùng của khách hàng, để đề xuất 
trên thông tin thu thập được (Ebay, Facebook, 
Google...). Các cơ sở nghiên cứu khoa học sử 
khoa học mới, ví dụ như xây dựng bản đồ 
gene của con người,... Việc sử dụng Big Data 
trong hoạt động thông tin - thư viện đã bắt 
đầu được quan tâm nghiên cứu. Campbell 
D. Grant, Cowan Scott R. (2016) phân tích 
và dữ liệu liên kết [1]. Kim Young Seok (2017) 
khuôn mặt Chernoff [9]. Gerrard D., Mooney 
J. và Thompson D. (2017) xem xét kiến trúc 
phân tích dữ liệu, các bộ tài nguyên được bảo 
thời gian tới [6]. Waqar Ahmed và Kanwal 
Ameen (2017) tổng quan các khái niệm về 
thư viện [17]. Ye Chunlei (2017) nghiên cứu 
trong thư viện đại học [18]. Zhan Ming, Widén 
Gunilla (2018) nghiên cứu vai trò của thư 
viện công cộng trong thời đại Big Data [20]. 
Li Shuqing; Jiao Fusen; Zhang Yong; Xu Xia 
(2019) nghiên cứu các vấn đề và thay đổi của 
dịch vụ người dùng tin [10],... Các nghiên cứu 
trong thời đại Big Data. Bàn về xu hướng ứng 
tiếp nào, nhưng có thể tổng hợp thành các xu 
hướng chính như sau:
Một là, tổ chức lưu trữ, bảo quản dữ liệu
Marydee Ojala nhận định: “Các thư viện 
ngày nay. Bộ sưu tập các tài nguyên số được 
các thư viện. Khối lượng và tính đa dạng dữ 
thư viện phải có phương pháp tổ chức lưu trữ, 
bảo quản dữ liệu hợp lý” [12]. Nguồn dữ liệu 
thư viện bao gồm: nguồn dữ liệu mô tả tài liệu 
thư viện, nguồn tài nguyên số hóa tài liệu thư 
viện, nguồn tài liệu số thư viện bổ sung qua 
việc mua hay sử dụng chung, nguồn dữ liệu 
khảo sát thư viện, dữ liệu định tính, dữ liệu 
tương tác xã hội,... Trước đây, các thư viện 
băng, đặt trong các cơ sở lưu trữ. Trước tác 
mạng công nghiệp lần thứ tư, các thư viện 
chi phí hiệu quả. Dữ liệu được lưu trữ theo 
hai cách, cả trên các thiết bị ngoại tuyến (thẻ 
nhớ SD, ổ cứng ngoài, ổ đĩa flash) và lưu trữ 
trực tuyến trên đám mây. Với phương thức 
kết hợp sử dụng băng từ để bảo quản lưu trữ, 
được yêu cầu, và sử dụng lưu trữ đám mây 
cho các Big Data. Các thư viện hướng đến 
thư viện (bao gồm cả tài nguyên vật chất và 
dữ liệu), xác định nhu cầu của người dùng 
thư viện. Trong thời gian tới, khi các yêu cầu 
mới thúc đẩy việc sử dụng Big Data, các thư 
viện hướng tới việc thu nhận, tổ chức lưu trữ 
dữ liệu (lưu trữ vật lý trong các máy chủ hoặc 
trong các cơ sở dữ liệu), bảo tồn dữ liệu và 
phổ biến dữ liệu, làm cho dữ liệu có sẵn trong 
qua các sản phẩm trực quan. Các thư viện 
tiến tới xây dựng, tạo lập hệ thống bảo quản 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
26
kỹ thuật số (bảo tồn cả tài nguyên số và siêu 
dữ liệu mô tả) có thể phát triển trong nhiều 
năm tới để đáp ứng với các yêu cầu mới. 
Hai là, cung cấp sản phẩm, dịch vụ thư 
viện mang tính cá nhân hóa, tùy chỉnh 
 Hiện nay, các thư viện đang có xu 
vụ trực tuyến. Nhiều thư viện đang sử dụng 
facebook, instagram để quảng bá các sản 
phẩm và dịch vụ thư viện. Các phương tiện 
thư viện. Cùng với các dữ liệu khảo sát thư 
viện, dữ liệu định tính (thông qua phỏng vấn, 
bảng trò chuyện...), dữ liệu tương tác xã hội 
(từ các trang truyền thông xã hội)... các thư 
thư viện. Từ đó, thư viện cung cấp các sản 
cầu của người dùng. Tuy nhiên, trong thời 
gian tới, “sự gia tăng của Big Data làm cho 
nhanh hơn, cho phép các thư viện vượt ra 
học tập và phân tích hiệu suất nghiên cứu” 
[19]. “Trong kỷ nguyên Big Data, không chỉ 
Big Data để đổi mới.” [10]. “Big Data có thể 
cũng sẽ thay đổi cho phù hợp” [17]. Các nhà 
có thể tác động đến hoạt động thông tin - thư 
viện, chuyển đổi phương thức cung cấp dịch 
vụ và tích hợp các hệ thống xử lý. Các hỗ trợ 
cạnh tranh để thư viện thu hút người dùng tin. 
Theo Li Shuqing, Jiao Fusen, Zhang Yong, 
Xu Xia: “Các vấn đề và tiềm năng của các thư 
dữ liệu, công nghệ, dịch vụ và người dùng 
tin. Sử dụng Big Data hiện có và xem xét các 
tại theo quan điểm của người dùng tin, thư 
viện có thể đưa ra các ý tưởng, phương pháp 
có trong các thư viện số” [10]. Đồng thời, nhu 
dùng tin. Kim Young Seok cho rằng: “Bằng 
thực, các thư viện có thể thiết kế các dịch 
tin. Big Data cung cấp thông tin chuyên sâu 
dùng tin, từ đó tạo ra trải nghiệm cá nhân 
hóa” [9]. Ví dụ, người dùng tin tìm kiếm trên 
gì người dùng tin gõ ở mục tìm kiếm, tần suất 
tìm kiếm, số lần tham khảo danh mục tài liệu, 
số lần xem mô tả tài liệu,... được thu thập và 
phân tích để tối ưu trải nghiệm, tạo cơ hội lớn 
hóa. Đặc biệt, với các công cụ phân tích dự 
báo của Big Data, thư viện sẽ nắm được thị 
hiếu, nhu cầu chính xác để cung cấp các sản 
phẩm, dịch vụ phù hợp với người dùng tin 
trong thời gian thực.
Ba là, ứng dụng dịch vụ phân tích dự báo
Giống như hầu hết các ngành khác, phân 
tích dự báo sẽ là một sự thay đổi lớn, quan 
trọng trong các cơ quan thông tin - thư viện. 
hoạt động hiệu quả hơn, đồng thời làm thay 
người dùng tin. Theo cách truyền thống, mối 
khá đơn giản. Người dùng thư viện nộp tiền, 
làm thẻ thư viện và đổi lại, họ được phục vụ 
trong các dịch vụ khác nhau của thư viện. Tuy 
nhiên, mối quan hệ này đang dần thay đổi 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 27
viện. Người dùng thư viện cung cấp dữ liệu 
hành vi người dùng. Thông qua các dữ liệu cá 
nhân như lịch sử sử dụng tài liệu thư viện, lịch 
sử tìm kiếm, cách thức, thói quen tìm kiếm,... 
các công cụ Big Data phân tích dữ liệu, đưa 
ra thông tin chi tiết, xác định khuynh hướng, 
nhu cầu sử dụng thư viện, nhu cầu tài liệu 
người dùng thư viện trong tương lai; các công 
cầu,... Simovic Aleksandar (2018) nhận định: 
“Các công cụ Big Data kết hợp với các thuật 
toán khoa học cho phép các thư viện dự đoán 
lai, giúp dự báo tốt hơn các vấn đề phát sinh 
tin tốt nhất cho người dùng tin” [14]. Về phía 
thư viện, việc sử dụng tài nguyên Big Data 
của người dùng tin, có thể đưa ra các ý tưởng 
các dịch vụ hiện có trong thư viện kỹ thuật số. 
Đồng thời, căn cứ vào các kết quả phân tích, 
dự báo, thư viện có thể xác định thời gian, 
và dịch vụ thư viện đến người dùng thư viện. 
Về phía người dùng thư viện, dựa vào các 
kết quả dự báo về hành vi tìm kiếm, tra cứu, 
sử dụng thư viện, các hệ thống khuyến nghị 
(Recommendation Engine) sẽ gửi đến người 
dùng tin các tài liệu có thể họ quan tâm.
Bốn là, mở rộng dịch vụ chăm sóc 
thư viện, đặc biệt trong môi trường thư viện 
điện tử, thư viện số. Các thư viện đang cố 
gắng để hiểu được người dùng thư viện, giúp 
các thắc mắc, nhu cầu của mình. Big Data 
hoạt, tạo ra giá trị từ quá trình xây dựng mối 
quan hệ thân thiết với người dùng thư viện. 
Cùng với Big Data, hệ thống trả lời tự động 
(như Chatbot) không cần sự trợ giúp của 
con người phát triển tương ứng, giúp tăng 
hiệu quả phân tích dữ liệu Big Data. Hiện 
nay, nhiều thư viện sử dụng Chatbot để giao 
tiếp, trao đổi với người dùng thư viện, tiếp 
các nhu cầu của người dùng. Khi người dùng 
viện, họ có rất nhiều thắc mắc và muốn được 
giải đáp. Chatbot sẽ đưa ra các gợi ý, hỗ trợ 
từng bước một, cung cấp thông tin về các 
sản phẩm, dịch vụ của thư viện cho người 
dùng. Chatbot được thiết kế và phát triển để 
đối thoại. Qua những dữ liệu người dùng thu 
thập được, công cụ phân tích dữ liệu Big Data 
tiến hành phân tích, xác định những nhu cầu, 
dùng thư viện. Bên cạnh đó, Chatbot nhắc 
viện như thời hạn trả tài liệu, thời hạn đổi thẻ 
sử dụng,... Đặc biệt, Chatbot giúp thư viện 
chủ động hỗ trợ 24/7, tăng trải nghiệm tối 
đa cho người dùng thư viện mọi lúc. Chatbot 
lưu lại lịch sử đối thoại, thông tin người dùng 
trong chính thư viện. Chatbot hỗ trợ các thư 
viện khai thác Big Data phục vụ người dùng. 
Trong tương lai, số thư viện sử dụng Chatbot 
tính năng và lợi ích mà Chatbot mang lại. 
Cùng với đó, thông qua dữ liệu người dùng, 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
28
các thư viện có thể phân tích, dự đoán các 
các sản phẩm, dịch vụ thông qua phân tích 
hiện các giải pháp kịp thời. 
Có thể thấy, Big Data giúp tối ưu hóa hoạt 
động thư viện bằng việc thu thập, phân tích 
thông tin, tăng trải nghiệm của người dùng 
tin bằng cách cá nhân hóa thư viện số. Cùng 
với đó, Big Data có thể giúp các thư viện tiến 
hành phân tích dự báo, tìm ra các đặc điểm 
chung dự báo thị hiếu đọc, tình trạng sử dụng 
các cơ sở dữ liệu. Không chỉ vậy, Big Data tạo 
dùng tin trong quá trình sử dụng thư viện.
1. Campbell D. Grant, Cowan Scott R. (2016). “The 
Paradox of privacy: revisiting a core library value in 
an age of big data and linked data”, Library trends, 
Vol. 64, No. 3, P. 492-811.
2. 
Boyd, 
Kate 
(2012). 
Critical Question for Big Data, Information, 
Communications and Society.
3. Doug Laney (2001). “3D Data Managment: 
controlling Data Volume, Velocity, and Variety”, 
Application Delivery Strategies, Meta Group. 
File:949.
4. Francis X. Diebold (2000). “Big Data Dynamic 
and Forecasting”, Discussion of Reichlin and 
Watson paper, in Economics and Econometrics, 
Eighth World Congress of the Econometric Society. 
5. Gartner (2013). Survey Analysis: Big Data Adoption 
in 2013 shows substance behind the hype. 
6. Gerrard, D., Mooney, J. , Thompson, D. (2017). 
“Digital Preservation at Big data scale: proposing a 
step - change in preservation system architectures”, 
Library Hi Tech, http://doi.org/10.1108/LHT-06-
2017-0122, truy cập ngày 17/10/2019.
7. Howe D. (2008). “The future of biocuration”, 
Nature 455, P. 47-50.
8. Kenneth Cukier (2010). “Data, data everywhere”, A 
special report on managing information, Economist 
Newspaper, Volume 394.
9. Kim Young Seok (2017). “Big data analysis of 
chernoff face method”, Journal of Documentation, 
Vol. 73, No. 3, P. 466-480.
10. Li Shuqing; Jiao Fúen; Zhang Yong; Xu Xia 
(2019). “Problems and Changes in Digital Libraries 
services”, Journal of Academic Librarianship, Vol, 
45.
11. Michael Cox, David Ellsworth (1997). “Application 
- Controlled Demand Paging for Out - of - Core 
Visualization”, Report NAS-97-010, NASA Ames 
Research Center.
12. Marydee Ojala (2018). “Big Data and AI: 
technology, transparency, and trust”, http://www.
infotoday.com, truy cập ngày 14/11/2019.
13. Randal E. Bryant, Randy H. Katz, và Edward D. 
Lazowska (2008). “Big - Data Computing: Creating 
in 
Commerce, 
Science and Society”, Computing Community 
Consortium, http://www.cra.org/ccc/initiatives, truy 
cập ngày 14/11/2019.
14. Simovic Aleksandar (2018). “A Big Data smart 
institution”, Library Hi Tech, Bradford, Vol. 36, Iss. 
3, tr.498-523
15. Steve Bryson, David Kenwright, Michael 
Cox, David Ellsworth, Robert Haimes (1999). 
“Visually exploring gigabyte data sets in real”, 
Communications of the ACM, Vol. 42, No.8, tr.83-
90.
16. Tan Jee Toon (2014). “Dữ liệu lớn, nhân 
tố thay đổi “cuộc chơi” của doanh nghiệp”, 
http://vneconomy.vn/cuoc-song-so/du-lieu-
lon-nhan-to-thay-doi-cuoc-choi-cua-doanh-
nghiep-20140422025542917.htm, truy cập ngày 
14/11/2019.
17. Waqar Ahmed, Kanwal Ameen (2017). “Defining 
the field of information and library management”, 
Library Hi Tech News, p. 21-24.
18. Ye Chunlei (2017). “Research on the key 
technology of big data service in university library”, 
the Institude of Electrical and Electronics Engineers, 
Inc. Conference Proceedings, Piscataway.
19. Yu Jen Chien (2016). “Library Data, Big Data or 
Better Data: Challenges from the Field”, ASIST 
Meeting, Proceeding of ASIST annual meeting, 
Vol 53, No. 1.
20. Zhan Ming, Widén Gunilla (2018). “Public 
libraries: roles in big data”, The Electronic library, 
Vol. 36, No.1, P. 133-145.
(Ngày Tòa soạn nhận được bài: 26-12-2019; 
Ngày phản biện đánh giá: 10-03-2020; Ngày chấp 
nhận đăng: 15-03-2020).
NGHIÊN CỨU - TRAO ĐỔI

Removed lines from big-data.pdf:
Adding Value to Manufacturing, Retail, Supply Chain, and 
Oklahoma State University, Stillwater, OK 74078 
The concept of big data has been around for many years.  Only in the last few years have organizations 
started to understand how they can use big data to gain insightful knowledge about their business operations, 
which is enabling them to make better business decisions. While there is no single definition, big data 
usually works on the principles of four Vs - Volume, Velocity, Variety, and Veracity. As the name suggests, 
big data is really big, meaning a huge amount of data is being generated daily, reaching the scale of 
petabytes. This data comes in all forms -  structured, semi-structured, and unstructured and is pouring in 
from all directions and generated by many systems and devices, such as transactional systems, log files, 
GPS devices, smartphones, RFID readers, surveillance cameras, sensor networks, Internet of Things (IoT), 
and social media. Finally, as big data becomes an important asset for enterprises, the focus is also on the 
trustworthiness of data and its sources. 
According to Gartner, Inc., “Big data is high-volume, high-velocity and high-variety information assets that 
demand cost-effective, innovative forms of information processing for enhanced insight and decision 
making.”a In this article, we first elaborate on the big data concept and present the storage and processing 
technologies that have been developed to deal with big data.  We then briefly discuss the evolution of 
traditional analytical processing to today’s big data analytics.   Through several applications and use cases, 
we illustrate how big data analytics is adding value to manufacturing, retail, supply chain, and logistics 
operations.  Finally, we conclude by discussing key challenges that businesses have to face as the use of 
big data analytics becomes more widespread. 
Regardless of the decision to be made - optimized production/work schedules, accurate forecasts, customer 
preferences - data nowadays has the potential to help businesses succeed more than ever before.  From an 
organizational perspective, big data is a holistic approach of obtaining actionable insight to create a 
competitive advantage over others.1 There are two distinct approaches to applying big data - improve the 
propositions. A challenge that organizations increasingly face is finding and working with trusted data. 
Working with inaccurate and untrusted data can be worse than having no data at all. As data requirements 
and regulations become more complex, organizations must be aware of where all their data is coming from, 
where it is getting stored, and who is interacting with this data as conclusions are drawn.2 
a https://www.gartner.com/it-glossary/big-data/ 
2 
What is Big Data? 
of data that needs to be handled and tracked, the speed at 
which the information is flowing into online systems, and 
regular basis. Because of the changes happening in the Web 
environment, new definitions for big data have been 
proposed, with a focus on technologies that handle this data. 
O’Reilly defines big data as “Big data is data that exceeds 
the processing capacity of conventional database systems. 
The data is too big, moves too fast, and doesn’t fit the 
structures of traditional database architectures. To gain 
value from this data, organizations must choose an 
alternative way to process it.”3 
To understand how big data is transforming businesses, we 
focus on the size of data in storage.4 Size is important but 
there are other aspects to big data namely variety, volume, 
and more recently, veracity.2 Together they are called the 4 
Vs of big data: Volume, Velocity, Variety, and Veracity. 
databases, data warehouses, and data marts.6 Here, the data 
is uploaded to operational data stores using Extract, 
Transform, and Load (ETL) tools which extract data from 
internal and external sources, transform the data to fit the 
operational needs, and finally load the data into the data 
warehouse. The key point is that the data is getting cleaned, 
transformed, and cataloged before being made available for 
data mining and online analytical functions. This traditional 
data warehouse approach discourages the incorporation of 
new data sources until they are cleansed and integrated.  
Since data is ubiquitous these days, big data storage 
environments need to be “magnetic” in nature, attracting 
data from all sources. Hence, big data calls for Magnetic, 
Agile, and Deep (MAD) analysis skills, which differs from 
the traditional data warehousing approach. Given the growing number of data sources and the sophisticated 
tools for data analysis, big data storage should allow analysts to easily process and use data rapidly. 
Solutions like distributed file systems and Massive Parallel Processing (MPP) databases are available 
nowadays for providing high query performance and platform scalability. Non-relational databases such as 
Not Only SQL (NoSQL) were developed for storing and managing unstructured data.7 These newer 
technologies aim for scalability, data model flexibility, and simplified application development and 
deployment. They separate data management and data storage and focus on high performance scalable data 
Volume. The ability to process a large 
social media, from Internet of Things to 
system logs, etc. 
Velocity. The rate at which data is 
getting created every second of the day. 
contributor, 
more 
data 
is 
generated and logged than ever before.5 
Also, the rapid adoption of social 
created a deluge of data. Advances in 
useful now. 
Variety.  It is the diversity of data 
which organizations are witnessing. 
processing a limited set of data, such as 
and 
logs. 
includes images, voice recordings, 
videos, and texts generated from 
media to deliver new insight. 
Veracity.  It is not just the quality of 
data, but also the trustworthiness of 
data sources.  Basic issues are the 
accuracy and applicability of data.  
uncertainty due to inconsistencies, 
incompleteness, ambiguities, etc. 
3 
storage, allowing management tasks to be written in the application layer instead of having it written in 
database specific languages.   
Why Big Data? 
When organizations adopt big data as a part of their business model, the first tangible question is usually 
what value this big data will provide to the company.7 Data must be used to make better decisions, to 
optimize resource consumption, and improve process quality and performance. It should also aim to 
perform precise customer segmentation, optimize customer satisfaction, and increase customer loyalty. 
from existing products and create additional revenue from new products. 
Newer Data Sources, Newer Opportunities 
The new sources of big data include industries which are taking a big step step towards digitization, and as 
a result, data growth in the past few years has been phenomenal. Some of the areas where data is coming 
from include social media, internet browsing pattern data, advertising response data, financial forecasts, 
location information, driving patterns, vehicle diagnostics, and traffic and weather data from sensors, 
monitors, and forecast systems. Other sources of data include data from healthcare, where the healthcare 
industry is implementing electronic medical records and digital imaging, which is used for short-term public 
health monitoring and long-term research programs.  Similarly, low cost gene sequencing can generate 
effectiveness in life sciences.8 Another area is data from video surveillance which is transitioning from 
patterns for security and service enhancement. Transportation and logistics industry has been generating 
and storing enormous amount of data coming from sensors, GPS transceivers, RFID tag readers, smart 
meters, cell phones, material handling equipment enabled with sensors, etc. This data can be used to 
opportunities. 
information contained therein.9 It involves applying algorithmic processes to derive insights. Analytics is 
used to extract previously unknown, useful, valid, and hidden patterns and information from large data 
sets.6 While the focus of analytics has been on inference, it can also provide prescriptive insights as 
explained later in this section.  Hence, analytics has a significant impact on research and technology, as 
businesses recognize its great potential in helping them gain competitive advantage. 
“Big data analytics is the use of advanced analytic techniques against very large and diverse data sets that 
include structured, semi-structured, and unstructured data from different sources, and in different sizes from 
terabytes to zettabytes.”b It helps in uncovering hidden patterns, unknown correlations, market trends, 
customer preferences and other useful information. Advanced analytics can help organizations discover 
what has changed and how they should react.  Analytics is the best way to discover new customer segments, 
identify the best suppliers, associate products of affinity, understand sales seasonality and so on.4 
Organizations are implementing specific forms of analytics tools and techniques which include data mining, 
statistical analysis, data visualization, artificial intelligence, machine learning, and other data capabilities 
b https://www.ibm.com/analytics/hadoop/big-data-analytics 
4 
which support analytics4. Though these techniques have been around for many years, organizations are 
using them now as most of these techniques adapt well to very large, multi-petabyte data sets. 
Big data’s worth is only realized when businesses can indulge in decision making using this data. To enable 
such data-driven decision making, organizations must use efficient processes to turn the high volume of 
fast moving and diverse data into meaningful insights. Analyzing big data allows researchers and businesses 
harness their data and use it to identify new opportunities which in turn leads to better and smarter business 
moves, more efficient operations, higher profits and satisfied customers and an overall competitive 
advantage.6 Big data analytics could be viewed as a sub-process in the complete process of knowledge 
extraction from big data.  
As organization began to adopt data analytics in the late 1990s and early 2000s, they faced many hurdles.  
professionals. Analysts used to spend more time collecting and preparing data than analyzing it.  They 
focused on finding more accurate and reliable solutions to business problems, while keeping the solutions 
simple at the same time so that business users could understand it.  Some examples of tools used during 
this time period are SAS, a tool for building backend data inference and modeling; Oracle and Teradata, 
detailed solution suites for easy development of solutions; IBM CPLEX, a tool for solving large 
optimization problems; and Cognos and MicroStrategy, tools for visualization, mostly in the form of 
reports. 
In late 2000s, social media giants like Google and Facebook and other internet-based companies in general 
started uncovering, collecting, and analyzing newer types of data which later evolved into big data. In 
addition to the data generated by companies in their internal operations and transactions, newer data was 
brought in from external sources including public data sources, social media, and mobile devices. Analysts 
realized this new data was qualitatively different (e.g., unstructured text, pictures, audio, and video) along 
with the much larger volumes as compared to internal company data.  This led to the development of newer 
tools and technologies, examples of which are Hadoop, a pioneer in distributed data storage and processing 
with low cost, flexibility, and scalability; Python and R, open source programming languages with vast and 
ever-evolving libraries for statistical data analysis; Tableau, Looker, and Microsoft Power BI, popular 
visualization products to develop, customize, and build visually appealing and interactive web dashboards. 
Descriptive, Predictive, and Prescriptive Analytics 
Analyzing data is not limited to deriving insights from the past, but it can also help businesses in predicting 
future outcomes and optimizing business performance. Currently organizations use three types of analytics 
at different stages in their decision-making process - Descriptive, Predictive, and Prescriptive analytics as 
shown in Figure 1. The latter two are also referred to collectively as advanced analytics. 
Descriptive analytics does exactly as the name suggests, ‘describe’ or summarize the data and convert it 
into something useful. It is the most basic type of analytics and almost 90% of the organizations today use 
this technique. Descriptive analytics is the analysis of historical data using data aggregation or data mining 
and lies at the bottom of the big data analytics value chain. However, it is extremely valuable because it 
organization’s future.  
Descriptive analytics is an important step to make raw data understandable to its users, and it helps in 
answering questions like “What is happening?” Consider for example, a metric that companies get from 
5 
web servers using Google Analytics tools, namely page views.  It can be used to determine if a strategy was 
a success or not. The main objective in descriptive analytics is to find the reasons behind the previous 
it can help the organization in strategizing.  
The majority of the statistics we use comes from descriptive analytics – e.g., calculations as simple as 
averages and standard deviations. Descriptive models use basic mathematical and statistical techniques to 
derive key performance indicators that can highlight the historical trends in data. STATA, MS Excel, and 
SPSS represent the older generation of descriptive analytics tools, while R and Python are quickly becoming 
the preferred tools in industry because of vast open-source libraries and the ease of development and 
deployment. Descriptive analytics can yield historical insights into an organization’s production, inventory 
levels, sales, operations, financials, and customer behavior.  
Figure 1. Analytics Framework by Tom Davenport26 
Predictive analytics can be defined as the ability to “predict” what might happen and a better understanding 
of future outcomes. It is one of the more sophisticated types of analytics techniques and employs statistical 
techniques and machine learning. It is used to detect clusters, tendencies, and exceptions, and to predict 
future trends, making it a valuable tool for forecasting. The foundation of predictive analytics is probability. 
It takes the data which the user has and tries to fill in the missing data values with best guesses. It helps in 
finding the answer to ‘What could happen?’ With properly tuned models, predictive analytics can support 
complex forecasting in marketing and sales. This helps an organization to set realistic goals for business, 
restrain expectations, and do effective planning. 
Tools used to apply predictive modeling vary by the nature of model’s complexity, but some commonly 
used tools are SAS, MATLAB, R, Python, among others. The common functionality of these tools is that 
they combine historical data found in POS, ERP, CRM, and HR systems to identify patterns in the data and 
apply algorithms such as random forest and Generalized Linear Model (GLM) for prediction, and K-means 
clustering for identifying clusters. Finally, simulation can be employed to statistically predict the outcomes 
of specific decision scenarios.   
6 
An application of predictive analytics is to produce credit scores, which are used by financial services to 
determine the probability of customer making timely payments. Other business uses include, how sales 
might close at the end of a year, inventory level forecasts, predicting what items a customer might purchase 
together and other customer purchasing patterns. Despite all the advantages that predictive analytics brings 
to the table, it is important to understand that forecasting is just an estimation, and its accuracy depends on 
the quality and stability of data.  
Prescriptive analytics is the most sophisticated analytics approach which makes use of optimization 
techniques to explore a given set of options and prescribe the best possible solution for a given scenario. 
As the name suggests, it “prescribes” a solution to a specific problem. One approach is machine learning 
neurons using training data sets.  Once trained, the neural network model can suggest the optimal course of 
action supporting the business objective for a given set of business inputs. Simulation, a predictive analytics 
tool at its core, can also be part of a powerful prescriptive analytics approach when combined with 
appropriate search or optimization techniques.  Prescriptive analytics not only predicts ‘What will happen?’, 
but also determines “What the company should do?” It provides recommendations for the actions to be 
taken to achieve optimal business performance. Because it has power to suggest optimal solutions, 
prescriptive analytics is the ultimate frontier for advanced analytics. 
Prescriptive analytical models are complex in nature. However, when implemented efficiently, prescriptive 
analytics can have a significant impact on the decision-making effectiveness of the organization. Technical 
advancements such as cloud computing have made deployment of these complex models much easier. 
prescriptive analytics to optimize production and inventory decisions in supply chains, optimize customer 
experience, and to make sure that the right product is being delivered at the right time.  Airline systems use 
travel factors, demand levels, purchasing patterns, timings, etc., in order to maximize the revenue generated.  
hence, they are ensuring to choose the right kind of analytics solutions to reduce operational cost, enhance 
service quality, and increase ROI. 
Big Data Analytics Applications and Use Cases 
Supply chain activities produce a huge amount of data, which is being continuously generated by systems 
and devices such as POS, ERP, SCM, RFID, GPS, blogs, and wiki entries, not to mention the unlimited 
data generated from sources like CCTVs, digital clickstreams, imagery, social media posts, and discussions 
on various forum platforms. Advanced connected devices and technologies which support today’s supply 
chain such as sensors, smart devices, and tags are continuously gathering real-time data and providing an 
end-to-end visibility in the supply chain. It becomes the task of supply chain managers to tap and process 
this data to make insightful decisions which could help boost productivity and reduce costs. 
7 
Application 
Area
Technique/Technology/
Supplier Insight Program Greater insight into suppliers' financial stability, 
performance, and ability to provide services.
Achieved product excellence, reduction in time-to-
market through co-development and co-production. 
Better alignment between engineers, suppliers, and 
customers.
Better service level; accurate prediction of customer 
needs and demand; automated planning and 
forecasting operations.
Inventory management with streaming analytics, real-
time data delivery and updates every few hours, and 
accurate performance analysis of each store.
Machine learning-based 
accuracy and greater profit margins.
application
Accurate forecasts, reduction in delivery time by 
upto 50%, and better service levels.
& Co.
Analytics application 
using Intel's Trusted 
Better tracking of in-store items using RFID tags; 
updating item location and inventory; helping 
salesperson track misplaced item to avoid lost sales.
Data-intensive 
Increase in forcasting accuracy; reduced inventory, 
stockouts, and obsolescence; better access to 
company's logistics needs.
Anticipatory shipping
before actual customer orders.
and packing area.
Drone-based delivery
Goods delivered to locations less than 30 minutes 
Cloud-based 3D 
Optimize picking accuracy, inventory turns, and 
warehouse productivity in real-time using inputs 
from sensors, such as  shelf weight and weight on 
forklift.
Quality early-warning 
Reduced rework, increased productivity and cost 
savings, higher quality standards, and improved 
service levels, by detecting and prioritizing quality 
related issues much sooner in the supply chain.
Greater demand and supply visibility, better 
distribution channel management, better service level, 
and improved inventory management.
collection.
Co.
Real-time monitoring and 
Greater visibility for customers, better pallet 
management, optimized space utilization, greater 
labor productivity, inventory accuracy of 99.9%, and 
improved customer satisfaction.
UPS
Optimized 55,000 delivery routes in North America, 
saving close to $400 million annually. Reducton in 
transportation.
shipment information, reduction in mileage and cost, 
and improved CO2 efficiency.
Resilience 360
Accuracy in risk detection, prevent production 
inefficiencies and revenue losses, maintain service 
levels, and reduce emergency cost by efficiently re-
routing shipments in case of unforeseen events.
analyze potential business opportunities. Real-time 
in a given location.
quality of address information is poor. Real-time 
ddress verification to optimize route planning.
DHL
Applications of Big Data Analytics
IBM
Supply Chain 
and 
8 
Applications and Use Cases in Manufacturing 
Raytheon, a major U.S. based defense contractor and industrial corporation, made use of data analytics to 
reduce costs within their supply chain and production operations. They developed a Supplier Insight 
program, which integrated structured and unstructured data from internal and external sources.10 With more 
than 10,000 suppliers, they needed a platform that could provide rapid, data-driven decision making 
capability. With this new system, they could track suppliers’ financial stability, performance, and their 
ability to provide services in the face of disruptive events. Raytheon was able to immediately identify if a 
supplier could provide what they needed, and quickly made decisions that reduced any adverse impact on 
their customers. Supplier Insight has allowed them to negotiate the cost better, by engaging in long-term 
contracts with suppliers for multiple programs.10 They now have an ability to look across all their suppliers 
and programs to achieve cost reductions. Raytheon has also developed smart factories which have the 
capacity to handle big data coming from different sources like sensors, instruments, CAD models, internet 
transactions, simulations, and digital records in the company, which equips them with real-time control of 
various elements of the production processes. For example, their Immersive Design Center (IDC) makes 
use of a 3-D immersive environment to achieve product excellence and decrease time-to-market through 
co-development and co-production of products by immersive data visualization and interaction.10 This also 
resulted in better alignment between their engineers, suppliers, and customers. They work together to refine 
the design and detect potential problems without the work and rework associated with expensive prototypes, 
resulting in reduced costs.10  
Lennox International, a U.S. based cooling and heating devices manufacturing company, integrated 
their expansion throughout North America.11 With the help of machine learning algorithms, they accurately 
predicted customer needs, while understanding customer demand better. It also helped the company to 
automate its planning and forecasting operations. 
Many companies gather data on supplier information and purchasing volumes for annual supplier 
performance review, spend analysis, and cost savings analysis functions to support strategic decisions.  For 
example, a pharmaceutical company created a database of all the bids submitted for packaging.12 This data 
was then evaluated to understand the cost structure of suppliers and to create detailed cost models for 
different packaging options. Such models can help in the selection of the most cost-effective supplier for 
new packaging.12 Another example is how IoT with its network of sensors embedded in millions of devices 
can enable new opportunities in manufacturing. For example, real-time information on a machine’s 
condition can initiate a production order for a spare part, which then can be shipped using a drone to the 
plant engineer for replacing the faulty or near faulty part.12 It also helps in determining when and how 
critical maintenance is required by a specific machine, thereby avoiding costly equipment breakdowns and 
improving the overall production efficiency.  
Daily production needs to be monitored to maintain the efficiency and output of a company. Big data 
analytics uses the data collected from operational machines, employee records, and data logs of the number 
of units produced, to provide insights to the operations manager, helping him/her to make changes that are 
profitable for the company. Manufacturers are also exploring predictive analytics to realize significant 
savings in product testing and improving product quality. Since different products and parts require 
different tests, instead of performing numerous quality tests on each part, data mining and pattern 
recognition can be used to determine the type and number of tests truly needed for each part or product.13  
9 
Applications and Use Cases in Retail 
Walmart, the number one fortune 500 company, has the world’s largest private cloud, which helps support 
real-time data feeds to its decision makers. Walmart’s Data Café based at their Bentonville, Arkansas 
headquarters takes care of most of this cloud architecture.14 Their original data infrastructure only enabled 
managers to get weekly reports, which prevented them from making decisions based on real-time market 
conditions. Also, the reports were standardized with little room for customization. Data café, which was 
built on SAP’s HANA in-memory analytics engine, enabled inventory management with streaming 
analytics, and provided an enterprise view of timely information flow for a large cross-sectional staff 
looking to resolve every-day business issues.14 The data delivered through this system is almost real-time 
and updated every few hours.  Furthermore, the system was designed to be responsive to providing reports 
and queries required by managers in the given time frame, which helped them gain timely insight and make 
better decisions. These insights are derived from “200 streams of internal and external data which includes 
40 petabytes of recent transactional data, and can be manipulated, modeled, and visualized.”14 The 
importance of near real-time insights is crucial since it helps managers respond to challenges in real-time 
as they arise. For example, on Black Friday, Walmart’s Data Café provides near real-time insights on the 
performance of east-coast stores, which enables Walmart to make pricing adjustments for west-coast stores 
before they open.14 During a recent Halloween, sales analysts were able to see that two stores were not 
selling a novelty cookie that was very popular in most stores. Using near real-time data from Data Café, it 
was discovered that simple stocking oversight led to the cookies not being put on shelves in these stores.14 
The company was able to react in real-time to avoid additional lost sales. Data Café also provides automated 
alerts to managers when a metric falls below a threshold in a department. This tool has reduced the problem-
solving time from weeks to minutes using reliable internal and external sources of data. 
Levi Strauss & Co, a leading American clothing company, provides better in-store shopping experience for 
items using IoT technology coupled with advanced analytics. Levi’s in collaboration with Intel® 
implemented a solution using Intel’s Trusted Analytics Platform (TAP), which helped salespersons to 
quickly find misplaced items in the store.15 This application made use of RFID tags woven into clothing 
items, in-store antenna sensors installed in the ceiling of the store to continuously track the RFID tags, and 
cloud-based analytical tool built on TAP for detailed analysis. This technology helped determine when 
items are no longer in their correct place or no longer available at that time. TAP algorithms use data 
collected overnight to determine the exact location of various groups of items, and during store hours 
sensors track the location of items and an algorithm determines if an item is in its assigned location. If an 
item is placed in its assigned group location, no action is generated by the algorithm.  Suppose a pair of 
jeans is lying in the T-shirt section or left in the fitting room, the TAP algorithm will generate an alert on 
the mobile application instructing the salesperson to put the item back in its assigned location.15 This helps 
the salesperson to keep the item where it belongs and avoid lost sales. Levi’s also aims to generate customer 
insight using big data analytics with the data collected from sensors tracking customers’ in-store behavior 
to better understand their preferences.15 
Groupe Danone, a French multinational food-product corporation, found itself making accurate predictions 
only 30 percent of the time for responses to promotional offers, which was resulting in significant losses to 
the company.11 When they implemented machine learning in their planning architecture, they saw 
significant improvement in both sales and forecasting. Similarly, Granarolo, an Italian dairy company, used 
machine learning to increase its forecasting accuracy by 5 percent, decreased delivery times by up to 50 
percent of the original time, which resulted in better service levels.11 Morrisons, one of UK’s largest food 
10 
retailers, was able to dramatically improve same store sales and achieve a 30% reduction in shelf gap and 
from Blue Yonder, which uses AI technology to “improve demand planning and reinvigorate replenishment 
based on customer behavior in every store.”16 Blue Yonder’s data-intensive forecasting methods deployed 
as cloud-based services is making such advanced capabilities accessible to other retailer’s as well.12 
Applications and Use Cases in Supply Chains and Warehouses 
In supply chain operations, planning and forecasting are among the most data-driven operations, which use 
an array of supply chain planning tools supported by ERP systems. With the use of supply chain analytics, 
it is now possible to re-envision the planning processes by using external and internal data sources to make 
real-time decisions based on market trends, uncertainty, seasonality, and other fluctuations.  
IBM understood the value of big data analytics early and employed it in optimizing their supply chain 
operations. They have used various analytical tools to solve a range of problems, and a few of them are 
discussed here.17 IBM’s Quality Early-Warning System (QEWS) was typically deployed upstream at 
suppliers, IBM’s operations, and in the field.  QEWS detects and prioritizes quality related issues much 
sooner than the traditional quality control processes. Analyzing big data coming from across their supply 
chain, IBM was able to reduce rework, increase productivity, ensure higher quality standards, and improve 
customer satisfaction, leading to significant cost savings. For a company like IBM, ensuring correct 
inventory levels with so many business partners was challenging. They made use of IBM Buying Analysis 
Tool, which not only provided demand and supply visibility, but ensured better distribution channel 
management, delivery of the right product at the right time to meet customer demand, while maintaining 
proper inventory levels. IBM also used a tool named Accounts Receivable, which uses advanced analytics 
to optimize the resources needed to collect revenues. They also make use of supply chain social listening, 
disrupt the supply chain.17 It also helps them obtain timely information and feedback on their products. As 
an early adopter, IBM has been using predictive and prescriptive analytics in its supply chain over the last 
several years. 
Warehousing is another area where big data analytics is creating new opportunities.  Logivations, a German 
supply chain solutions provider, offers a cloud-based 3D warehouse layout planning and optimization tool, 
camera-guided AGVs and tracking, and various other supply chain analytics solutions.18 Such technologies 
existing warehouse by simulating new configurations. Another example is the analysis of images and videos 
captured by AGVs, and sensor inputs including shelf weight and weight on the forklift, to monitor picking 
accuracy, inventory turns, and warehouse productivity in real-time.12 Also, forklift drive picking 
productivity and route optimization can be achieved by analyzing the route choices and driving behaviors.12  
A leading forklift provider is looking into all these opportunities, and figuring out how a forklift truck can 
be used as a big data hub - collecting  real-time data to identify additional sources of waste in the warehouse 
operations, using a hybrid of analytics and ERP and WMS data. Amazon is another warehouse automation 
pioneer, deploying Kiva robots that bring the items (racks) to the picking and packing area in their 
fulfillment centers. With increasing pressure to reduce order-to-delivery times, warehouses are turning to a 
flexible automation strategy by using autonomous technologies such as Amazon’s Kivac robots and 
GreyOrange’s Butlerd system to increase their picking efficiency.  Amazon has also tried to deliver goods 
c https://www.amazonrobotics.com/#/  
d https://www.greyorange.com/butler-goods-to-person-system  
11 
to people living less than 30 minutes away from an Amazon warehouse or distribution center via a drone. 
Amazon has also patented an “Anticipatory Shipping” technology to identify which orders should be 
packed and pushed into the logistics network before the actual customer orders are placed.12 
Merchandise Warehouse Co. (MW), a logistics provider of multi-temperature warehouse services in the US 
mid-west, provides services such as tempering, inspection, blast freezing, temperature monitoring, labeling, 
import/export, and packaging.19 With such operations there is little room for error, since clients’ food 
products could get spoiled if they are not maintained at correct temperatures.19 MW needed real-time 
enable quality assurance with comprehensive traceability.  They wanted this for all operations including 
inspections and holds.19 Technologies such as CCTV, WMS, electronic data interchange, mobile 
computers, and scanners were employed to help track and analyze data to get real-time information in the 
warehouse and manage inventory. It helped MW’s customers gain visibility by having on-line access to 
temperatures, activity reports, and information about inventory levels. MW’s solution also includes tools 
for pallet management for tracing every pallet from the time it arrives in the warehouse to until it leaves. 
inventory to marked temperature zones were provided by the new system. It also ensures greater labor 
productivity and accuracy using workflow-based warehouse management and could automate processes 
designed for specific customer needs. MW reaped various other benefits from this initiative like accurately 
capturing billing events in real-time resulting in reduced labor used for billing and paperwork. The system 
helped the company deal with the issue of “catch weight”, where the actual weight of the product,  especially 
meat, varies when it hits the retail shelves, a common problem in cold storage warehouses and food 
industry.19  Increased customer satisfaction levels were also achieved, since clients had real-time access to 
information and reports when needed. The solution helped MW achieve an inventory accuracy of 99.9 
percent from a previous 98.6 percent.19 
Applications and Use Cases in Logistics 
Logistics companies need to keep the goods moving at all times, even in the face of disruptions such as 
storms, cargos getting stranded due to ship crashes, and geopolitical events in order to keep the businesses 
running.  A Netherlands based logistics management company uses big data analytics on Microsoft’s Azure 
cloud to keep its customers informed about the number of goods in each container, their location at a given 
time, and expected delivery times.20 Purchase orders are tracked using mobile applications to identify 
challenges which could delay the delivery of an order. Tariff calculations and fees related to the movement 
of shipping containers are calculated by another application which can be accessed by the client, giving 
them a greater insight into financial risks.20 These mobile applications make use of big data analytics in 
internal supply chain operations to provide actionable business insights. Previously the time it took to 
identify a challenge and develop a solution to address it could be anywhere from 3 to 9 months. With the 
use of big data technologies, this time has been brought down to a couple of weeks depending upon how 
complex the problem is.20  
Companies managing their own supply chains and those outsourcing to third-party logistics providers 
manage a massive flow of freight, goods, and products daily while at the same time creating vast data sets. 
Millions of shipments are tracked daily from origins to destinations, generating information such as the 
content, weight, size, location, and route of each individual shipment, across a large number of networks. 
Companies are exploiting and analyzing these large data sets to improve their operational efficiencies, 
effectiveness, and customer service. A study by the Council of Supply Chain Management Professionals 
shows that 93 percent of shippers and 98 percent of 3PL providers feel that data-driven decision making is 
12 
a crucial supply chain activity.21 Also, 71 percent of these believe big data improves performance and 
quality. Logistics companies can utilize big data analytics to consolidate, interpret, and store the data 
coming from various sources for immediate or future use based on their requirements.  
Courier and delivery companies like UPS use real-time routing of deliveries using the trucks’ geo-location 
and traffic information data. UPS spent almost 10 years developing its On-Road Integrated Optimization 
and Navigation system (ORION) to optimize close to 55,000 routes in North America in its delivery 
network.22 This system saves the company $300 million to $400 million annually by saving about 100 
million miles per year, which is a reduction of 10 million gallons of fuel consumed and reducing CO2 
emissions by almost 100,000 metric tons.22  Data mining techniques also help logistics companies deliver 
services with fewer delivery attempts, by using predictive analytics to predict when a customer is more 
likely to be available at home.12 Costs and carbon emissions can also be reduced by selecting the right mode 
of transportation for deliveries. An example is the use of supply chain analytics to understand the priority 
and can be delivered by rail.12  
Better transportation planning can be achieved with the use of Transportation Management System (TMS) 
which can help identify future shipping patterns, optimize routes, carrier selection, or loads, and secure 
necessary capacity. This is achieved by tracking shipment frequency and identifying the endpoints of supply 
chains by studying precise inbound and outbound statistics. Direct application of predictive analytics is 
helping logistics providers make real-time decisions which result in reduced costs, greater reliability, and 
improved customer satisfaction. For example, data streams produced by sensors on delivery trucks, beacons 
which broadcast their presence to nearby devices such as computers and smartphones, radar devices, and 
employing simulation models.23 When a shipment is going to be late, a carrier can make real-time 
adjustments to prevent bottlenecks further down the supply chain.23 
DHL, a global logistics provider, has extensively explored big data analytics in their supply chain activities 
and is currently employing several smart systems around their services. Increasing the last mile efficiencies 
is often the most expensive step in the supply chain.24 Last mile optimization is an extensively studied area 
and researchers have found promising applications of big data analytics here. Data analytics is applied to 
achieve real-time optimization of delivery routes, where streams of data are processed to maximize the 
performance of the delivery fleet. Rapid processing of real-time information supports the goal of route 
optimization on the last mile, saving time in the delivery process. When the vehicles are loaded and 
unloaded, manual sequencing of shipments is eliminated by the use of sensors, and dynamic calculations 
are used to find the optimal delivery sequence. Based on real-time traffic conditions on the road, telematic 
databases are used to change the delivery route automatically. DHL’s SmartTruck uses data mining, 
machine learning, and other data analytics techniques to optimize the initial tour planning based on 
incoming shipment on a daily basis.24 Dynamic routing system recalculates the routes depending on the 
traffic situations and delivery times. This also results in cost reduction and improved CO2 efficiency by 
reducing the miles travelled. 
It is vital for robust supply chains to be able to cope with unforeseen events in today’s rapidly changing 
world. Apart from being flexible and resilient, businesses need accurate risk detection systems to keep 
running smoothly. Big data analytics and complex event processing algorithms are used to alert businesses 
when a pattern falls in the set of critical conditions such as tornadoes or floods in an area, or breakdown of 
fleet. These alert systems send a report on the probability and impact of the risk and provide suitable 
actionable insight to alleviate potential interruption. With this information on hand, customers can re-route 
their shipments or manage supplies from other distribution locations. DHL’s Resilience 360 risk 
13 
management solutions aims to provide such functionalities.24 It is equipped with two components, a risk 
assessment portion and supply chain monitoring instruments, both operating in real time. This improves the 
resilience of the supply chain and prevents production inefficiencies and revenue losses. Resilience 360 is 
designed to maintain prescribed service levels, protect sales and operations, and reduce emergency costs, 
creating a competitive advantage for the company.24 
Future economic development is often modeled on global transportation of goods and services. The type of 
goods shipped indicate the local demand and supply preferences. Logistics providers make use of big data 
shipments by their distribution networks. These shipment records are a valuable resource for market 
intelligence research, and logistics providers refine this data to substantiate existing market research. 
Regression analysis techniques are used to produce demand and supply forecasts with the use of the 
shipment records and market research outcomes. The primary target group for these advanced data analytics 
services are small and medium-sized enterprises, which lack capacity to conduct their own market research. 
The results from regression-based analytics have high predictive value, which can help these enterprises 
serve a larger customer base, and generate accurate forecasts based on industry, geography, and product 
category. DHL Geovista is one such online geo-marketing tool available for small and medium-sized 
enterprises to analyze potential business opportunities.24  
shipments more accurately.24 Customer’s delivery address verification is a fundamental requirement for 
any logistics provider. This can be troublesome in developing countries and other remote areas, where the 
area. Address Management uses daily freight and parcel delivery data and matches this data with reference 
data and returns the incorrect incoming data with validated data from the database, in order to verify the 
address in real-time and optimize route planning for retailers and public sector entities.  
Other Applications 
There are several other applications of big data analytics which a company can encounter on a regular basis. 
Locating a new store is a strategic decision for a company, and big data analytics could play an important 
role here. Extensive data analysis is performed by the analysts in exploring customer data, demographic 
factors, retailer network, location of other competitors in the area, and market potential. A recent example 
of this is the location for Amazon’s HQ2. Visualizing the growth of a company has become easier with the 
use of data analytics, since it is now possible to quickly compare the performance matrix of different sites 
and identify the reasons behind such results. Predictive analytics comes in handy in analyzing the market 
and gaining insight on questions related to global growth strategy, site relocation, new product introduction, 
and supplier selection. 
business profitable.25 Data analytics tools simplify the process of price formation, which not only accounts 
for the cost of production of an item, but also the spending capacity of the customers and presence of 
competitors in the market. Price flexibility, buying patterns of the customers, competitors’ prices, and 
seasonality are analyzed using the data coming from various sources. Machine learning algorithms help 
to changes in prices. Furthermore, using real-time price optimization techniques, retailers can attract new 
customers and retain existing customers by adjusting the price as per market trends. Recommendation 
engines is another great way of predicting customers’ behavior, since they give a retailer insight into 
customers’ reviews and opinions. It also helps the retailers to increase sales and stay abreast with trends. 
14 
Based on machine learning algorithms, recommendation engines make adjustments depending on customer 
preferences, previous shopping and browsing experience, demographic data, need, and usefulness. 
Collaborative or content-based data filtering is used in this process to gain useful insight which gives 
leverage to retailers on customers’ opinions.  
Companies often fail to understand what big data is, its benefits, and more importantly the computing and 
the human infrastructure required to realize its true potential. Without a clear understanding of the concept 
of big data, adopting and implementing a project using big data tools can seriously challenge its success. 
Having discussed various applications and use cases of implementing big data technologies in 
manufacturing, retail, supply chain, and logistics, it is important to understand the associated challenges. 
they decide to invest in technologies using big data.  
reducing their dependence on legacy systems. Even though the industry is shifting its focus to the digital 
age with adoption of IoT and artificial intelligence, it is still a long way before the full potential of big data 
is realized. Industry has to develop an awareness of the various elements of the big data landscape, which 
include sensors to social media that collect data, in-memory to cloud for data storage, data mining to deep 
learning to convert data into useful business insights or actions.  Any new business solution will involve a 
significantly. Most people are resistant to change, and it shows in companies when workers stick to to an 
old way of thinking and doing work. An example is the use of Excel, which to the present day remains one 
of the popular tools in many companies, despite having  many limitations when compared to newer tools.27 
While there is a need to educate industry to change this legacy mentality, there is no need for an abrupt or 
complete shift to newer tools. A viable option is to slowly augment existing systems with big data analytics 
tools and capabilities.   
With the phenomenal increase in the size of data, the problem of storage space for big data has become a 
real problem for many companies. Cloud storage is soon becoming the only viable alternative with the ever-
increasing need for storage space.  With the maturity of the cloud computing infrastructure, which includes 
storage, applications, and computing platforms, companies are beginning to consider shifting to the cloud 
infrastructure for most of their computing needs. But transitioning from the traditional in-house computing 
infrastructure to the cloud infrastructure has its own challenges.  According to McAfee, “Most organizations 
that have been around awhile have a hodgepodge of hardware, operating systems, and applications, often 
described as ‘legacy spaghetti’.”28 First, companies have to address legacy system issues and simplify their 
system before moving to the cloud. For the most part, cloud is cost-effective compared to building and 
running an IT infrastructure.  However, a company needs to carefully evaluate the cost factor based on their 
specific needs, for example, in-house applications requiring continuous transfer of large data sets. 
analytics with specialized MS degrees in Data Science.  These degree programs are housed mostly in 
business schools or computer science departments.  Engineering schools to a large extent are still lagging 
in providing adequate training in data science to their graduates.   Data science professionals can manage 
and analyze large volumes of real-time data coming from multiple sources and in different formats. With 
several new technologies such as the NoSQL data management framework, Hadoop, cloud computing, and 
in-memory analytics, their skills are vital for the rapidly changing computing landscape. Given that 
engineering schools are still looking for the right curriculum mix (e.g., minors, degree options, and 
15 
certificates) to train engineers in data science, training employees at entry level is a challenging and 
expensive proposition for companies dealing with these newer technologies. When industry hires data 
science professionals, akin to software developers and programmers, they need guidance from subject 
matter experts (SMEs) to build the right tools and techniques that can help industry harness the power of 
big data in the long-run.  Industry needs to quickly educate SMEs to understand the big data analytics 
professionals.   
As seen in recent times, data privacy has become one of the major concerns of organizations. With recent 
threats like hacking of personal data, individuals and companies have become apprehensive about linking 
data from multiple sources as it may compromise an individual’s privacy. Also, with an increase in the 
number of connected devices within the industry, data security has also become a big concern and presently 
this risk is greater than ever. Big data analysis uses huge amounts of data for analysis and mining purposes 
to reach some meaningful conclusion, and security of this big data can be enhanced by using techniques 
such as authentication, authorization, and encryption.   
Effective flow and sharing of information among supply chain partners is critical to the success of today’s 
digital supply chains.  Unauthorized disclosure and data leakage of information shared among supply chain 
partners have been identified as two main threats in today’s digital supply chains.29 Visibility needed within 
a supply chain and consumers’ demand for transparency seem to be at odds with security requirements.  
With newer, secure technologies such as blockchain and data cleanroom, it is possible to achieve both 
visibility and transparency.30  Data cleanroom is a shared environment between two or more supply chain 
visibility to their data. Blockchain, a decentralized, distributed database is one of the most secure options 
available for supply chain partners for real-time information tracking.  Another important, but often 
overlooked challenge is the ethical use of data.  The legal infrastructure has not kept up with the rapid 
development in technology, which is able to collect and store vast amounts of consumer data with or without 
their knowledge.  While it may be legal, certain use of the data may be considered unethical.  Such actions 
may have a negative impact on a company as today’s consumers are more educated and have experienced 
negative consequences of such unethical usage. 
In a recent survey of supply chain professionals conducted by APQC, “lack of people with the needed 
skills” was identified as the biggest barrier to advanced analytics applications in industry.31  In addition, 
these employees need “a good understanding of the business to provide solid advice.”29  Resistance to 
change and lack of access to data across disparate systems were the second and third biggest barriers, 
respectively.  In addition to lack of access to data, issues such as inconsistent and unorganized data are also 
issues in some cases as different companies record their data in different formats, platforms, and systems.27 
useful insights.  
As companies make a push for big data analytics applications, they should first establish a clear business 
need such as “solving a problem or seizing an opportunity.”7  According to Watson, “big data initiatives 
should start with a specific or narrowly defined set of objectives rather than a ‘build it and they will come’ 
approach.”7 Pilot schemes are a good way to demonstrate the value of big data analytics.32  It is common to 
focus the initial business case for big data analytics on customer-centric objectives.7 The various 
applications and uses cases discussed earlier cover many different areas that have benefited from big data 
analytics. Whatever be the area, it is desirable that the pilot project address a problem tied to a specific 
16 
business outcome.  The pilot project should not only help solve a business problem, but also demonstrate 
the effectiveness of big data analytics for the organization and its stakeholders. Finally, for successful big 
data initiatives it is essential to have strong, committed sponsorship and alignment between the business 
and analytics strategies.7 In the early stages of adoption, the sponsor could be the CIO and then shifting to 
function-specific executives as business opportunities are identified.   
To benefit from big data analytics companies must also establish a data-driven decision-making culture, 
which calls for acting on insights from data rather than on pure managerial intuition.32 Promotion of data-
sharing practices, increased availability of training in data analytics, and communication of the benefits of 
data-driven decision making are some of the strategies for promoting a data-drive culture.7 While workforce 
training needs to focus on improving technological and digital proficiency, the future work environment 
also demands training in certain soft skills.  The work environment is changing with the rapid introduction 
of AI, automation, and analytics-driven solutions.  Workers need to be open to new ways of working and 
have openness to agility, adaptability, and working in teams to cope with a constantly changing external 
environment.  In the long-run, big data needs to become an integral part of the organization’s operating 
model. There also needs to be clear ownership for big data in the organization with leadership positions 
such as a chief analytics officer.32 Data science should become another established skill in the organization.   
during the development of this white paper.  We would like convey our appreciation to Scott Wahl for his 
guidance and feedback during the formative stages of this effort.  We would also like to thank John 
Ashodian, John Hill, Ying Tat Leung, Juan Ma, Hari Padmanabhan, and John Paxton for carefully reading 
an earlier version of this white paper and providing several constructive suggestions and feedback, which 
have helped us greatly improve the quality of the white paper. 
17 
1. Morten Brinch, Jan Stentoft, and Jesper K. Jensen, “Big Data and its Applications in Supply Chain 
Management: Findings from a Delphi Study,” Proceedings of the 50th Hawaii International Conference 
on System Sciences, 2017: 1351-1360.  
2. IBM Corporation, “The Path to Data Veracity,” IBM Big Data and Analytics Hub, May 2018, 
https://www.ibmbigdatahub.com/whitepaper/path-data-veracity 
3. DataStax Corporation, “Big Data: Beyond the Hype,” October 2013, 
https://www.datastax.com/resources/whitepapers/bigdata 
4. Phillip Russom, “Big Data Analytics,” TDWI Research, 2011, 
https://tdwi.org/research/2011/09/best-practices-report-q4-big-data-
analytics.aspx?tc=page0&m=1  
5. DXC Technology Company, “Five Industries Where Big Data is Making a Difference,” November 
2015, https://assets1.dxc.technology/analytics/downloads/DXC-Analytics-
Five_Industries_Where_Big_Data_is_Making_a_Difference-4AA5-6292ENW.pdf 
6. Nada Elgendy and Ahmed Elragal, “Big Data Analytics: A Literature Review Paper,” In: Perner P. 
(eds) Advances in Data Mining. Applications and Theoretical Aspects. ICDM 2014. Lecture Notes in 
Computer Science, vol 8557, Springer, Cham., 2014, https://link.springer.com/chapter/10.1007/978-
3-319-08976-8_16  
7. Hugh J. Watson, "Tutorial: Big Data Analytics: Concepts, Technologies, and Applications," 
Communications of the Association for Information Systems, 34 (2014), Article 65. 
http://aisel.aisnet.org/cais/vol34/iss1/65  
8. Richard L. Villars, Carl W. Olofson, and Matthew Eastwood, “Big Data: What It Is and Why You 
Should Care,” International Data Corporation, 2011. 
http://www.tracemyflows.com/uploads/big_data/idc_amd_big_data_whitepaper.pdf 
9. Sunil Tiwari, H.M. Wee, and Yosef Daryanto, “Big Data Analytics in Supply Chain Management 
Between 2010 and 2016: Insights to Industries,” Computers and Industrial Engineering, 115 (2017): 319-
330.  
10. Bob Trebilcock, “Supply Chain, Data Analytics, and Big Data,” Logistics Management, August 2015.  
https://www.logisticsmgmt.com/article/supply_chain_data_analytics_and_big_data  
11. Kaushik Pal, “How Machine Learning Can Improve Supply Chain Efficiency,” Techopedia, February-
2018.  https://www.techopedia.com/2/31846/trends/big-data/how-machine-learning-can-improve-
supply-chain-efficiency  
12.  McKinsey & Company, “Big Data and the Supply Chain: The Big-Supply-Chain Analytics 
Landscape: Part 1,” February 2016,  https://www.mckinsey.com/business-functions/operations/our-
insights/big-data-and-the-supply-chain-the-big-supply-chain-analytics-landscape-part-1#  
13. Lorenzo Romano, “Big Data Analytics: A Key Ingredient for Agility in Manufacturing,” May 2019, 
https://www.orange-business.com/en/blogs/big-data-analytics-key-ingredient-agility-manufacturing  
18 
14. Joe McKendrick, “Walmart’s Gigantic Private Cloud for Real-Time Inventory Control,” RT 
Insights.com, January 2017. https://www.rtinsights.com/walmart-cloud-inventory-management-
real-time-data/  
15. RT Insights team, “Levi’s Real-Time Tracking of Jeans: RFID in Retail,” RT Insights.com, April 
2016. https://www.rtinsights.com/rfid-in-retail-customer-experience-levis/ 
16.  JDA, “Store Replenishment at Morrisons,” 2017, https://jda.com/knowledge-center/collateral/by-
morrisons-case-study  
17.  Hans W. Ittmann, “The Impact of Big Data and Business Analytics on Supply Chain Management,” 
Journal of Transport and Supply Chain Management, 9, no. 1 (2015). 
https://jtscm.co.za/index.php/jtscm/article/view/165/331  
18. Logivation, https://www.logivations.com/en/solutions/plan/design_efficiency.php  
19. RT Insights team, “Using Mobile Device for a Real-Time Warehouse,” 2016, 
https://www.rtinsights.com/zebra-omnii-xt15-datek-real-time-warehouse/  
 20. Motifworks, “How Big Data Analytics Can Benefit Supply Chain and Logistics Industry,” 2017. 
https://motifworks.com/2017/02/23/how-big-data-analytics-can-benefit-supply-chain-logistics-
industry/  
 21. “2017 Third-Party Logistics Study,” https://jda.com/-/media/jda/knowledge-center/thought-
leadership/2017stateoflogisticsreport_new.ashx  
 22. UPS, “ORION Backgrounder,” 2019, 
https://www.pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=Factsheet
s&id=1426321616277-282  
23. “Data-Driven Logistics: The Growing Use of Predictive Analytics,” July 2018, https://www.smith-
howard.com/data-driven-logistics-the-growing-use-of-predictive-analytics/ 
 24. Martin Jeske, Moritz Grüner, and Frank Weiẞ, “Big Data in Logistics – A DHL Perspective on How 
to Move Beyond the Hype,” December 2013. 
http://www.dhl.com/content/dam/downloads/g0/about_us/innovation/CSI_Studie_BIG_DATA.pdf  
25.  McKinsey & Company, “Big Data, Analytics, and the Future of Marketing and Sales,” March 2015, 
https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/Marketing%20and%20Sales/Our
%20Insights/EBook%20Big%20data%20analytics%20and%20the%20future%20of%20marketing%20sal
es/Big-Data-eBook.ashx 
26. Gurobi Optimization, “The Power of Analytics,” accessed September 8, 2019. 
http://www.gurobi.com/resources/prescriptive-analytics,  
27.  Transmetrics, “ Big Data and Big Roadblocks:  How the Logistics Industry can Overcome its Big 
Data Challenges,” March 2018, https://www.youredi.com/blog/logistics-industry-can-overcome-big-data-
28. Andrew McAfee, “What Every CEO Needs to Know About the Cloud,” Harvard Business Review, 
Nov. 2011: 124-132. 
19 
29. Bharat Bhargava, Rohit Ranchal, and Lotfi Ben Othmane, “Secure Information Sharing in Digital 
Supply Chains,” 3rd IEEE International Advanced Computing Conference, May 2013, 
https://www.cs.purdue.edu/homes/bb/Bhargava-Supply_Chain-Feb2013-india.pdf  
30. Megan Ray Nicholas, “How to Share Data Safely Across your Supply Chain,” 
https://www.smartdatacollective.com/share-data-safely-across-supply-chain/  
31. APQC, “APQC Quick Poll:  The Current State of Big Data & Advanced Analytics in Supply Chain,” 
May 2019, 
https://www.scmr.com/article/apqc_quick_poll_the_current_state_of_big_data_advanced_analytics_in_su
pply  
32. David Meer, “A Call to Action on Big Data,” Forbes, October 2014, 
https://www.forbes.com/sites/strategyand/2014/10/28/a-call-to-action-on-big-data/#6a4b6c22314  

Removed lines from KimAnh-HTKhoa.pdf:
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381804857
Tích Hợp Big Data Và Điện Toán Đám Mây: Động Lực Thúc Đẩy Thay Đổi Cho
Doanh Nghiệp.
Conference Paper · June 2024
CITATIONS
0
READS
376
1 author:
Vo Thi Kim Anh
Ton Duc Thang University
28 PUBLICATIONS   2 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Vo Thi Kim Anh on 29 June 2024.
The user has requested enhancement of the downloaded file.
KỶ YẾU
HỘI THẢO KHOA HỌC
KHOA CÔNG NGHỆ THÔNG TIN
LẦN 6
2024
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
i 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
KHOA CÔNG NGHỆ THÔNG TIN 
-------------- 
KỶ YẾU HỘI THẢO 
KHOA HỌC CÔNG NGHỆ LẦN 6 
Thành Phố Hồ Chí Minh, tháng 06 năm 2024 
(Lưu hành nội bộ) 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
ii 
BAN BIÊN TẬP 
1. TS. Nguyễn Hà Giang - Trưởng Khoa CNTT 
2. TS. Văn Thị Thiên Trang - Phó Trưởng Khoa CNTT 
3. ThS. Nguyễn Minh Tuấn - Phó Trưởng Khoa CNTT 
4. ThS. Trần Thành Công - Trợ lý Trưởng Khoa, Trưởng Ngành TMĐT 
5. ThS. Hoàng Văn Hiếu - Trưởng Ngành CNTT 
6. ThS. Võ Đình Ngà - Trưởng Ngành TKĐH 
7. ThS. Nguyễn Thị Hoài Linh - Trưởng Ngành KHDL 
8. ThS. Ngô Văn Công Bằng - Trưởng Bộ môn THUD 
9. ThS. Trương Nhã Bình - Trưởng Bộ môn Toán 
THƯ KÝ 
1. KS. Phạm Hữu Kỳ – Giảng viên Khoa CNTT 
2. Trần Thị Phương Anh – Thư ký Khoa CNTT 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iii 
LỜI GIỚI THIỆU 
Công nghệ thông tin đã và đang là yếu tố cốt lõi thúc đẩy nền kinh tế - xã hội phát triển 
mạnh mẽ, đặc biệt trong thời đại kỹ thuật số ngày nay. Sự bùng nổ của các công nghệ mới 
và ứng dụng tiên tiến đã thay đổi cách chúng ta sống, làm việc và tương tác. Với mục đích 
tạo ra một diễn đàn để các nhà nghiên cứu, học giả, giảng viên, cũng như các chuyên gia, 
trao đổi kết quả nghiên cứu, chia sẻ kiến thức, thảo luận quan điểm, ý tưởng về các xu 
hướng mới nhất trong lĩnh vực công nghệ thông tin và ứng dụng, Khoa Công nghệ thông 
tin, Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh (UEF) tổ chức hội thảo 
với chủ đề “Hội thảo khoa học công nghệ Khoa CNTT lần 6 năm 2024”.  
Hội thảo không chỉ nhằm mục đích nâng cao năng lực nghiên cứu mà còn thúc đẩy các 
phát minh, đổi mới và chuyển giao công nghệ trong lĩnh vực công nghệ thông tin. Đây là 
cơ hội để các chuyên gia đầu ngành, nhà nghiên cứu, giảng viên và sinh viên gặp gỡ, học 
hỏi và hợp tác, cùng nhau phát triển và ứng dụng các thành tựu khoa học kỹ thuật vào thực 
tiễn. Qua đó, hội thảo mong muốn góp phần nâng cao chất lượng giáo dục, nghiên cứu và 
thực hành trong lĩnh vực công nghệ thông tin. 
Do thời gian chuẩn bị có hạn, việc biên tập Kỷ yếu này không tránh khỏi những thiếu 
sót. Ban biên tập rất mong ý kiến đóng góp cũng như sự lượng thứ từ quý độc giả để các 
kỳ hội thảo sau được tổ chức ngày một tốt hơn, hiệu quả hơn 
Trân trọng! 
Tp. Hồ Chí Minh, tháng 6 năm 2024 
BAN BIÊN TẬP 
KHOA CÔNG NGHỆ THÔNG TIN 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
141-145 ĐIỆN BIÊN PHỦ, P.15, Q.BÌNH THẠNH, TP.HCM 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iv 
TỐI ƯU HÓA TRUY VẤN TRONG SQL SERVER: PHƯƠNG PHÁP VÀ ỨNG 
DỤNG..........................................................................................Trang 1 
Nguyễn Minh Tuấn - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KÊ CỦA CÔNG CỤ CHATGPT.......................................................Trang 14 
Nguyễn Văn Vinh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
RANSOMWARE: MỐI ĐE DỌA TRONG THỜI ĐẠI SỐ........................Trang 24 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
CHỮ KÝ.....................................................................................Trang 29 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC THÚC ĐẨY 
THAY ĐỔI CHO DOANH NGHIỆP.................................................Trang 35 
Võ Thị Kim Anh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
NHÂN CHO SINH VIÊN NGÀNH THIẾT KẾ ĐỒ HỌA.........................Trang 44 
Võ Đình Ngà - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
ỨNG DỤNG BÀI TOÁN VẬN TẢI: TỐI ƯU CHI PHÍ THU GOM RÁC SINH 
HOẠT CỦA CÁC BỆNH VIỆN..........................................................Trang 59 
Trương Nhã Bình - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
Ngô Thuận Dủ - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
DOANH CỦA DOANH NGHIỆP.......................................................Trang 70 
Hoàng Văn Hiếu - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
SÁNG TẠO NỘI DUNG AI: CÁCH MẠNG HÓA TƯƠNG LAI CỦA TIẾP THỊ NỘI 
DUNG .......................................................................................Trang 85 
Trần Thành Công - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 35 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC 
THÚC ĐẨY THAY ĐỔI CHO DOANH NGHIỆP. 
INTEGRATION OF BIG DATA AND CLOUD COMPUTING: A 
1Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh, anhvtk@uef.edu.vn  
Tóm tắt: Kỷ nguyên số mang đến sự bùng nổ dữ liệu, tạo ra cả thách thức và cơ hội cho doanh nghiệp. 
Sự hội tụ của Big Data và điện toán đám mây nổi lên như giải pháp mạnh mẽ, cách mạng hóa cách 
thức xử lý và khai thác dữ liệu. Bài viết này khám phá tác động biến đổi của sự kết hợp này, đồng thời 
đề xuất những cân nhắc thực tế cho doanh nghiệp bắt đầu áp dụng Big Data trên nền tảng đám mây. 
Từ khóa: Kỷ nguyên số, Big Data, điện toán đám mây, biến đổi, doanh nghiệp. 
Abstract: The digital era has ushered in an unprecedented surge of data, presenting both challenges 
and opportunities for businesses. The convergence of big data and cloud computing has emerged as a 
powerful solution, revolutionizing the way data is processed and harnessed. This paper delves into the 
embarking on their big data on cloud journey. 
Key words: Digital Era, Big Data, Cloud Computing, Transformation, Business 
1. Sự kết hợp mạnh mẽ giữa Big Data và 
liệu, mang đến cả thách thức và cơ hội cho 
doanh nghiệp. Khái niệm Big Data, với đặc 
trưng khối lượng, tốc độ và sự đa dạng, lần 
đầu tiên được giới thiệu bởi Laney (2001) [1] 
và khai thác thông tin. Tuy nhiên, việc quản 
minh là rất phức tạp. 
Sự xuất hiện của điện toán đám mây [2] 
Data, cung cấp giải pháp mạnh mẽ để giải 
quyết thách thức này. Điện toán đám mây 
internet, giúp doanh nghiệp tận dụng tối đa 
linh hoạt. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 36 
Dikaiakos et al. (2009) [3] nhấn mạnh về khả 
năng mở rộng, hiệu quả chi phí và khả năng 
truy cập. 
Đối với khả năng mở rộng: cơ sở hạ tầng 
trên nhu cầu xử lý, loại bỏ nhu cầu đầu tư ban 
đầu tốn kém vào phần cứng. Doanh nghiệp chỉ 
cần trả tiền cho các tài nguyên họ sử dụng, tối 
tức đầu tư [3]. 
Về hiệu quả về chi phí: doanh nghiệp chỉ 
trả tiền cho các tài nguyên họ sử dụng, tối ưu 
đầu tư [3]. 
Còn đối với khả năng truy cập: các giải 
năng truy cập mọi lúc, mọi nơi, thúc đẩy cộng 
tác và sự linh hoạt. 
động, phát triển sản phẩm mới, gia tăng lợi thế 
doanh đầy biến động (xem thống kê Bảng 1).
Bảng 1: Lợi ích của Big Data và Điện toán đám mây  
Tự động mở rộng/thu hẹp tài nguyên, tối ưu hóa chi phí. 
Chỉ trả tiền cho tài nguyên sử dụng. 
Truy cập mọi lúc, mọi nơi, thúc đẩy cộng tác. 
Tự động hóa quy trình, cải thiện ra quyết định, tối ưu hóa 
chuỗi cung ứng. 
Xác định xu hướng thị trường và nhu cầu khách hàng. 
Đưa ra quyết định sáng suốt và nhanh chóng dựa trên dữ liệu. 
Phân tích dữ liệu để dự đoán rủi ro và nắm bắt cơ hội mới. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 37 
2. Ứng dụng thực tiễn của Big Data  
giới thực. Các doanh nghiệp đang tận dụng 
động, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Dưới đây là một số ví dụ cụ 
thể:  
Trước tiên, đó là ở ngành bán lẻ: Các gã 
nền tảng đám mây để quản lý hàng tồn kho, 
thông tin chi tiết về khách hàng [4]. Ví dụ, câu 
chuyện về Amazon retail (Amazon.com). 
Ngày 10 tháng 11 năm 2010 là ngày 
Amazon.com tắt máy chủ web vật lý cuối 
cùng 
trong 
trung 
tâm 
dữ 
liệu 
của 
Amazon.com ([17]). Amazon.com là nhà bán 
lẻ trực tuyến lớn nhất thế giới. Do 
Amazon.com tạo ra rất nhiều dữ liệu, trước 
trữ dữ liệu đó. Nhưng khi Amazon.com phát 
triển lớn hơn, kích thước cơ sở dữ liệu Oracle 
cùng khó khăn. Điều này khiến họ phải cân 
AWS. Bằng cách chuyển sang AWS, họ đã 
trải nghiệm cải thiện hiệu suất gấp 12 lần và 
giảm thời gian khôi phục từ khoảng 15 giờ 
xuống 2,5 giờ ([18]). Amazon.com đã vượt 
qua chi phí cao, hiệu suất chậm và quản lý tốn 
AWS. Họ tận dụng Amazon S3 vì tính tiết 
kiệm chi phí, khả năng mở rộng, bảo mật và 
lưu trữ bền vững, giúp sao lưu và khôi phục 
nhanh hơn đáng kể. Ngoài ra, việc di chuyển 
mạch. Nhìn chung, việc chuyển sang AWS 
giúp giảm chi phí, cải thiện hiệu quả và cung 
phát triển của Amazon (Bảng 3, 4). 
Trong ngành chăm sóc sức khỏe: Ngành 
Data. Nghiên cứu của [5] cho thấy các tổ chức 
mây để phân tích dữ liệu bệnh nhân, từ đó cải 
sáng kiến nghiên cứu. Ví dụ, Mayo Clinic sử 
điều trị mới, chẩn đoán bệnh chính xác hơn và 
cải thiện hiệu quả chăm sóc.  
Và trong ngành dịch vụ tài chính: Phân 
chính. Các nghiên cứu điển hình của [6] cho 
để xác định các giao dịch gian lận, đánh giá 
rủi ro tín dụng và quản lý danh mục đầu tư. Ví 
dụ, JPMorgan Chase sử dụng Big Data để 
phát hiện các trường hợp rửa tiền, ngăn chặn 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 38 
đầu tư.  
vực. Các giải pháp Big Data dựa trên nền tảng 
doanh đầy biến động. Bảng 2 sau đây thống 
kê các ứng dụng:
Bảng 2: Ứng dụng thực tiễn của Big Data 
Quản lý hàng tồn kho, cá nhân hóa 
chiến dịch tiếp thị, thu thập thông tin 
khách hàng, đề xuất sản 
[4, 18] 
Phân tích dữ liệu bệnh nhân, cải thiện 
chất lượng chăm sóc, thúc đẩy nghiên 
cứu 
phương pháp điều trị mới, 
[5] 
Quản lý rủi ro, phát hiện gian lận, 
đánh giá rủi ro tín dụng, quản lý danh 
rửa tiền, ngăn chặn gian lận 
thẻ tín dụng, tối ưu hóa danh 
[6] 
Bảng 3: Bảng so sánh Lưu trữ truyền thống vs Lưu trữ đám mây Amazon S3 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Chi phí trả trước cao cho phần cứng băng, dung 
lượng trung tâm dữ liệu và giấy phép phần mềm. 
Mô hình trả tiền theo nhu cầu, loại 
bỏ chi phí trả trước. 
liệu ngày càng tăng. 
của Amazon. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 39 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Sao lưu và phục hồi chậm do thời gian đọc băng. 
kể so với băng. 
liệu, dễ bị lỗi phần cứng. 
11 số chín (99.999999999%). 
tầng băng. 
cầu quản lý tối thiểu. 
Bảng 4: Bảng so sánh Máy chủ cục bộ vs AWS EC2 đám mây 
Máy chủ On-premises 
trung tâm dữ liệu cục bộ. 
chuyên dụng để giao tiếp liền mạch. 
máy chủ web, cơ sở dữ liệu và 
các công cụ. 
AWS. 
3. Giải quyết thách thức và triển khai hiệu 
toán đám mây mang lại nhiều lợi ích, nó cũng 
thận. Bảo mật dữ liệu là một trong những 
mối quan tâm hàng đầu. Pearson (2013) [7] 
vệ dữ liệu nhạy cảm trên đám mây. Các biện 
pháp này bao gồm: mã hóa dữ liệu, kiểm soát 
quyền truy cập, và tuân thủ các quy định. 
truy cập trái phép. Kiểm soát quyền truy cập 
vào dữ liệu và mức độ truy cập của dữ liệu đó. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 40 
Đối với việc tuân thủ các quy định, như là, 
bảo mật dữ liệu, chẳng hạn như quy định về 
bảo vệ dữ liệu (GDPR) của Liên minh châu 
Âu. 
pháp Big Data trên nền tảng đám mây. 
Achanta (2023) [8] và Setiyawan & Patel 
(2019) [9] đã nêu bật tầm quan trọng của 
việc: chất lượng dữ liệu, và tuân thủ quy định. 
việc xem xét dữ liệu tính chính xác, đầy đủ 
và nhất quán để có thể phân tích hiệu quả. 
quản lý dữ liệu, chẳng hạn như Sarbanes-
Oxley Act (SOX) của Hoa Kỳ.  
Ngoài ra, còn có một số thách thức khác 
Big Data trên nền tảng đám mây, bao gồm: sự 
tương tác, kỹ năng nhân sự, và chi phí triển 
khai – vận hành. Về khả năng tương tác, thì 
doanh nghiệp. Đối với các kỹ năng, thì doanh 
đám mây. Còn lại, đối với quản lý chi phí, thì 
toán đám mây. 
thức và cân nhắc này, các doanh nghiệp có thể 
tranh (Bảng 3). 
Bảng 5: Giải quyết thách thức và triển khai hiệu quả Big Data dựa trên điện toán đám mây 
Mã hóa mạnh mẽ, kiểm soát quyền truy 
cập, tuân thủ quy định 
[7] 
Đảm bảo chất lượng dữ liệu, tuân thủ 
[8, 9] 
[10] 
[11, 12] 
[13, 14] 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 41 
4. Tương lai của việc ra quyết định dựa 
mây. Các xu hướng mới nổi như điện toán 
lý và sử dụng dữ liệu. 
Theo Banjanovic & Husaković (2023) 
[15], điện toán biên tích hợp phân tích Big 
liệu thời gian thực tại ranh giới của mạng. 
nguồn khác nhanh chóng và hiệu quả hơn. 
McGrath & Brenner (2017) [16] cho rằng 
liệu. Nhờ vậy, doanh nghiệp có thể thúc đẩy 
đổi mới và tăng trưởng nhanh hơn. 
Sự kết hợp của Big Data, điện toán đám 
tâm trong việc ra quyết định, đổi mới và tăng 
trưởng (Bảng 4). Doanh nghiệp cần nắm bắt 
đại dữ liệu. 
Bảng 6: Tương lai của việc ra quyết định dựa trên dữ liệu 
biên 
mạng 
liệu nhanh chóng, hiệu quả 
[15] 
chủ 
[16] 
năng lưu trữ, xử lý và phân tích dữ liệu mạnh 
mẽ. Nhờ đó, doanh nghiệp có thể nâng cao 
hiệu quả hoạt động, hiểu rõ hơn về khách 
hàng, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Việc nắm bắt sức mạnh của Big 
số. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 42 
mây hiệu quả, doanh nghiệp cần xác định rõ 
mục tiêu, lựa chọn giải pháp phù hợp, xây 
dựng chiến lược quản trị dữ liệu, đầu tư đào 
án nhỏ đến mở rộng dần. Việc áp dụng thành 
doanh nghiệp thành công trong kỷ nguyên số. 
hiệu quả, doanh nghiệp có thể khai thác sức 
đạt được những lợi ích to lớn. 
[1] Laney, D. (2001) 3D Data Management: 
Controlling Data Volume, Velocity and Variety. 
META Group Research Note, 6. 
[2] Armbrust, M., Griffith, R., Joseph, A. D., Katz, 
R., Konwinski, A., Lee, G., Patterson, D., et al. 
(2010). 
A 
view 
of 
computing. 
Communications of the ACM, 53(4), 50-58. 
ACM. 
[3] Dikaiakos, M., Katsaros, D., Mehra, P., Pallis, G., 
& Vakali, A. (2009). “Cloud computing: 
scientific research”. IEEE Internet Computing, 
13(5), 10-13. 
[4] Chen, W., Li, J., & Jin, X. J. (2016). The 
replenishment policy of agri-products with 
stochastic demand in integrated agricultural 
supply chains. Expert Systems with Applications, 
48, 55-66. 
[5] Halamka, J. (2014). The Argonaut Project 
Charter. Life as a Healthcare CIO. 
[6] Rizvi, S. (2021). Role of big data in financial 
institutions for financial fraud. SSRN Electronic 
Journal, 4, 35. 
[7] Pearson, S. (2013). Privacy, Security and Trust in 
Cloud Computing. In: Pearson, S., Yee, G. (eds) 
Privacy and Security for Cloud Computing. 
and 
Networks. 
Springer, London. https://doi.org/10.1007/978-1-
4471-4189-1_1  
[8]  Achanta, M. (2023). Data governance in the age of 
cloud computing: Strategies and considerations. 
(IJSR), 12, 1338-1343. 
[9]  Setiyawan, D., & Patel, C. (2019). A proposed 
and data management in higher education. SSRN 
Electronic Journal, 6, 19-25. 
[10] Agrawal, D. & Das, S. & Abbadi, A. (2011). Big 
Data and Cloud Computing: Current State and 
Opportunities. 
ACM 
Series. 
530-533. 
10.1145/1951365.1951432. 
[11] Ghaleb, E.A.A.; Dominic, P.D.D.; Fati, S.M.; 
Muneer, A.; Ali, R.F. 2021. The Assessment of Big 
Data Adoption Readiness with a Technology–
Organization–Environment 
Framework: 
A 
Employees. 
2021, 
13, 
8379. 
https://doi.org/10.3390/su13158379 
[12] Shamim, S., Zeng, J., Choksy, U.S. & Shariq, S. 
M. 2020. Connecting big data management 
employee level, International Business Review, 
Volume 29, Issue 6, 101604, ISSN 0969-5931, 
https://doi.org/10.1016/j.ibusrev.2019.101604. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 43 
[13] Muniswamaiah, M., Agerwala, T. & Tappert, C. 
(2019). Big data in cloud computing review and 
opportunities.  International Journal of Computer 
Science & Information Technology (IJCSIT) Vol 
11, 
No 
4. 
https://arxiv.org/ftp/arxiv/papers/1912/1912.108
21.pdf 
[14] El-Seoud, S. A., El-Sofany, H. F., Abdelfattah, 
M. A. F., & Mohamed, R. (2017). Big Data and 
Cloud Computing: Trends and Challenges. 
(iJIM), 
11(2), 
pp. 
34–52. 
https://doi.org/10.3991/ijim.v11i2.6561 
[15] Banjanovic, M. L., & Husaković, A. (2023). Edge 
AI: Reshaping the Future of Edge Computing 
with 
Intelligence. 
10.5644/PI2023.209.07. 
[16] McGrath, G., & Brenner, P. R. (2017). 
"Serverless 
Computing: 
Design, 
Implementation, and Performance," 2017 IEEE 
Computing Systems Workshops (ICDCSW), 
Atlanta, GA, USA, 2017, pp. 405-410, doi: 
10.1109/ICDCSW.2017.36. 
[17] [Amazon Web Services]. (2012, December 10). 
AWS re: Invent ENT 205- Drinking Our Own 
[Video]. 
Https://www.Youtube.com/User/AmazonWebSe
rvices/Cloud 
. 
https://www.youtube.com/watch?v=f45Uo5rw6
YY  
[18] Chavan, A. (2020, September 21). How Amazon 
retail (Amazon.Com) uses the AWS cloud. 
Medium. 
June 
7, 
2024, 
from 
https://ankush-chavan.medium.com/how-
amazon-retail-amazon-com-uses-the-aws-cloud-
View publication stats

Removed lines from sybca-bigdata-ppt.pdf:
Introduction to Big Data
What is Data?
The quantities, characters, or symbols on which operations are performed by a computer, 
magnetic, optical, or mechanical recording media.
What is Big Data?
Big Data is also data but with a huge size. Big Data is a term used to describe a 
collection of data that is huge in volume and yet growing exponentially with time. In 
tools are able to store it or process it efficiently.
“Extremely large data sets that may be analyzed computationally to reveal patterns , 
trends and association, especially relating to human behavior and interaction are 
known as Big Data.”

Following are some the examples of Big Data-
The New York Stock Exchange generates about one terabyte of new trade data per day.
Social Media
The statistic shows that 500+terabytes of new data get ingested into the databases of social 
media site Facebook, every day. This data is mainly generated in terms of photo and video 
uploads, message exchanges, putting comments etc.
A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many 
thousand flights per day, generation of data reaches up to many Petabytes.
Name
Size(In Bytes)
Bit
1/8
1/2 (rare)
Byte
1
1024 bytes
1024
1, 024kilobytes
1, 048, 576
1, 024 megabytes
1, 073, 741, 824
1, 024 gigabytes
1, 099, 511, 627, 776
1, 024 terrabytes
1, 125, 899, 906, 842, 624
1, 024 petabytes
1, 152, 921, 504, 606, 846, 976
1, 024 exabytes
1, 180, 591, 620, 717, 411, 303, 424
1, 024 zettabytes
1, 208, 925, 819, 614, 629, 174, 706, 176
Characteristics Of Big Data
•
The following are known as “Big Data Characteristics”.
1. Volume
2. Velocity
3. Variety
4. Veracity
1. Volume:
Volume means “How much Data is generated”. Now-a-days, 
very vast amount of Data say TB(Tera Bytes) to PB(Peta Bytes) to Exa
Byte(EB) and more.
2. Velocity:
Velocity means “How fast produce Data”. Now-a-days, Organizations or 
fast rate.
3. Variety:
Variety means “Different forms of Data”. Now-a-days, Organizations or 
rate in different formats. We will discuss in details about different formats of 
Data soon.
4. Veracity
Veracity means “The Quality or Correctness or Accuracy of Captured Data”. 
Out of 4Vs, it is most important V for any Big Data Solutions. Because without 
Correct Information or Data, there is no use of storing large amount of data at 
fast rate and different formats. That data should give correct business value.
Types of Digital Data
1. Structured
2. Unstructured
3. Semi-structured
Structured

Any data that can be stored, accessed and processed in the form of fixed format is 
termed as a 'structured' data. 

Over the period of time, talent in computer science has achieved greater success in 
developing techniques for working with such kind of data (where the format is well 
known in advance) and also deriving value out of it.

However, nowadays, we are foreseeing issues when a size of such data grows to a huge 
extent, typical sizes are being in the range of multiple zettabytes.
Do you know? 1021 bytes equal to 1 zettabyte or one billion terabytes forms a zettabyte.
given and imagine the challenges involved in its storage and processing.
Do you know? Data stored in a relational database management system is one 
example of a 'structured' data.
• Examples Of Structured Data
An 'Employee' table in a database is an example of Structured Data
2365
Male
650000
3398
650000
7465
Male
500000
7500
Male
500000
7699
550000
Unstructured

Any data with unknown form or the structure is classified as unstructured data.

In addition to the size being huge, un-structured data poses multiple challenges in terms 
of its processing for deriving value out of it.

combination of simple text files, images, videos etc. 

Now day organizations have wealth of data available with them but unfortunately, they 
don't know how to derive value out of it since this data is in its raw form or unstructured 
format.
• Examples Of Un-structured Data
The output returned by 'Google Search'
Semi-structured

Semi-structured data can contain both the forms of data. 

We can see semi-structured data as a structured in form but it is actually not defined 
with e.g. a table definition in relational DBMS.

Example of semi-structured data is a data represented in an XML file.
Examples Of Semi-structured Data
Personal data stored in an XML file-
<rec><name>Prashant Rao</name><sex>Male</sex><age>35</age></rec>
<rec><name>Seema R.</name><sex>Female</sex><age>41</age></rec>
<rec><name>Satish Mane</name><sex>Male</sex><age>29</age></rec>
<rec><name>Subrato Roy</name><sex>Male</sex><age>26</age></rec>
<rec><name>Jeremiah J.</name><sex>Male</sex><age>35</age></rec>
Big Data Analytics
Big Data Analytics: 

Big Data analytics is the process of collecting, organizing and analyzing 
large sets of data (called Big Data) to discover patterns and other useful 
information.

that is most important to the business and future business decisions. 
from analyzing the data.
High-Performance Analytics Required: 

To analyze such a large volume of data, Big Data analytics is typically 
performed using specialized software tools and applications for predictive 
analytics, data mining, text mining, forecasting and data optimization. 

high-performance analytics.

large volumes of data that a business has collected to determine which data is 
relevant and can be analyzed to drive better business decisions in the future.
The Challenges:

For most organizations, Big Data analysis is a challenge. Consider the sheer 
volume of data and the different formats of the  
data(both structured and unstructured data) that is collected across the entire 
combined, contrasted and analyzed to find patterns and other useful business 
information.

organization stores in different places and often in different systems. 

easily as structured data. 

This massive volume of data is typically so large that it's difficult to process 
using traditional database and software methods.
How Big Data Analytics is Used Today:

data improves, business can be transformed in all sorts of ways. 

Today's advances in analyzing big data allow researchers to decode human DNA in 
minutes, predict where terrorists plan to attack, determine which gene is mostly likely 
to be responsible for certain diseases and, of course, which ads you are most likely to 
respond to on Facebook.

Another example comes from one of the biggest mobile carriers in the world.

France's Orange launched its Data for Development project by releasing subscriber 
data for customers in the Ivory Coast.

The 2.5 billion records, which were made anonymous, included details on calls and 
text messages exchanged between 5 million users.

as the foundation for development projects to improve public health and safety.

cell phone data to map where people went after emergencies; another showed how to 
use cellular data for disease containment. (source)
The Benefits of Big Data Analytics:

data. Many big data projects originate from the need to answer specific 
business questions. With the right big data analytics platforms in place, an 
enterprise can boost sales, increase efficiency, and improve operations, 
customer service and risk management.

Webopedia parent company, QuinStreet, surveyed 540 enterprise decision-
companies plan to use Big Data analytics to improve operations. About half 
of all respondents said they were applying big data analytics to improve 
customer retention, help with product development and gain a competitive 
advantage.

Notably, the business area getting the most attention relates to increasing 
efficiency and optimizing operations. Specifically, 62 percent of respondents 
said that they use big data analytics to improve speed and reduce complexity.
Application of Big Data 
Here is the list of top Big Data applications in today’s world:
•
•
•
Big Data in E-commerce
•
•
•
•
•
Let’s discuss the applications of Big Data in detail.
1. Big Data in Retail

The retail industry is the one that faces the most fierce competition of all. Retailers 
constantly hunt for ways that will give them a competitive edge over others. 
Customers are the real king sounds legit for the retail industry in particular.

For retailers to thrive in this competitive world, they need to understand their 
customers in a better way. If they are aware of their customers’ needs and how to 
fulfill those needs in the best possible way, then they know everything.

– Big Data in Retail.

Through advanced analysis of their customer’s data, retailers are now able to 
understand them from every angle possible. They gather this data from various 
sources such as social media, loyalty programs, etc.

Even a minute detail about any customer has now become significant for them. They are 
now closer to their customers than they have ever been. This empowers them to provide 
customers with more personalized services and predict their demands in advance.

This helps them in building a loyal customer base. Some of the biggest names in the retail 
world like Walmart, Sears and Holdings, Costco, Walgreens, and many more now have Big 
Data as an integral part of their organizations.

are responsible for as much as 30% of retail annual sales.
2. Big Data in Healthcare

Big Data and healthcare are an ideal match. It complements the healthcare industry better 
than anything ever will. The amount of data the healthcare industry has to deal with is 
unimaginable.
Gone are the days when healthcare practitioners were incapable of harnessing this data. 
From finding a cure to cancer to detecting Ebola and much more, Big Data has got it all 
under its belt and researchers have seen some life-saving outcomes through it.

medications. Data analysts are harnessing this data to develop more and more effective 
treatments. Identifying unusual patterns of certain medicines to discover ways for 
developing more economical solutions is a common practice these days.

Explore how Big Data helps to speed up the treatment process – Big Data in 
Healthcare.

people of all age groups. This generates massive amounts of real-time data in the 
form of alerts which helps in saving the lives of the people.
3. Big Data in Education

When you ask people about the use of the data that an educational institute gathers, the 
need it for future references.

Even you had the same perception about this data, didn’t you? But the fact is, this data 
holds enormous importance. Big Data is the key to shaping the future of the people and 
has the power to transform the education system for better.

curriculum. Additionally, universities can even track the dropout rates of the students 
and are taking the required measures to reduce this rate as much as possible.
4. Big Data in E-commerce

One of the greatest revolutions this generation has seen is that of E-commerce. It is now part 
and parcel of our routine life. Whenever we need to buy something, the first thought that 
provokes our mind is E-commerce. And not your surprise, Big Data has been the face of it.

Some of the biggest E-commerce companies of the world like Amazon, Flipkart, Alibaba, and 
popularity Big Data has gained in recent times.

Big Data is now as important as anyone else in these organizations. Amazon, the biggest E-
commerce firm in the world and one of the pioneers of Big Data and analytics, has Big Data as 
the backbone of its system. Flipkart, the biggest E-commerce firm in India, has one of the most 
robust data platforms in the country.

See how Flipkart used Big Data to have one of the most robust data platforms.

Big Data’s recommendation engine is one of the most amazing applications the Big Data world 
has ever witnessed. It furnishes the companies with a 360-degree view of its customers.

Companies then suggest customers accordingly. Customers now experience more personalized 
services than they have ever had. Big Data has completely redefined people’s online shopping 
experiences.
5. Big Data in Media and Entertainment

sheer piece of art. Art and science are often considered to be the two completely 
contrasting domains but when employed together, they do make a deadly duo and Big 
Data’s endeavors in the media industry are a perfect example of it.

Viewers these days need content according to their choices only. Content that is 
relatively new to what they saw the previous time. Earlier the companies 
broadcasted the Ads randomly without any kind of analysis.

But after the advent of Big Data analytics in the industry, companies now are 
aware of the kind of Ads that attracts a customer and the most appropriate time to 
broadcast it for seeking maximum attention.

Customers are now the real heroes of the Media and entertainment industry -
courtesy to Big Data and Analytics.
6. Big Data in Finance

data is one of the toughest challenges any financial firm faces. Data has been the second most 
important commodity for them after money.

Even before Big Data gained popularity, the finance industry was already conquering the 
technical field. In addition to it, financial firms were among the earliest adopters of Big Data 
and Analytics.

has been at the heart of it. Big Data is bossing the key areas of financial firms such as fraud 
detection, risk analysis, algorithmic trading, and customer contentment.

This has brought much-needed fluency in their systems. They are now empowered to focus 
more on providing better services to their customers rather than focussing on security issues. 
Big Data has now enhanced the financial system with answers to its hardest of the challenges.
7. Big Data in Travel Industry

with it, the travel industry was a bit late to realize its worth. Better late than never though. 
Having a stress-free traveling experience is still like a daydream for many.

And now Big Data’s arrival is like a ray of hope, that will mark the departure of all the 
hindrances in our smooth traveling experience.
See how Big Data is revolutionizing the travel & tourism sector.

Through Big Data and analytics, travel companies are now able to offer more 
customized traveling experience. They are now able to understand their customer’s 
requirements in a much-enhanced way.

From providing them with the best offers to be able to make suggestions in real-time, 
Big Data is certainly a perfect guide for any traveler. Big Data is gradually taking the 
window seat in the travel industry.
8. Big Data in Telecom

The telecom industry is the soul of every digital revolution that takes place around the world. 
With the ever-increasing popularity of smartphones, it has flooded the telecom industry with 
massive amounts of data.

And this data is like a goldmine, telecom companies just need to know how to dig it properly. 
Through Big Data and analytics, companies are able to provide the customers with smooth 
connectivity, thus eradicating all the network barriers that the customers have to deal with.
Companies now with the help of Big Data and analytics can track the areas with the lowest as 
well as the highest network traffics and thus doing the needful to ensure hassle-free network 
connectivity.
Big Data alike other industries have helped the telecom industry to understand its customers 
pretty well. 
Telecom industries now provide customers with offers as customized as possible.

Big Data has been behind the data revolution we are currently experiencing.
9. Big Data in Automobile

“A business like an automobile, has to be driven, in order to get results.” B.C. Forbes

smoothly. Big Data is driving the automobile industry towards some unbelievable and never 
before results.

wings to it. Big Data has helped the automobile industry achieve things that were beyond our 

From analyzing the trends to understanding the supply chain management, from taking care 
of its customers to turning our wildest dream of connected cars a reality, Big Data is well 
and truly driving the automobile industry crazy.

Removed lines from TNG_QUAN_V_D_LIU_LN_BIGDATA.pdf:
1 
TỔNG QUAN Vӄ DӲ LIӊU LӞN (BIGDATA) 
Ks. Nguyễn Công Hoan 
Trung Tâm Thông tin Khoa học thống kê (Viện KHTK) 
Trước đây, chúng ta mới chỉ biết đến dữ liệu có cấu trúc (structure data), ngày 
nay, với sự kết hợp của dữ liệu và internet, đã xuất hiện một dạng khác của dữ liệu -  Big 
data (dịch là “dữ liệu lớn”). Dữ liệu này có thể từ các nguồn như: hồ sơ hành chính,giao 
dịch điện tử, dòng trạng thái (status), chia sẻ hình ảnh, bình luận, nhắn tin...của chính 
chúng ta, nói cách khác chúng là dữ liệu được sản sinh qua quá trình chia sẻ thông tin 
trực tuyến liên tục của người sử dụng. Để cung cấp cái nhìn tổng quan, chúng tôi xin giới 
liệu lớn mang lại.    
1.  Khái niӋm, đặc trưng của dӳ liӋu lӟn và sự khác biӋt vӟi dӳ liӋu truyӅn thống 
1.1. Khái niệm về dữ liệu lớn 
- Theo wikipedia: Big data là một thuật ngữ chỉ bộ dữ liệu lớn hoặc phức tạp mà các 
phương pháp truyền thống không đӫ các ứng dөng để xử lỦ dữ liệu này. 
-  Theo Gartner: Dữ liệu lớn là những nguồn thông tin có đặc điểm chung khối lượng lớn,  
tốc độ nhanh và dữ liệu định dạng dưới nhiều hình thức khác nhau, do đó muốn khai thác 
được  đòi hỏi phải có hình thức xử lỦ mới để đưa ra quyết định, khám phá và tối ưu hóa 
quy trình.  
1.2. Nguồn hình thành dữ liệu và phương pháp khai thác và quản lý dữ liệu lớn. 
Qua thống kê và tổng hợp, nguồn dữ liệu lớn được hình thành chӫ yếu từ 6 nguồn: 
(1) Dữ liệu hành chính (phát sinh từ chương trình cӫa một tổ chức, có thể là chính phӫ 
hay phi chính phӫ). Ví dө, hồ sơ y tế điện tử ở bệnh viện, hồ sơ bảo hiểm, hồ sơ ngân 
hàng...; (2) Dữ liệu từ hoạt động thương mại (phát sinh từ các giao dịch giữa hai thực 
thể). Ví dө, các giao dịch thẻ tín dөng, giao dịch trên mạng, bao gồm cả từ các thiết bị di 
động; (3) Dữ liệu từ các thiết bị cảm biến như thiết bị chөp hình ảnh vệ tinh, cảm biến 
đường, cảm biến khí hậu; (4) Dữ liệu từ các thiết bị theo dõi, ví dө theo dõi dữ liệu từ 
điện thoại di động, GPS; (5) Dữ liệu từ các hành vi, ví dө như tìm kiếm trực tuyến về 
(một sản phẩm, một dịch vө hay thông tin khác), đọc các trang mạng trực tuyến...; (6) Dữ 
liệu từ các thông tin về  Ủ kiến, quan điểm cӫa các cá nhân, tổ chức, trên các phương tiện 
thông tin xã hội. 
theo các nguồn hình thành dữ liệu lớn. Mỗi nguồn dữ liệu lớn khác nhau sẽ có phương 
pháp khai thác và quản lỦ dữ liệu lớn khác nhau. Tuy nhiên, hiện nay phần lớn các tổ 
dữ liệu lớn. 
1.3. Đặc trưng 5V cͯa dữ liệu lớn 
Dữ liệu lớn có 5 đặc trưng cơ bản như sau (mô hình 5V):   
(1) Khối lượng dữ liệu (Volume)  
2 
Đây là đặc điểm tiêu biểu nhất cӫa dữ liệu lớn, khối lượng dữ liệu rất lớn. Kích cỡ 
cӫa Big Data đang từng ngày tăng lên, và tính đến năm 2012 thì nó có thể nằm trong 
khoảng vài chөc terabyte cho đến nhiều petabyte (1 petabyte = 1024 terabyte) chỉ cho 
một tập hợp dữ liệu. Dữ liệu truyền thống có thể lưu trữ trên các thiết bị đĩa mềm, đĩa 
cứng. Nhưng với dữ liệu lớn chúng ta sẽ sử dөng công nghệ “đám mây” mới đáp ứng khả 
năng lưu trữ được dữ liệu lớn. 
 (2) Tốc độ (Velocity) 
Tốc độ có thể hiểu theo 2 khía cạnh: (a) Khối lượng dữ liệu gia tăng rất nhanh (mỗi 
giây có tới 72.9 triệu các yêu cầu truy cập tìm kiếm trên web bán hàng cӫa Amazon); (b) 
Xử lỦ dữ liệu nhanh ở mức thời gian thực (real-time), có nghĩa dữ liệu được xử lỦ ngay 
tức thời ngay sau khi chúng phát sinh (tính đến bằng mili giây). Các ứng dөng phổ biến 
trên lĩnh vực Internet, Tài chính, Ngân hàng, Hàng không, Quân sự, Y tế – Sức khỏe như 
hiện nay phần lớn dữ liệu lớn được xử lỦ real-time. Công nghệ xử lỦ dữ liệu lớn ngày nay 
đã cho phép chúng ta xử lỦ tức thì trước khi chúng được lưu trữ vào cơ sở dữ liệu. 
(3) Đa dạng (Variety) 
Đối với dữ liệu truyền thống chúng ta hay nói đến dữ liệu có cấu trúc, thì ngày nay 
hơn 80% dữ liệu được sinh ra là phi cấu trúc (tài liệu, blog, hình ảnh, vi deo, bài hát, dữ 
liệu từ thiết bị cảm biến vật lỦ, thiết bị chăm sóc sức khỏe...). Big Data cho phép liên kết 
và phân tích nhiều dạng dữ liệu khác nhau. Ví dө, với các bình luận cӫa một nhóm người 
dùng nào đó trên Facebook với thông tin video được chia sẻ từ Youtube và Twitter. 
(4) Độ tin cậy/chính xác (Veracity) 
Một trong những tính chất phức tạp nhất cӫa Dữ liệu lớn là độ tin cậy/chính xác cӫa 
dữ liệu. Với xu hướng phương tiện truyền thông xã hội (Social Media) và mạng xã hội 
(Social Network) ngày nay và sự gia tăng mạnh mẽ tính tương tác và chia sẻ cӫa người 
dùng Mobile làm cho bức tranh xác định về độ tin cậy & chính xác cӫa dữ liệu ngày một 
khó khăn hơn. Bài toán phân tích và loại bỏ dữ liệu thiếu chính xác và nhiễu đang là tính 
chất quan trọng cӫa BigData. 
 (5) Giá trị (Value) 
Giá trị là đặc điểm quan trọng nhất cӫa dữ liệu lớn, vì khi bắt đầu triển khai xây 
thông tin mang lại như thế nào, khi đó chúng ta mới có quyết định có nên triển khai dữ 
liệu lớn hay không. Nếu chúng ta có dữ liệu lớn mà chỉ nhận được 1% lợi ích từ nó, thì 
không nên đầu tư phát triển dữ liệu lớn. Kết quả dự báo chính xác thể hiện rõ nét nhất về 
giá trị cӫa dữ liệu lớn mang lại. Ví dө, từ khối dữ liệu phát sinh trong quá trình khám, 
chữa bệnh sẽ giúp dự báo về sức khỏe được chính xác hơn, sẽ giảm được chi phí điều trị 
và các chi phí liên quan đến y tế.  
1.4. Sự khác biệt giữa dữ liệu lớn với dữ liệu truyền thống  
3 
Dữ liệu lớn khác với dữ liệu truyền thống (ví dө, kho dữ liệu - Data Warehouse) ở 4 
điểm cơ bản: Dữ liệu đa dạng hơn; lưu trữ dữ liệu lớn hơn; truy vấn nhanh hơn; độ chính 
xác cao hơn. 
(1) Dữ liệu đa dạng hơn: Khi khai thác dữ liệu truyền thống (Dữ liệu có cấu trúc), 
chúng ta thường phải trả lời các câu hỏi: Dữ liệu lấy ra kiểu gì? định dạng dữ liệu như thế 
nào? Đối với dữ liệu lớn, không phải trả lời các câu hỏi trên. Hay nói khác, khi khai thác, 
chúng; điều quan tâm là giá trị mà dữ liệu mang lại có đáp ứng được cho công việc hiện 
tại và tương lai hay không. 
(2) Lưu trữ dữ liệu lớn hơn: Lưu trữ dữ liệu truyền thống vô cùng phức tạp và luôn 
đặt ra câu hỏi lưu như thế nào? dung lượng kho lưu trữ bao nhiêu là đӫ? gắn kèm với câu 
hỏi đó là chi phí đầu tư tương ứng. Công nghệ lưu trữ  dữ liệu lớn hiện nay đã phần nào 
có thể giải quyết được vấn đề trên nhờ những công nghệ lưu trữ đám mây, phân phối lưu 
xác và xử lỦ nhanh trong thời gian thực. 
(3) Truy vấn dữ liệu nhanh hơn: Dữ liệu lớn được cập nhật liên tөc, trong khi đó 
tin đáp ứng theo yêu cầu. 
(4) Độ chính xác cao hơn: Dữ liệu lớn khi đưa vào sử dөng thường được kiểm định 
lại dữ liệu với những điều kiện chặt chẽ, số lượng thông tin được kiểm tra thông thường 
rất lớn, và đảm bảo về nguồn lấy dữ liệu không có sự tác động cӫa con người vào thay 
đổi số liệu thu thập. 
2. Bͱc tranh tổng thể ͱng dụng dữ liệu lớn  
 Dữ liệu lớn đã được ứng dөng trong nhiều lĩnh vực như: hoạt động chính trị; giao 
thông; y tế; thể thao; tài chính; thương mại; thống kê... dưới đây là một số ví dө về ứng 
dөng dữ liệu lớn. 
2.1. Ͱng dụng dữ liệu lớn trong hoạt động chính trị 
cӫa mình. Ông xây dựng một đội ngũ nhân viên chuyên đi 
triển khai về dữ liệu lớn. Đội ngũ nhân viên này thu thập tất 
cả thông tin về người dân ở các khu vực, sau đó phân tích và 
chỉ ra một số thông tin quan trọng về người dân Mỹ như: 
Thích đọc sách gì, thích mua loại thuốc gì, thích sử dөng phương tiện gì... Thậm chí còn 
biết được cả thông tin về mẹ cӫa cử tri đó đã bỏ phiếu tín nhiệm ai ở lần bầu cử trước. 
Trên cơ sở những thông tin này, Tổng thống Obama đã đưa ra kế hoạch vận động phù 
hợp, giúp ông tái đắc cử Tổng thống nước Mỹ lần thứ 2. 
4 
dөng như: Hệ thống chính phӫ điện tử; phân tích quy định và việc tuân thӫ quy định; 
phân tích, giám sát, theo dõi và phát hiện gian lận, mối đe dọa, an ninh mạng. 
2.2. Ͱng dụng dữ liệu lớn trong giao thông 
dòng giao thông trong thành phố vào các giờ cao điểm, từ đó có 
những kế hoạch phân luồng giao thông chi tiết, hợp lỦ giúp giảm 
thiểu kẹt xe. Ngoài ra còn đưa ra thông tin cho người tham gia 
đi vào giờ nào để tránh kẹt xe, hoặc đi đường nào là ngắn nhất.v.v. Ngoài ra dữ liệu lớn 
còn giúp phân tích định vị người dùng thiết bị di động, ghi nhận chi tiết cuộc gọi trong 
thời gian thực; và giảm thiểu tình trạng ùn tắc giao thông. 
2.3. Ͱng dụng dữ liệu lớn trong y tế 
để đưa ra dự đoán về nguy cơ mắc bệnh. Đồng thời cũng đưa ra 
được xu hướng lây lan cӫa bệnh. Ví dө, ứng dөng Google Flu 
dөng này dựa trên từ khóa tìm kiếm ở một khu vực nào đó, sau đó 
kiếm đó, sau cùng là đưa ra dự báo về xu hướng dịch cúm tại khu 
vực đó. Qua đó cho biết tình hình cúm tại khu vực đó sẽ diễn ra như thế nào để đưa ra các 
giải pháp phòng tránh. Những kết quả mà Google Flu Trend đưa ra, hoàn toàn phù hợp 
với báo cáo cӫa Tổ chức y tế thế giới WHO về tình hình bệnh cúm tại các khu vực đó. 
2.4. Ͱng dụng dữ liệu lớn trong thể thao 
cӫa đội tuyển Đức (hình bên) đã đưa ra những điểm bất hợp lỦ 
trong cấu trúc cӫa đội tuyển Đức, từ đó giúp cho đội tuyển Đức 
khắc phөc được điểm yếu và đã dành được World cup 2014. 
2.5. Ͱng dụng dữ liệu lớn trong tài chính 
Từ những dữ liệu chính xác, kịp thời thu thập được thông qua các giao dịch cӫa 
khách hàng, tiến hành phân tích, xếp hạng và quản lỦ các rӫi ro trong đầu tư tài chính, tín 
dөng. 
2.6. Ͱng dụng dữ liệu lớn trong thương mại 
sau: Phân khúc thị trường và khách hàng; phân tích hành vi khách hàng tại cửa hàng; tiếp 
thị trên nền tảng định vị; phân tích tiếp thị chéo kênh, tiếp thị đa kênh; quản lỦ các chiến 
dịch tiếp thị và khách hàng thân thiết; So sánh giá; Phân tích và quản lỦ chuỗi cung ứng; 
Phân tích hành vi, thói quen người tiêu dùng. 
2.7. Ͱng dụng dữ liệu lớn trong thống kê 
5 
thức, Ӫy ban Thống kê Liên hợp quốc cũng như các tổ chức thống kê khu vực và cơ quan 
thống kê quốc gia cӫa nhiều nước đã triển khai hàng loạt các hoạt động về Bigdata như: 
Hàn Quốc sử dөng ảnh vệ tinh để thống kê nông nghiệp và một số lĩnhvực khác;Australia 
sử dөng ảnh vệ tinh để thống kê diện tích đất nông nghiệp và năng suất; Italia sử dөng dữ 
liệu điện thoại di động để thống kê di cư; Bhutan dùng thiết bị di động để tính toán chỉ số 
giá tiêu dùng; Estonia dùng điện thoại di động định vị vệ tinh để thống kê du lịch; 
3. Nhӳng cơ hội và thách thͱc khi ͱng dụng Big data trong thống kê chính thͱc 
3.1 Cơ hội  
(1) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp cho chúng ta có thêm phương án 
giải quyết, xử lỦ và đối phó với những thách thức đối sản xuất số liệu thống kê chính thức 
trong hiện tại và tương lai. Những nghiên cứu thực nghiệm cần phải được tiến hành để 
khám phá những ứng dөng tiềm năng cӫa dữ liệu lớn trong số liệu thống kê chính thức, 
và nghiên cứu thực nghiệm đó phải là một phần trong quy trình sản xuất số liệu thống kê. 
(2) Nghiên cứu về dữ liệu lớn cần phải có cơ sở hạ tầng công nghệ thông tin hiện 
đại, đáp ứng các yêu cầu xử lỦ khối lượng lớn dữ liệu và nhanh, đồng thời có thể tập hợp 
dữ liệu từ nhiều nguồn khác nhau. Thực hiện được điều này chúng ta có được đội ngũ 
qua kinh nghiệm thực tế. 
(3) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp chúng ta có được những văn bản 
được khai thác dữ liệu thông qua hồ sơ hành chính, ngoài ra dữ liệu cũng được bảo đảm 
và giữ bí mật nhờ những văn bản pháp lỦ bổ sung này. 
(4) Sử dөng dữ liệu lớn đem lại niềm tin cӫa cộng đồng với thống kê chính thức do 
tác động chӫ Ủ cӫa con người. 
3.2 Thách thͱc  
(1)Tài chính 
Nhiều đơn vị, tổ chức không đo lường được vấn đề sẽ phát sinh trong quá trình triển 
khai thực hiện, dự toán kinh phí chưa chính xác, do vậy dự án không thực hiện được. Để 
triển khai được thành công, yếu tố tài chính có Ủ nghĩa rất quan trọng, một số tập đoàn 
Big data như IBM, website bán hàng thương mại điện tử Amazon ... 
(2) Chính sách, quy định Luật pháp về truy cập và sử dụng dữ liệu 
Việc sử dөng và khai thác dữ liệu lớn phө thuộc vào luật quy định cӫa mỗi quốc gia. 
1 Xem Báo cáo “Thống kê chính thức với Big data: Kinh nghiệm quốc tế và định hướng của Thống kê Việt Nam. 
6 
  Ví dө: ở Canada người dùng có thể được tiếp cận dữ liệu từ cả hai tổ chức chính 
phӫ và phi chính phӫ, nhưng ở những nước khác như Ireland thì phải được sự cho phép 
từ các cơ quan chính phӫ. Điều này có thể dẫn đến những hạn chế để truy cập vào một số 
loại dữ liệu lớn. 
 (3) Trình độ khai thác và quản lý dữ liệu  
quản lỦ là cũng khác nhau tuy nhiên, Một vấn đề liên quan đến quản lỦ thông tin hiện nay 
là nguồn nhân lực. Khoa học dữ liệu lớn đang phát triển mạnh trong những tổ chức tư 
nhân, trong khi đó bộ phận này chưa được liên kết với những tổ chức cӫa chính phӫ một 
cách chặt chẽ dẫn đến việc quản lỦ vẫn còn nhiều vướng mắc.. 
(4) Hạ tầng Công nghệ thông tin 
sử dөng giao diện ứng dөng cӫa Chương trình chuyên sâu tiêu chuẩn (API) để truy cập 
dữ liệu. Bằng cách này, nó có thể kết nối các ứng dөng cho dữ liệu thu về và xử lỦ dữ liệu 
trực tiếp với dữ liệu hành chính. Ngoài ra hệ thống khai thác dữ liệu lớn cũng cần phải 
được tính toán để có thể kết nối vào được kho cơ sở dữ liệu truyền thống, đó cũng là một 
trong những thách thức lớn cần được giải quyết.  
data, những lợi ích mà Big data mang lại cho chúng ta. Bên cạnh đó cũng chỉ ra những 
thách thức khi triển khai áp dөng khai thác Big data. 
cung cấpthông tin để chung ta xử lỦ được tình huống nhanh nhất, chính xác nhất và giá trị 
cӫa Big data mang lại luôn có tính định hướng đến tương  lai ? giải đáp những câu hỏi tại 
sao việc ấy lại xảy ra?;  Sau chuyện đó thì điều gì sẽ sảy ra? và chúng ta nên ứng phó như 
thế nào trong hoàn cảnh đó? 
1. Tài liệu cơ hội và thách thức với bigdata –E cӫa Liên Hợp Quốc: 
http://unstats.un.org/unsd/statcom/doc14/2014-11-BigData-E.pdf 
2. Báo cáo Hội thảo về tương lai cӫa Thống kê học London: 
https://statistics.stanford.edu/statistics-and-science-london-workshop-report 
3. Tài liệu về các khái niệm và đặc trưng cӫa Big data: 
https://viblo.asia/dovv/posts/3OEqGjWwv9bL 

Removed lines from what-is-big-data-ebook-4421383.pdf:
What is Big Data? 
04 
  08 
Big Data Use Cases                                                                      10 
                                                             13 
15 
18 
4 
5 
What exactly is big data? 
To put it simply: big data is larger, more 
complex data sets, especially from new data 
sources. These data sets are so voluminous that 
traditional data processing software just can’t 
manage them. But these massive volumes of 
you wouldn’t have been able to tackle before. 
To really understand big data, it’s helpful to have 
some historical background. Here’s Gartner’s 
defnition, circa 2001(which is still the go-to 
defnition): 
“Big data is data that contains greater variety 
arriving in increasing volumes and with ever 
higher velocity. This is known as the three Vs.” 
• Volume.The amount of data matters. With 
big data, you’ll have to process high volumes 
of low-density, unstructured data. This can be 
data of unknown value, such as Twitter data 
feeds, clickstreams on a webpage or a mobile 
app, or sensor-enabled equipment. For some 
organizations, this might be tens of 
terabytes of data. For others, it may be 
hundreds of petabytes. 
Velocity. Velocity is the fast rate at which data 
is received and (perhaps) acted on. Normally, 
memory versus being written to disk.  Some 
internet-enabled smart products operate in real 
real-time evaluation and action. 
• Variety. In today’s big data world, data 
comes in new unstructured data types. 
Unstructured and semi-structured data types, 
such as text, audio, and video require addition 
support metadata. 
6 
Volume
1 
2 
3 
THE VALUE—AND TRUTH—OF 
Since 2001, two more Vs have become apparent: 
value and veracity. Data has intrinsic value. But 
it’s of no use until that value is discovered. 
Equally important: How truthful is your data—and 
how much can you rely on it? 
Today, big data has become capital. Think of 
some of the world’s biggest tech companies. A 
their data, which they’re constantly analyzing to 
new products. 
and compute, making it easier and less expensive 
to store more data than ever before. With an 
increased volume of big data now cheaper and 
more accessible, you can make more accurate 
and precise business decisions. 
Finding value in big data isn’t only about 
analyzing it (which is a whole other beneft). 
It’s an entire discovery process that requires 
insightful analysts, business users, and 
executives who ask the right questions, recognize 
patterns, make informed assumptions, and 
predict behavior. 
But how did we get here? 
7 
8 
Around 2005, people began to realize just how 
much data users generated through Facebook, 
YouTube, and other online services. Hadoop (an 
open-source framework created specifcally to 
store and analyze big data sets) was developed 
that same year. NoSQL also began to gain 
popularity during this time. 
The development of open-source frameworks, 
such as Hadoop (and more recently, Spark) was 
store. In the years since then, the volume of big 
data has skyrocketed. Users are still generating 
huge amounts of data—but it’s not just humans. 
With the advent of Internet of Things (IoT) , more 
objects and devices are connected to the internet, 
product performance. The emergence of machine 
learning has produced still more data. 
While big data has come far, its popularity is only 
just beginning. Cloud computing has expanded 
big data possibilities even further. 
The cloud offers a truly elastic scalability, where 
test around a subset of data. It’s an exciting time 
to see what’s going to happen next. 
THE VALUE OF BIG DATA COMES IS TWOFOLD: 
1. Big data makes it possible for you to gain 
2. More complete answers means more 
confdence in the data–which means 
a completely different approach to 
9 
10 
cases that you haven’t been able to fully delve 
into before. Here are just a few.  (More use cases 
are on our solutions page): 
Companies like Netfix and Procter & Gamble 
use big data to anticipate customer demand. 
products or services, and then modeling the 
commercial success of the offerings, they build 
predictive models for new products and services. 
In addition, P&G uses data and analytics from 
focus groups, social media, test markets, and 
early store rollouts to plan, produce, and launch 
new products. 
be deeply buried in structured data, such as the 
equipment year, make, and model, as well as 
entries, senor data, error messages, and engine 
temperature., By analyzing these indications of 
potential issues before the problems happen, 
equipment uptime. 
The race for customers is on. A clearer view of 
ever before. Big data enables you to gather data 
from social media, web visits, call logs, and 
experience and maximize the value delivered. 
Start delivering personalized offers, reduce 
customer churn, and handle issues proactively. 
When it comes to security, it’s not just a few 
rogue hackers; you’re up against entire expert 
teams. Security landscapes and compliance 
requirements are constantly evolving. Big 
indicate fraud and aggregate large volumes of 
much faster. 
now. And data—specifcally big data—is one of 
the reasons why. It’s only recently that we’ve 
them. And the availability of big data to train 
machine-learning models makes that happen. 
11 
news, but it’s an area in which big data is having 
the most impact. With big data, you can analyze 
and assess production, customer feedback and 
returns, and other factors to reduce outages and 
anticipate future demands. Big data can also be 
used to improve decision-making in line with 
current market demand. 
interdependencies between humans, institutions, 
entities, and process and then determining new 
ways to use those insights. Use data insights to 
considerations. Examine trends and what 
services. Implement dynamic pricing. There are 
endless possibilities. 
While big data holds a lot of promise, it is not 
without its challenges.  
First, big data is... big. Although new 
technologies have been developed to store data, 
data volumes are doubling in size around every 
two years. Organizations still struggle to keep 
store it. 
But it’s not enough to just store the data. Data 
must be used to be valuable, and that depends 
on curation. Clean data, or data that’s relevant 
meaningful analysis requires a lot of work. 
Data scientists spend 50 to 80 percent of their 
actually be used. 
Finally, big data technology is changing at a fast 
pace. A few years ago, Apache Hadoop was the 
popular technology used to handle big data. That 
is, until Apache Spark was introduced in 2014. 
Today, a combination of the two frameworks 
appears to be the best approach. Keeping up with 
big data technology is an ongoing challenge. 
12 
13 
Oracle Cloud for  Big  Data  Analytics 
Data 
Enterprise Apps 
Data 
new opportunities and business models. Getting 
started involves three key actions:   
disparate sources and applications. Traditional 
data integration mechanisms, such as ETL 
(extract, transform, and load) generally aren’t 
up to the task. It requires new strategies and 
technologies to analyze big data sets at terabyte, 
or even petabyte, scale. At the same time, big 
data has the same requirements for quality, 
governance, and confdence as traditional data 
sources. During integration, you need to bring in 
the data, process it, and make sure it’s formatted 
analysts can get started with. 
Big data requires storage. Your storage solution 
can be in the cloud, on-premises, or both. 
on an on-demand basis. Many people choose 
data is currently residing. The cloud is gradually 
gaining popularity because it supports your 
to spin up resources as needed. 
analyze and act on your data. Get new clarity with 
a visual analysis of your varied data sets. Explore 
the data further to make new discoveries. Share 
your fndings with others. Build data models with 
machine learning and artifcial intelligence. Put 
your data to work. 
To help you on your big data journey, we’ve put 
in mind. Here are our guidelines for building a 
successful big data foundation. 
14 
15 
#1: ALIGN BIG DATA WITH 
new discoveries. To that end, it is important to 
base new investments in skills, organization, 
or infrastructure with a strong business-
investments and funding. To determine if 
you are on the right track, ask how big data 
supports and enables your top business and IT 
priorities. Examples include understanding how 
behavior, deriving sentiment from social 
media and customer support interactions, and 
and their relevance for customer, product, 
manufacturing, and engineering data. 
#2: EASE SKILLS SHORTAGE 
shortage. You can mitigate this risk by ensuring 
that big data technologies, considerations, 
governance program. 
Standardizing your approach will allow you 
to manage costs and leverage resources. 
proactively identify any potential skill gaps. These 
can be addressed by training/cross-training 
existing resources, hiring new resources, and 
leveraging consulting frms. 
#3: OPTIMIZE KNOWLEDGE 
Use a Center of Excellence approach to share 
knowledge, control oversight, and manage 
project communications. Whether big data is a 
new or an expanding investment, the soft and 
hard costs can be shared across the enterprise. 
Leveraging this approach can help increase 
systematic way. 
#4:TOP PAYOFF IS ALIGNING 
own.But you can bring even greater business 
already using today. 
16 
Whether you are capturing customer, product, 
equipment, or environmental big data, the goal 
core master and analytical summaries, leading 
to better conclusions. For example, there is 
sentiment from that of only your best customers. 
capabilities, data warehousing platform, and 
information architecture. 
processes and models can be both human- and 
machine-based. Big data analytical capabilities 
include statistics, spatial analysis, semantics, 
interactive discovery, and visualization. Using 
analytical models, you can correlate different 
and meaningful discoveries. 
#5: PLAN YOUR DISCOVERY LAB 
straightforward. Sometimes we don’t even 
know what we’re looking for. That’s expected. 
Management and IT needs to support this “lack 
of direction” or “lack of clear requirement.” 
At the same time, it’s important for analysts and 
requirements. To accommodate the interactive 
statistical algorithms, you need high-performance 
work areas. Be sure that sandbox environments 
have the power they need—and are 
properly governed. 
#6: ALIGN WITH THE CLOUD 
jobs. A big data solution includes all data 
realms including transactions, master data, 
reference data, and summarized data. Analytical 
sandboxes should be created on demand. 
control of the entire data fow including pre- 
and post-processing, integration, in-database 
summarization, and analytical modeling. A well-
supporting these changing requirements. 
17 
18 
Clearly, big data has tremendous potential. 
customers, make more accurate decisions, and 
create new growth opportunities. Contact us to 
learn more. 
See how Oracle can help your big data journey. 
Start your free trial today. 
Contact us URL: 
https://www.oracle.com/marketingcloud/contact­
sales.html 
Free Trial URL: 
https://go.oracle.com/LP=50758/? 
19 
Oracle Corporation 
Copyright © 2019, Oracle and/or its affiliates. All rights reserved. This document is provided for information purposes only, and the contents hereof are subject 
to change without notice. This document is not warranted to be error-free, nor subject to any other warranties or conditions, whether expressed orally or 
implied in law, including implied warranties and conditions of merchantability or fitness for a particular purpose. We specifically disclaim any liability with 
500 Oracle Parkway 
respect to this document, and no contractual obligations are formed either directly or indirectly by this document. This document may not be reproduced or 
transmitted in any form or by any means, electronic or mechanical, for any purpose, without our prior written permission. 
CA 94065 
Oracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners. 
USA 
Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or 
registered trademarks of SPARC International, Inc. AMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks or registered trademarks of 
Advanced Micro Devices. UNIX is a registered trademark of The Open Group. 
Phone: +1.650.506.7000 
+1.800.ORACLE1 
Fax: 
+1.650.506.7200 
oracle.com 

Removed lines from 2_iis_2015_81-90.pdf:
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
81 
BIG DATA ANALYTICS 
Jasmine Zakir, Minot State University,	  jasminezakir@outlook.com 
Tom Seymour, Minot State University,	  tom.seymour@minotstateu.edu 
Kristi Berg, Minot State University,	  kristi.berg@minostatateu.edu 
ABSTRACT 
Today Big Data draws a lot of attention in the IT world. The rapid rise of the Internet and the digital economy has 
fuelled an exponential growth in demand for data storage and analytics, and IT department are facing tremendous 
challenge in protecting and analyzing these increased volumes of information. The reason organizations are 
collecting and storing more data than ever before is because their business depends on it. The type of information 
being created is no more traditional database-driven data referred to as structured data rather it is data that 
include documents, images, audio, video, and social media contents known as unstructured data or Big Data. Big 
Data Analytics is a way of extracting value from these huge volumes of information, and it drives new market 
opportunities and maximizes customer retention. This paper primarily focuses on discussing the various 
technologies that work together as a Big Data Analytics system that can help predict future volumes, gain insights, 
take proactive actions, and give way to better strategic decision-making. Further this paper analyzes the adoption, 
usage and impact of big data analytics to the business value of an enterprise to improve its competitive advantage 
using a set of data algorithms for large data sets such as Hadoop and MapReduce.  
Keywords: Big Data, Analytics, Hadoop, MapReduce 
Big Data is an important concept, which is applied to data, which does not conform to the normal structure of the 
traditional database. Big Data consists of different types of key technologies like Hadoop, HDFS, NoSQL, 
MapReduce, MongoDB, Cassandra, PIG, HIVE, and HBASE that work together to achieve the end goal like 
extracting value from data that would be previously considered dead. According to a recent market report published 
by Transparency Market Research, the total value of big data was estimated at $6.3 billion as of 2012, but by 2018, 
it’s expected to reach the staggering level of $48.3 billion that’s almost a 700 percent increase [29]. Forrester 
Research estimates that organizations effectively utilize less than 5 percent of their available data. This is because 
the rest is simply too expensive to deal with. Big Data is derived from multiple sources. It involves not just 
traditional relational data, but all paradigms of unstructured data sources that are growing at a significant rate. For 
instance, machine-derived data multiplies quickly and contains rich, diverse content that needs to be discovered. 
Another example, human-derived data from social media is more textual, but the valuable insights are often 
overloaded with many possible meanings.  
Big Data Analytics reflect the challenges of data that are too vast, too unstructured, and too fast moving to be 
managed by traditional methods. From businesses and research institutions to governments, organizations now 
routinely generate data of unprecedented scope and complexity. Gleaning meaningful information and competitive 
advantages from massive amounts of data has become increasingly important to organizations globally. Trying to 
efficiently extract the meaningful insights from such data sources quickly and easily is challenging. Thus, analytics 
increase their market share. The tools available to handle the volume, velocity, and variety of big data have 
improved greatly in recent years. In general, these technologies are not prohibitively expensive, and much of the 
software is open source. Hadoop, the most commonly used framework, combines commodity hardware with open-
source software. It takes incoming streams of data and distributes them onto cheap disks; it also provides tools for 
analyzing the data. However, these technologies do require a skill set that is new to most IT departments, which will 
need to work hard to integrate all the relevant internal and external sources of data. Although attention to technology 
isn’t sufficient, it is always a necessary component of a big data strategy. This paper discusses some of the most 
commonly used big data technologies mostly open source that work together as a big data analytics system for 
leveraging large quantities of unstructured data to make more informed decisions.  
https://doi.org/10.48009/2_iis_2015_81-90
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
82 
Big Data is a data analysis methodology enabled by recent advances in technologies that support high-velocity data 
capture, storage and analysis. Data sources extend beyond the traditional corporate database to include emails, 
mobile device outputs, and sensor-generated data where data is no longer restricted to structured database records 
but rather unstructured data having no standard formatting [30]. Since Big Data and Analytics is a relatively new 
and evolving phrase, there is no uniform definition; various stakeholders have provided diverse and sometimes 
contradictory definitions. One of the first widely quoted definitions of Big Data resulted from the Gartner report of 
2001. Gartner proposed that, Big Data is defined by three V’s volume, velocity, and variety. Gartner expanded its 
definition in 2012 to include veracity, representing requirements about trust and uncertainty pertaining to data and 
the outcome of data analysis. In a 2012 report, IDC defined the 4th V as value—highlighting that Big Data 
applications need to bring incremental value to businesses. Big Data Analytics is all about processing unstructured 
information from call logs, mobile-banking transactions, online user generated content such as blog posts and 
tweets, online searches, and images which can be transformed into valuable business information using 
computational techniques to unveil trends and patterns between datasets. 
Another dimension of the Big Data definition involves technology. Big Data is not only large and complex, but it 
requires innovative technology to analyze and process. In 2013, the National Institute of Standard and Technology 
(NIST) Big Data workgroup proposed the following definition of Big Data that emphasizes application of new 
technology; Big Data exceed the capacity or capability of current or conventional methods and systems, and enable 
novel approaches to frontier questions previously inaccessible or impractical using current or conventional methods. 
Business challenges rarely show up in the appearance of a perfect data problem, and even when data are abundant, 
practitioners have difficulties to incorporate it into their complex decision-making that adds business value. In 2012, 
McKinsey & Company conducted a survey of 1,469 executives across various regions, industries and company 
sizes, in which 49 percent of respondents said that their companies are focusing big data efforts on customer 
insights, segmentation and targeting to improve overall performance [10] An even higher number of respondents 60 
percent said their companies should focus efforts on using data and analytics to generate these insights. Yet, just 
one-fifth said that their organizations have fully deployed data and analytics to generate insights in one business unit 
or function, and only 13 percent use data to generate insights across the company. As these survey results show, the 
question is no longer whether big data can help business, but how can business derive maximum results from big 
data. 
Predictive Analytics is the use of historical data to forecast on consumer behavior and trends [18]. It is the use of 
past/historical data to predict future trends. This analysis makes use of the statistical models and machine learning 
algorithms to identify patterns and learn from historical data [25]. Predictive Analysis can also be defined as a 
process that uses machine learning to analyze data and make predictions [22].  
future, and 68% sight competitive advantage as the prime benefit of predictive analysis [17]. Broadly speaking, 
predictive analysis can be applied in ecommerce for product recommendation, price management, and predictive 
search. Typically a large e-commerce site offers thousands of product and services for sale. Navigating and 
searching for a product out of thousands on a website could be a major setback to consumers. However, with the 
invention of recommender system, an E-Commerce site/application can quickly identify/predict products that 
closely suit the consumer’s taste [24].  
Using a technology called Collaborative Filtering a database of historical user preferences is created. When a new 
customer access the ecommerce site, the customer is matched with the database of preferences, in order to discover a 
preference class that closely matches the customer taste. These products are then recommended to the customer [24]. 
Another technology that is used in ecommerce is the clustering algorithm. Clustering algorithm works by identifying 
groups of users that have similar preferences. These users are then clustered into a single group and are given a 
unique identifier.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
83 
New customers cluster are predicted by calculating the average similarities of the individual members in that cluster. 
Hence a user could be a partial member of more than one cluster depending of the weight of the user’s average 
opinion [24]. Advanced analytics is defined as the scientific process of transforming data into insight for making 
better decisions. As a formal discipline, advanced analytics have grown under the Operational Research domain. 
There are some fields that have considerable overlap with analytics, and also different accepted classifications for 
the types of analytics [2].  
Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large 
amounts of log data from many different sources to a centralized data store. Flume deploys as one or more agents, 
each contained within its own instance of the Java Virtual Machine (JVM). Agents consist of three pluggable 
components: sources, sinks, and channels. Flume agents ingest incoming streaming data from one or more sources. 
Data ingested by a Flume agent is passed to a sink, which is most commonly a distributed file system like Hadoop. 
agent to be the sink of another. Flume sources listen and consume events. Events can range from newline-terminated 
strings in stdout to HTTP POSTs and RPC calls — it all depends on what sources the agent is configured to use. 
Flume agents may have more than one source, but at the minimum they require one. Sources require a name and a 
type; the type then dictates additional configuration parameters. 
Channels are the mechanism by which Flume agents transfer events from their sources to their sinks. Events written 
to the channel by a source are not removed from the channel until a sink removes that event in a transaction. This 
allows Flume sinks to retry writes in the event of a failure in the external repository (such as HDFS or an outgoing 
network connection). For example, if the network between a Flume agent and a Hadoop cluster goes down, the 
channel. Sink is an interface implementation that can remove events from a channel and transmit them to the next 
agent in the flow, or to the event’s final destination and also sinks can remove events from the channel in 
transactions and write them to output. Transactions close when the event is successfully written, ensuring that all 
events are committed to their final destination.  
Apache Sqoop is a CLI tool designed to transfer data between Hadoop and relational databases. Sqoop can import 
been transformed using MapReduce. Sqoop also has the ability to import data into HBase and Hive. Sqoop connects 
imported. Both import and export utilize MapReduce, which provides parallel operation as well as fault tolerance. 
During import, Sqoop reads the table, row by row, into HDFS. Because import is performed in parallel, the output in 
HDFS is multiple files.  
Apache’s Pig is a major project, which is lying on top of Hadoop, and provides higher-level language to use 
Hadoop’s MapReduce library. Pig provides the scripting language to describe operations like the reading, filtering 
and transforming, joining, and writing data which are exactly the same operations that MapReduce was originally 
designed for. Instead of expressing these operations in thousands of lines of Java code which uses MapReduce 
directly, Apache Pig lets the users express them in a language that is not unlike a bash or Perl script.  
Pig was initially developed at Yahoo Research around 2006 but moved into the Apache Software Foundation in 
2007. Unlike SQL, Pig does not require that the data must have a schema, so it is well suited to process the 
unstructured data. But, Pig can still leverage the value of a schema if you want to supply one. PigLatin is relationally 
complete like SQL, which means it is at least as powerful as a relational algebra. Turing completeness requires 
conditional constructs, an infinite memory model, and looping constructs.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
84 
SQL for querying. Being a SQL dialect, HIVEQL is a declarative language. In PigLatin, you specify the data flow, 
but in Hive we describe the result we want and hive figures out how to build a data flow to achieve that result. 
Unlike Pig, in Hive a schema is required, but you are not limited to only one schema. Like PigLatin and SQL, 
HiveQL itself is a relationally complete language but it is not a Turing complete language.  
Apache Zoo Keeper is an effort to develop and maintain an open-source server, which enables highly reliable 
distributed coordination. It provides a distributed configuration service, a synchronization service and a naming 
registry for distributed systems. Distributed applications use ZooKeeper to store and mediate updates to import 
configuration information. ZooKeeper is especially fast with workloads where reads to the data are more common 
than writes. The ideal read/write ratio is about 10:1. ZooKeeper is replicated over a set of hosts (called an ensemble) 
and the servers are aware of each other and there is no single point of failure.   
Figure 1. Intel Manager for Hadoop [3] 
MongoDB is an open source, document-oriented NoSQL database that has lately attained some space in the data 
industry. It is considered as one of the most popular NoSQL databases, competing today and favors master-slave 
replication. The role of master is to perform reads and writes whereas the slave confines to copy the data received 
from master, to perform the read operation, and backup the data. The slaves do not participate in write operations 
but may select an alternate master in case of the current master failure. MongoDB uses binary format of JSON-like 
documents underneath and believes in dynamic schemas, unlike the traditional relational databases. The query 
system of MongoDB can return particular fields and query set compass search by fields, range queries, regular 
expression search, etc. and may include the user-defined complex JavaScript functions. As hinted already, 
MongoDB practice flexible schema and the document structure in a grouping, called Collection, may vary and 
common fields of various documents in a collection can have disparate types of the data. 
The MongoDB is equipped with the suitable drivers for most of the programming languages, which are used to 
develop the customized systems that use MongoDB as their backend player. There is an increasingly demand of 
using MongoDB as pure in-memory database; in such cases, the application dataset will always be small. Though, it 
is probably are easy for maintenance and can make a database developer happier; this can be a bottle neck for 
complex applications that require tremendous database management capabilities. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
85 
In order to efficiently address the challenges of Big Data, the leading vendor developed the Oracle NoSQL database. 
It was built by Oracle Berkeley DB team and the Berkeley DB Java Edition is the building block of Oracle NoSQL. 
Berkeley DB is a robust and scalable key-value store and used as the underlying storage for several popular data 
model such as Amazon Dynamo, GenieDB, MemcacheDB and Voldemort [28].  
scalability, throughput, and reliability with little tuning efforts. It is an efficient and a resilient transaction model that 
significantly eases the development process of applications, involving Big Data. It is a distributed, scalable yet 
simple key-value pair data model that fully supports the ACID transactions and JSON format and integrated with 
Oracle Database and Hadoop. It offers scalable throughput with bounded latency. The model very well 
accommodates the horizontal scaling with dynamic annexation of new capacity, citing high availability; the design 
architecture of Oracle NoSQL does not support single point of failure, and lucid load balancing. Actually, the goals 
of high availability, rapid failover in the event of a node failure, etc. are achieved by replicating the storage nodes. 
which is able to handle big data requirements. It is a highly scalable and high-performance distributed database 
management system that can handle real-time big data applications that drive key systems for modern and successful 
businesses. It has a built-for-scale architecture that can handle petabytes of information and thousands of concurrent 
users/operations per second as easily as it can manage much smaller amount of data and user traffic. It has a peer to 
peer design that offers no single point of failure for any database process or function, in addition to the location 
independence capabilities that equate to a true network-independent method of storing and accessing data, data can 
be read and written anywhere. Apache Cassandra is also equipped with flexible/dynamic schema design that 
accommodates all formats of big data applications, including structured, semi-structured, and unstructured data. 
online.  
clusters of computers. It is designed to scale up from single servers to thousands of machines, with each offering 
local computation and storage. The basic notion is to allow a single query to find and collect results from all the 
cluster members, and this model is clearly suitable for Google's model of search support. One of the largest 
technological challenges in software systems research today is to provide mechanisms for storage, manipulation, and 
information retrieval on large amount of data. Web services and social media produce together an impressive 
amount of data, reaching the scale of petabytes daily (Facebook, 2012). These data may contain valuable 
information, which sometimes is not properly explored by existing systems. Most of this data is stored in a non-
structured manner, using different languages and format, which, in many cases, are in compatible. 
large datasets. Over the last years, commodity hardware became part of clusters, since the x86 platform cope with 
the need of having an overall better cost/performance ratio, while decreasing maintenance cost. Apache Hadoop is a 
framework developed to take advantage of this approach, using such commodity clusters for storage, processing and 
manipulation of large amount of data. The framework was designed over the MapReduce paradigm and uses the 
HDFS as a storage file system. Hadoop presents key characteristics when performing parallel and distributed 
computing, such as data integrity, availability, scalability, exception handling, and failure recovery.   
Hadoop is a popular choice when you need to filter, sort, or pre-process large amounts of new data in place and 
distill it to generate denser data that theoretically contains more information. Pre-processing involves filtering new 
data sources to make them suitable for additional analysis in a data warehouse.  Hadoop is a top-level open source 
project of the Apache Software Foundation. Several suppliers, including Intel, offer their own commercial Hadoop 
distributions, packaging the basic software stack with other Hadoop software projects such as Apache Hive, Apache 
Pig, and Apache Sqoop. These distributions must integrate with data warehouses, databases, and other data 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
86 
process or query.	  
Figure 2. Data Architecture with Hadoop Integrated with existing data system [12] 
MapReduce is the original massively scalable, parallel processing framework commonly used with Hadoop and 
other components such as the Hadoop Distributed File System (HDFS) and YARN. YARN can be described as a 
large-scale, distributed operating system for big data implementations. As Hadoop has matured, the batch-oriented, 
disk-intensive MapReduce’ s limitations have become more apparent as Big Data analytics moves to more real-time, 
streaming processing and advanced implementations such as the aforementioned machine learning.  
MapReduce is the model of distributed data processing introduced by Google in 2004. The fundamental concept of 
MapReduce is to divide problems into two parts: a map function that processes source data into sufficient statistics 
and a reduce function that merges all sufficient statistics into a final answer. By definition, any number of 
concurrent map functions can be run at the same time without intercommunication. Once all the data has had the 
map function applied to it, the reduce function can be run to combine the results of the map phases.  For large scale 
batch processing and high speed data retrieval, common in Web search scenarios, MapReduce provides the fastest, 
most cost-effective and most scalable mechanism for returning results. Today, most of the leading technologies for 
managing "big data" are developed on MapReduce. With MapReduce there are few scalability limitations, but 
leveraging it directly does require writing and maintaining a lot of code. 
Splunk is a general-purpose search, analysis and reporting engine for time-series text data, typically machine data. 
Splunk software is deployed to address one or more core IT functions: application management, security, 
compliance, IT operations management and providing analytics for the business. The Splunk engine is optimized for 
quickly indexing and persisting unstructured data loaded into the system. Specifically, Splunk uses a minimal 
schema for persisted data – events consist only of the raw event text, implied timestamp, source (typically the 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
87 
filename for file based inputs), source type (an indication of the general type of data) and host (where the data 
originated).  
Once data enters the Splunk system, it quickly proceeds through processing, is persisted in its raw form and is 
indexed by the above fields along with all the keywords in the raw event text. Indexing is an essential element of the 
canonical “super-grep” use case for Splunk, but it also makes most retrieval tasks faster. Any more sophisticated 
processing on these raw events is deferred until search time. This serves four important goals: indexing speed is 
increased as minimal processing is performed, bringing new data into the system is a relatively low effort exercise as 
no schema planning is needed, the original data is persisted for easy inspection and the system is resilient to change 
as data parsing problems do not require reloading or re-indexing the data. 
Apache Spark an open source big data processing framework built around speed, ease of use, and sophisticated 
analytics. It was originally developed in 2009 in UC Berkeley’s AMP Lab, and open sourced in 2010 as an Apache 
project. Hadoop as a big data processing technology has been around for ten years and has proven to be the solution 
of choice for processing large data sets. MapReduce is a great solution for one-pass computations, but not very 
efficient for use cases that require multi-pass computations and algorithms. Each step in the data processing 
workflow has one Map phase and one Reduce phase and you'll need to convert any use case into MapReduce pattern 
to leverage this solution. Spark takes MapReduce to the next level with less expensive shuffles in the data 
processing. With capabilities like in-memory data storage and near real-time processing, the performance can be 
several times faster than other big data technologies.  
Spark also supports lazy evaluation of big data queries, which helps with optimization of the steps in data processing 
workflows. It provides a higher-level API to improve developer productivity and a consistent architect model for big 
data solutions. Spark holds intermediate results in memory rather than writing them to disk, which is very useful 
especially when you need to work on the same dataset multiple times. It’s designed to be an execution engine that 
works both in-memory and on-disk. Spark operators perform external operations when data does not fit in memory. 
Spark can be used for processing datasets that larger than the aggregate memory in a cluster. Spark will attempt to 
store as much as data in memory and then will spill to disk. It can store part of a data set in memory and the 
remaining data on the disk. You have to look at your data and use cases to assess the memory requirements. With 
this in-memory data storage, Spark comes with a great performance advantage. 
Spark is written in Scala Programing Language and runs on the Java Virtual machine. It currently supports 
programming languages like Scala, java, python, Clojure and R. Other than Spark Core API, there are additional 
libraries that are part of the Spark ecosystem and provide additional capabilities in Big Data analytics. Spark 
Streaming is one among the spark library that can be used for processing the real-time streaming data. This is based 
on micro based on micro batch style of computing and processing. Spark SQL provides the capabilities to expose the 
visualization tools. MLlib, GraphX are some other libraries from spark. 
Thomas H. Davenport was perhaps the first to observe in his Harvard Business Review article published in January 
2006 (“Competing on Analytics”) how companies who orientated themselves around fact based management 
approach and compete on their analytical abilities considerably out-performed their peers in the marketplace. The 
reality is that it takes continuous improvement to become an analytics-driven organization. In a presentation given at 
the Strata New York conference in September 2011, McKinsey & Company showed the eye opening; 10-year 
category growth rate differences (see Figure 7, below) between businesses that smartly use their big data and those 
that do not.  
Amazon uses Big Data to monitor, track and secure 1.5 billion items in its inventory that are laying around 200 
fulfillment centers around the world, and then relies on predictive analytics for its ‘anticipatory shipping’ to predict 
when a customer will purchase a product, and pre-ship it to a depot close to the final destination. Wal-Mart handles 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
88 
more than a million customer transactions each hour [23], imports information into databases to contain more than 
2.5 petabytes and asked their suppliers to tag shipments with radio frequency identification (RFID) systems [8] that 
can generate 100 to 1000 times the data of conventional bar code systems. UPS deployment of telematics in their 
freight segment helped in their global redesign of logistical networks [6]. Amazon is a big data giant and the largest 
online retail store. The company pioneered e-commerce in many different ways, but one of its biggest successes was 
the personalized recommendation system, which was built from the big data it gathers from its millions of 
customers’ transactions. 
The U.S. federal government collects more than 370,000 raw and geospatial datasets from 172 agencies and sub 
agencies.  It leverages that data to provide a portal to 230 citizen-developed apps, with the aim of increasing public 
access to information not deemed private or classified. Professional social network LinkedIn uses data from its more 
than 100 million users to build new social products based on users’ own definitions of their skill sets. Silver Spring 
Networks deploys smart, two-way power grids for its utility customers that utilize digital technology to deliver more 
help manage energy use and maximize efficiency. Jeffrey Brenner and the Camden Coalition mapped a city’s crime 
trends to identify problems with its healthcare system, revealing services that were both medically ineffective and 
expensive. 
Today’s technology landscape is changing fast. Organizations of all shapes and sizes are being pressured to be data-
driven and to do more with less. Even though big data technologies are still in a nascent stage, relatively speaking, 
the impact of the 3V’s of big data, which now is 5v’s cannot be ignored. The time is now for organizations to begin 
planning for and building out their Hadoop-based data lake. Organizations with the right infrastructures, talent and 
vision in place are well equipped to take their big data strategies to the next level and transform their businesses. 
They can use big data to unveil new patterns and trends, gain additional insights and begin to find answers to 
pressing business issues. The deeper organizations dig into big data and the more equipped they are to act upon 
what’s learned, the more likely they are to reveal answers that can add value to the top line of the business. This is 
where the returns on big data investments multiply and the transformation begins. Harnessing big data insight 
delivers more than cost cutting or productivity improvement but it definitely reveals new business opportunities. 
Data-driven decisions always tend to be better decisions. 
1. Apache Software Foundation. (2010). Apache ZooKeeper. Retrieved April 5, 2015 from 
https://zookeeper.apache.org 
2. Chae, B., Sheu, C., Yang, C. and Olson, D. (2014). The impact of advanced analytics and data accuracy on 
operational performance: A contingent resource based theory (RBT) perspective, Decision Support Systems, 59, 
119-126. 
3. Chambers, C., Raniwala, A., Adams, S., Henry, R., Bradshaw, R., and Weizenbaum, N. (2010). Flume Java: 
Easy, Efficient Data-Parallel Pipelines. Google, Inc. Retrieved April 1, 2015 from 
http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf 
4. Cisco Systems. Cisco UCS Common Platform Architecture Version 2 (CPA v2) for Big Data with 
Comprehensive Data Protection using Intel Distribution for Apache Hadoop. Retrieved March 15, 2015, from 
http://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/Cisco_UCS_CPA_for_Big_Data_wi
th_Intel.html 
5. DATASTAX Corporation. (2013, October). Big Data: Beyond the Hype - Why Big data Matters to you [White 
paper]. Retrieved March 15, 2015 from https://www.datastax.com/wp-content/uploads/2011/10/WP-DataStax-
BigData.pdf 
6. Davenport, T & Patil, D. (2012). Data Scientist: The Sexiest Job of the 21st Century. Harvard Business Review, 
90, 70-76. 
7. Dhawan, S & Rathee, S. (2013). Big Data Analytics using Hadoop Components like Pig and Hive. American 
International Journal of Research in Science, Technology, Engineering & Mathematics, 88, 13-131. Retrieved 
from http://iasir.net/AIJRSTEMpapers/AIJRSTEM13-131.pdf 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
89 
8. Edwards, P., Peters, M. and Sharman, G. (2001). The Effectiveness of Information Systems in Supporting the 
Extended Supply Chain, Journal of Business Logistics 22 (1), 1-27 
9. EMC Corporation. (2013, January). EMC Accelerates Journey to Big Data with Business Analytics-as-a-
Service [White paper]. Retrieved from http://www.emc.com/collateral/white-papers/h11259-emc-accelerates-
journey-big-data-ba-wp.pdf 
10. EMC Corporation. Big Data, Big Transformations [White paper]. Retrieved from 
http://www.emc.com/collateral/white-papers/idg-bigdata-umbrella-wp.pdf 
11. EMC Solutions Group. (2012, July). Big Data-as-a-Service [White paper]. Retrieved from 
https://www.emc.com/collateral/software/white-papers/h10839-big-data-as-a-service-perspt.pdf 
12. Enterprise Hadoop: The Ecosystem of Projects. Retrieved from http://hortonworks.com/hadoop/ 
13. George, L. (2014, September). Getting Started with Big Data Architecture. Retrieved April 5, 2015, from 
http://blog.cloudera.com/blog/2014/09/getting-started-with-big-data-architecture/ 
14. IBM Corporation. IBM Big Data Platform. Retrieved from http://www-
01.ibm.com/software/in/data/bigdata/enterprise.html 
15. Intel Corporation. Big Data Analytics - Extract, Transform, and Load Big data with Apache Hadoop [White 
paper]. Retrieved April 3, 2015 from https://software.intel.com/sites/default/files/article/402274/etl-big-data-
with-hadoop.pdf 
16. McClary, D. (2013, June). Acquiring Big Data Using Apache Flume. Retrieved March 3, 2015 from 
http://www.drdobbs.com/database/acquiring-big-data-using-apache-flume/240155029 
17. Millard, S. (2013). Big Data Brewing Value in Human Capital Management – Ventana Research. Retrieved 
April 2, 2015 from http://stephanmillard.ventanaresearch.com/2013/08/28/big-data-brewing-value-in-human-
capital-management 
18. Mosavi, A. and Vaezipour, A. (2013). Developing Effective Tools for Predictive Analytics and Informed 
Decisions. Technical Report. University of Tallinn.  
19. Oracle Corporation. (2013, March). Big Data Analytics - Advanced Analytics in Oracle Database [White 
paper]. Retrieved March 5, 2015 from http://www.oracle.com/technetwork/database/options/advanced-
analytics/advanced-analytics-wp-12c-1896138.pdf?ssSourceSiteId=ocomen 
20. Oracle Enterprise Architecture. (2015, April). An Enterprise Architect's Guide to Big Data - Reference 
Architecture Overview [White paper]. Retrieved from 
http://www.oracle.com/technetwork/topics/entarch/articles/oea-big-data-guide-1522052.pdf 
21. Penchikala, S. (2015, January). Big Data Processing with Apache Spark - Part 1: Introduction. Retrieved from 
http://www.infoq.com/articles/apache-spark-introduction 
22. Puri, R. (2013). How Online Retailers Use Predictive Analytics To Improve Your Shopping Experience. 
Retrieved April5, 2015 from http://blogs.sap.com/innovation/analytics/how-online-retailers-use-predictive-
analytics-to-improve-your-shopping-experience-0108060 
23. Sanders, N.R. (2014). Big Data Driven Supply Chain Management: A Framework for Implementing Analytics 
and Tuning Information into Intelligence, 1st Edition, Pearson, NJ 
24. Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. (2002). Recommendation systems for large e-commerce: 
Scalable neighborhood formation using clustering. In Proceedings of the fifth international conference on 
computer and information technology, 1.  
25. Shmueli, G. & Koppius, O. (2011). Predictive Analytics in Information Systems Research. MIS Quarterly, 
35(3), pp. 553-72. 
26. Sorkin, S. (2011). Splunk Technical Paper: Large-Scale, Unstructured Data Retrieval and Analysis Using 
Splunk. Retrieved April 15, 2015 from https://www.splunk.com/content/dam/splunk2/pdfs/technical-
briefs/splunk-and-mapreduce.pdf 
27. The Bloor Group. IBM and the Big Data Information Architecture. Retrieved April 3, 2015 from 
http://insideanalysis.com/wp-content/uploads/2014/08/BDIAVendor-IBMv01.pdf 
28. Tiwari, S. (2011). Using Oracle Berkeley DB as a NoSQL Data Store. Retrieved April 5 2015 from 
http://www.oracle.com/technetwork/articles/cloudcomp/berkeleydb-nosql-323570.htm 
29. Transparency Market Report. (May, 2015).Big Data Applications in Healthcare likely to Propel Market to 
US$48.3 Bn by 2018. Retrieved June 26, 2015, from 
http://www.transparencymarketresearch.com/pressrelease/big-data-market.htm 
30. Villars, R. L., Olofson, C. W., & Eastwood, M. (2011, June). Big data: What it is and why you should care. IDC 
White Paper. Framingham, MA: IDC. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
90 
31. Wolpe, T. (2015, March). How Facebook is speeding up the Pesto SQL query engine. Retrieved April 3, 2015, 
from http://www.zdnet.com/article/how-facebook-is-speeding-up-the-presto-sql-query-engine 
32. Zahari et al. (2010). Spark: Cluster Computing with Working Sets. Retrieved April 7, 2015, from 
http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf 

Removed lines from 48077-157-151840-1-10-20200520.pdf:
THÔNG TIN VÀ TƯ LIỆU - 2/2020 23
BIG DATA VÀ XU HƯỚNG ỨNG DỤNG TRONG HOẠT ĐỘNG THÔNG TIN - THƯ VIỆN
ThS Nguyễn Lê Phương Hoài
Viện Thông tin Khoa học xã hội 
● Tóm tắt: Big Data là một thuật ngữ được sử dụng để chỉ những bộ dữ liệu khổng lồ, chủ yếu không 
có cấu trúc, được thu thập từ nhiều nguồn khác nhau. Big Data có nhiều tác động, ứng dụng và được 
xem như một yếu tố quyết định đến việc phát triển, mang lại lợi thế cạnh tranh cho tổ chức. Bài viết 
tổng quan lược sử các quan điểm về Big Data, đồng thời nhấn mạnh vào các xu hướng ứng dụng 
trong hoạt động thông tin - thư viện.
● Từ khóa: Big Data; dữ liệu lớn; hoạt động thư viện.
BIG DATA APPLICATION IN LIBRARY AND INFORMATION ACTIVITIES
● Abstract: Big Data is a term used to refer to huge, mostly unstructured datasets, collected from 
a variety of sources. Big Data has many impacts, applications and is considered as a decisive 
factor in the development, bringing competitive advantages to the organization. The overview paper 
summarizes the views on Big Data and emphasizes application trends in library and information 
activities.
● Keywords: Big Data; library activities.
1. LƯỢC SỬ CÁC QUAN ĐIỂM VỀ BIG DATA
Hiện nay, chưa có một định nghĩa chính 
xác cho thuật ngữ Big Data. Big được ghi 
nhận lần đầu tiên trong báo cáo “Application-
controlled demand paging for out-of-core 
visualization” của Michael Cox và David 
thứ 8 (vào tháng 10 năm 1997). Ý tưởng đưa 
xử lý không còn đủ số lượng cần khảo sát, do 
thể phân tích được tất cả các thông tin [11].
Tháng 8 năm 1999, Steve Bryson, David 
Kenwright, Michael Cox, David Ellsworth, và 
Robert Haimes đăng bài “Visually exploring 
gigabyte data sets in real time” trên Tạp chí 
Communications of the ACM. Đây là bài viết 
đầu tiên sử dụng thuật ngữ “Big Data”. Các 
tác giả nhận định: “Những chiếc máy tính 
vực, cũng có thể là bất lợi; tính toán nhanh 
chóng tạo ra một lượng lớn dữ liệu. Nếu trước 
là lớn, thì bây giờ chúng ta có thể tìm thấy 
300 GB” [15]. 
Tháng 11 năm 2000, Francis X. Diebold 
Hiệp hội Kinh tế lượng bài viết “Big Data 
Measurement and Forecasting”. Trong bài 
viết này, tác giả khẳng định: “Gần đây, nhiều 
ngành khoa học như vật lý, sinh học, khoa 
học xã hội, vốn đang buộc phải đương đầu với 
khó khăn - đã thu được lợi từ hiện tượng Big 
Data và đã gặt hái được nhiều thành công. Big 
Data chỉ sự bùng nổ về số lượng (và đôi khi, 
chất lượng), khả năng liên kết cũng như độ 
sẵn sàng của dữ liệu, chủ yếu là kết quả của 
việc ghi lại dữ liệu và công nghệ lưu trữ” [4].
Tháng 2 năm 2001, Doug Laney - nhà 
phân tích của Tập đoàn Meta, công bố nghiên 
cứu “3D Data Managment: Controlling Data 
Volume, Velocity, and Variety”. Laney cho 
rằng, những thách thức và cơ hội nằm trong 
bằng mô hình “3Vs”: tăng về số lượng lưu trữ 
(Volume), tăng về tốc độ xử lý (Velocity) và 
tăng về chủng loại (Variety) [3]. Một thập kỷ 
sau, mô hình “3Vs” đã trở thành thuật ngữ 
dữ liệu lớn ba chiều. Nhiều công ty và tổ chức 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
24 THÔNG TIN VÀ TƯ LIỆU - 1/2020
24
dụng mô hình “3Vs” này để định nghĩa Big 
Data.
Tháng 12 năm 2008, Randal E. Bryant, 
Randy H. Katz, và Edward D. Lazowska 
công bố bài viết “Big-Data Computing: 
Commerce, Science and Society”, trong đó 
miêu tả: “Cũng như công cụ tìm kiếm đã làm 
thay đổi cách chúng ta tiếp cận thông tin, các 
ty, các nhà nghiên cứu khoa học, các học 
viên y tế, quốc phòng và tình báo,... Sử dụng 
nghệ máy tính suốt một thập kỷ qua. Chúng 
nó trong việc thu thập, sắp xếp và xử lý dữ 
liệu của tất cả các tầng lớp xã hội. Một khoản 
sẽ thúc đẩy phát triển và mở rộng nó” [13].
Tháng 2 năm 2010, Kenneth Cukier đăng 
“Data, data everywhere”. Cukier viết: “... thế 
mức không tưởng, và càng ngày càng được 
nhân rộng với tốc độ nhanh hơn bao giờ hết... 
Hiệu quả đã được thể hiện ở khắp mọi nơi, từ 
kinh doanh đến khoa học, từ chính phủ đến 
nghệ thuật. Các nhà khoa học và kỹ sư máy 
tượng này: Big Data” [8].
Tháng 5 năm 2012, Danah Boyd và Kate 
bài “Critical Question for Big Data” trên tờ 
Information, Communications and Society. 
Các tác giả định nghĩa Big Data như là “một 
hiện tượng văn hóa, công nghệ và học thuật 
dựa trên sự tương tác của: 1) Công nghệ tối 
thuật toán để thu thập, phân tích, liên kết, và 
so sánh các tập dữ liệu lớn; 2) Phân tích: tạo 
tuyên bố kinh tế, xã hội, kỹ thuật và pháp lý; 3) 
Thần thoại: niềm tin phổ biến rằng dữ liệu lớn 
biết mà trước đây không thể, với hào quang 
của sự thật, khách quan, chính xác” [2].
Sau đó, Gartner - công ty nghiên cứu và 
tư vấn công nghệ thông tin - bổ sung thêm 
rằng “Big Data ngoài 3 tính chất (số lượng, 
tốc độ xử lý và chủng loại) thì còn phải cần 
khám phá sâu vào sự vật/sự việc và tối ưu 
hóa các quy trình làm việc” [5]. Cùng quan 
điểm đó, Tan Jee Toon cho rằng Big Data 
mọi thứ xung quanh chúng ta, từ các thiết bị 
kỹ thuật số như di động, video, hình ảnh, tin 
nhắn tới các thiết bị cảm biến, các máy móc 
hội. Big Data có đặc điểm là được sinh ra với 
khối lượng (volume), tốc độ (velocity), độ đa 
dạng (variety) và tính xác thực (veracity) rất 
lớn [16].
Năm 2014, Gartner đưa ra khái niệm 
mới về Big Data bằng mô hình “5Vs”, gồm: 
Volume (khối lượng), Velocity (tốc độ), 
Variety (tính đa dạng), Veracity (tính xác 
thực) và Value (giá trị). Trong đó: Volume là 
khối lượng Big Data được tạo ra mỗi ngày. 
phân tán, nơi mà dữ liệu chỉ được lưu trữ một 
bởi phần mềm. Velocity là tốc độ dữ liệu mới 
được tạo ra và tốc độ dữ liệu chuyển động. 
giữ chúng trong các cơ sở dữ liệu. Variety là 
các kiểu khác nhau của dữ liệu. Công nghệ 
có cấu trúc truyền thống (được lưu trữ trong 
các bảng hoặc các cơ sở dữ liệu quan hệ) và 
phi cấu trúc (bao gồm các thông điệp, trao 
đổi của mạng xã hội, các hình ảnh, dữ liệu 
cảm biến, video, tiếng nói...). Veracity là tính 
hỗn độn hoặc tính tin cậy của dữ liệu. Công 
kiểm soát những loại dữ liệu này. Value là giá 
trị của dữ liệu. Việc tiếp cận Big Data sẽ chỉ 
thành những thứ có giá trị. Đây là khái niệm 
đầy đủ về 5 tính chất của Big Data [5].
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 25
THÔNG TIN VÀ TƯ LIỆU - 1/2020 25
2. XU HƯỚNG ỨNG DỤNG BIG DATA TRONG HOẠT 
ĐỘNG THÔNG TIN - THƯ VIỆN
Ngày nay, một lượng lớn dữ liệu số có thể 
các mạng xã hội. Theo Howe D. (2008): “Chỉ 
riêng trong các lĩnh vực nghiên cứu khoa học, 
trưởng theo cấp số nhân” [7]. Số lượng dữ 
nhiều lĩnh vực khác nhau và dữ liệu lớn (Big 
Data) được sử dụng rộng rãi trong các lĩnh 
vực, tổ chức với nhiều mục đích khác nhau. 
hành vi tiêu dùng của khách hàng, để đề xuất 
trên thông tin thu thập được (Ebay, Facebook, 
Google...). Các cơ sở nghiên cứu khoa học sử 
khoa học mới, ví dụ như xây dựng bản đồ 
gene của con người,... Việc sử dụng Big Data 
trong hoạt động thông tin - thư viện đã bắt 
đầu được quan tâm nghiên cứu. Campbell 
D. Grant, Cowan Scott R. (2016) phân tích 
và dữ liệu liên kết [1]. Kim Young Seok (2017) 
khuôn mặt Chernoff [9]. Gerrard D., Mooney 
J. và Thompson D. (2017) xem xét kiến trúc 
phân tích dữ liệu, các bộ tài nguyên được bảo 
thời gian tới [6]. Waqar Ahmed và Kanwal 
Ameen (2017) tổng quan các khái niệm về 
thư viện [17]. Ye Chunlei (2017) nghiên cứu 
trong thư viện đại học [18]. Zhan Ming, Widén 
Gunilla (2018) nghiên cứu vai trò của thư 
viện công cộng trong thời đại Big Data [20]. 
Li Shuqing; Jiao Fusen; Zhang Yong; Xu Xia 
(2019) nghiên cứu các vấn đề và thay đổi của 
dịch vụ người dùng tin [10],... Các nghiên cứu 
trong thời đại Big Data. Bàn về xu hướng ứng 
tiếp nào, nhưng có thể tổng hợp thành các xu 
hướng chính như sau:
Một là, tổ chức lưu trữ, bảo quản dữ liệu
Marydee Ojala nhận định: “Các thư viện 
ngày nay. Bộ sưu tập các tài nguyên số được 
các thư viện. Khối lượng và tính đa dạng dữ 
thư viện phải có phương pháp tổ chức lưu trữ, 
bảo quản dữ liệu hợp lý” [12]. Nguồn dữ liệu 
thư viện bao gồm: nguồn dữ liệu mô tả tài liệu 
thư viện, nguồn tài nguyên số hóa tài liệu thư 
viện, nguồn tài liệu số thư viện bổ sung qua 
việc mua hay sử dụng chung, nguồn dữ liệu 
khảo sát thư viện, dữ liệu định tính, dữ liệu 
tương tác xã hội,... Trước đây, các thư viện 
băng, đặt trong các cơ sở lưu trữ. Trước tác 
mạng công nghiệp lần thứ tư, các thư viện 
chi phí hiệu quả. Dữ liệu được lưu trữ theo 
hai cách, cả trên các thiết bị ngoại tuyến (thẻ 
nhớ SD, ổ cứng ngoài, ổ đĩa flash) và lưu trữ 
trực tuyến trên đám mây. Với phương thức 
kết hợp sử dụng băng từ để bảo quản lưu trữ, 
được yêu cầu, và sử dụng lưu trữ đám mây 
cho các Big Data. Các thư viện hướng đến 
thư viện (bao gồm cả tài nguyên vật chất và 
dữ liệu), xác định nhu cầu của người dùng 
thư viện. Trong thời gian tới, khi các yêu cầu 
mới thúc đẩy việc sử dụng Big Data, các thư 
viện hướng tới việc thu nhận, tổ chức lưu trữ 
dữ liệu (lưu trữ vật lý trong các máy chủ hoặc 
trong các cơ sở dữ liệu), bảo tồn dữ liệu và 
phổ biến dữ liệu, làm cho dữ liệu có sẵn trong 
qua các sản phẩm trực quan. Các thư viện 
tiến tới xây dựng, tạo lập hệ thống bảo quản 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
26
kỹ thuật số (bảo tồn cả tài nguyên số và siêu 
dữ liệu mô tả) có thể phát triển trong nhiều 
năm tới để đáp ứng với các yêu cầu mới. 
Hai là, cung cấp sản phẩm, dịch vụ thư 
viện mang tính cá nhân hóa, tùy chỉnh 
 Hiện nay, các thư viện đang có xu 
vụ trực tuyến. Nhiều thư viện đang sử dụng 
facebook, instagram để quảng bá các sản 
phẩm và dịch vụ thư viện. Các phương tiện 
thư viện. Cùng với các dữ liệu khảo sát thư 
viện, dữ liệu định tính (thông qua phỏng vấn, 
bảng trò chuyện...), dữ liệu tương tác xã hội 
(từ các trang truyền thông xã hội)... các thư 
thư viện. Từ đó, thư viện cung cấp các sản 
cầu của người dùng. Tuy nhiên, trong thời 
gian tới, “sự gia tăng của Big Data làm cho 
nhanh hơn, cho phép các thư viện vượt ra 
học tập và phân tích hiệu suất nghiên cứu” 
[19]. “Trong kỷ nguyên Big Data, không chỉ 
Big Data để đổi mới.” [10]. “Big Data có thể 
cũng sẽ thay đổi cho phù hợp” [17]. Các nhà 
có thể tác động đến hoạt động thông tin - thư 
viện, chuyển đổi phương thức cung cấp dịch 
vụ và tích hợp các hệ thống xử lý. Các hỗ trợ 
cạnh tranh để thư viện thu hút người dùng tin. 
Theo Li Shuqing, Jiao Fusen, Zhang Yong, 
Xu Xia: “Các vấn đề và tiềm năng của các thư 
dữ liệu, công nghệ, dịch vụ và người dùng 
tin. Sử dụng Big Data hiện có và xem xét các 
tại theo quan điểm của người dùng tin, thư 
viện có thể đưa ra các ý tưởng, phương pháp 
có trong các thư viện số” [10]. Đồng thời, nhu 
dùng tin. Kim Young Seok cho rằng: “Bằng 
thực, các thư viện có thể thiết kế các dịch 
tin. Big Data cung cấp thông tin chuyên sâu 
dùng tin, từ đó tạo ra trải nghiệm cá nhân 
hóa” [9]. Ví dụ, người dùng tin tìm kiếm trên 
gì người dùng tin gõ ở mục tìm kiếm, tần suất 
tìm kiếm, số lần tham khảo danh mục tài liệu, 
số lần xem mô tả tài liệu,... được thu thập và 
phân tích để tối ưu trải nghiệm, tạo cơ hội lớn 
hóa. Đặc biệt, với các công cụ phân tích dự 
báo của Big Data, thư viện sẽ nắm được thị 
hiếu, nhu cầu chính xác để cung cấp các sản 
phẩm, dịch vụ phù hợp với người dùng tin 
trong thời gian thực.
Ba là, ứng dụng dịch vụ phân tích dự báo
Giống như hầu hết các ngành khác, phân 
tích dự báo sẽ là một sự thay đổi lớn, quan 
trọng trong các cơ quan thông tin - thư viện. 
hoạt động hiệu quả hơn, đồng thời làm thay 
người dùng tin. Theo cách truyền thống, mối 
khá đơn giản. Người dùng thư viện nộp tiền, 
làm thẻ thư viện và đổi lại, họ được phục vụ 
trong các dịch vụ khác nhau của thư viện. Tuy 
nhiên, mối quan hệ này đang dần thay đổi 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 27
viện. Người dùng thư viện cung cấp dữ liệu 
hành vi người dùng. Thông qua các dữ liệu cá 
nhân như lịch sử sử dụng tài liệu thư viện, lịch 
sử tìm kiếm, cách thức, thói quen tìm kiếm,... 
các công cụ Big Data phân tích dữ liệu, đưa 
ra thông tin chi tiết, xác định khuynh hướng, 
nhu cầu sử dụng thư viện, nhu cầu tài liệu 
người dùng thư viện trong tương lai; các công 
cầu,... Simovic Aleksandar (2018) nhận định: 
“Các công cụ Big Data kết hợp với các thuật 
toán khoa học cho phép các thư viện dự đoán 
lai, giúp dự báo tốt hơn các vấn đề phát sinh 
tin tốt nhất cho người dùng tin” [14]. Về phía 
thư viện, việc sử dụng tài nguyên Big Data 
của người dùng tin, có thể đưa ra các ý tưởng 
các dịch vụ hiện có trong thư viện kỹ thuật số. 
Đồng thời, căn cứ vào các kết quả phân tích, 
dự báo, thư viện có thể xác định thời gian, 
và dịch vụ thư viện đến người dùng thư viện. 
Về phía người dùng thư viện, dựa vào các 
kết quả dự báo về hành vi tìm kiếm, tra cứu, 
sử dụng thư viện, các hệ thống khuyến nghị 
(Recommendation Engine) sẽ gửi đến người 
dùng tin các tài liệu có thể họ quan tâm.
Bốn là, mở rộng dịch vụ chăm sóc 
thư viện, đặc biệt trong môi trường thư viện 
điện tử, thư viện số. Các thư viện đang cố 
gắng để hiểu được người dùng thư viện, giúp 
các thắc mắc, nhu cầu của mình. Big Data 
hoạt, tạo ra giá trị từ quá trình xây dựng mối 
quan hệ thân thiết với người dùng thư viện. 
Cùng với Big Data, hệ thống trả lời tự động 
(như Chatbot) không cần sự trợ giúp của 
con người phát triển tương ứng, giúp tăng 
hiệu quả phân tích dữ liệu Big Data. Hiện 
nay, nhiều thư viện sử dụng Chatbot để giao 
tiếp, trao đổi với người dùng thư viện, tiếp 
các nhu cầu của người dùng. Khi người dùng 
viện, họ có rất nhiều thắc mắc và muốn được 
giải đáp. Chatbot sẽ đưa ra các gợi ý, hỗ trợ 
từng bước một, cung cấp thông tin về các 
sản phẩm, dịch vụ của thư viện cho người 
dùng. Chatbot được thiết kế và phát triển để 
đối thoại. Qua những dữ liệu người dùng thu 
thập được, công cụ phân tích dữ liệu Big Data 
tiến hành phân tích, xác định những nhu cầu, 
dùng thư viện. Bên cạnh đó, Chatbot nhắc 
viện như thời hạn trả tài liệu, thời hạn đổi thẻ 
sử dụng,... Đặc biệt, Chatbot giúp thư viện 
chủ động hỗ trợ 24/7, tăng trải nghiệm tối 
đa cho người dùng thư viện mọi lúc. Chatbot 
lưu lại lịch sử đối thoại, thông tin người dùng 
trong chính thư viện. Chatbot hỗ trợ các thư 
viện khai thác Big Data phục vụ người dùng. 
Trong tương lai, số thư viện sử dụng Chatbot 
tính năng và lợi ích mà Chatbot mang lại. 
Cùng với đó, thông qua dữ liệu người dùng, 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
28
các thư viện có thể phân tích, dự đoán các 
các sản phẩm, dịch vụ thông qua phân tích 
hiện các giải pháp kịp thời. 
Có thể thấy, Big Data giúp tối ưu hóa hoạt 
động thư viện bằng việc thu thập, phân tích 
thông tin, tăng trải nghiệm của người dùng 
tin bằng cách cá nhân hóa thư viện số. Cùng 
với đó, Big Data có thể giúp các thư viện tiến 
hành phân tích dự báo, tìm ra các đặc điểm 
chung dự báo thị hiếu đọc, tình trạng sử dụng 
các cơ sở dữ liệu. Không chỉ vậy, Big Data tạo 
dùng tin trong quá trình sử dụng thư viện.
1. Campbell D. Grant, Cowan Scott R. (2016). “The 
Paradox of privacy: revisiting a core library value in 
an age of big data and linked data”, Library trends, 
Vol. 64, No. 3, P. 492-811.
2. 
Boyd, 
Kate 
(2012). 
Critical Question for Big Data, Information, 
Communications and Society.
3. Doug Laney (2001). “3D Data Managment: 
controlling Data Volume, Velocity, and Variety”, 
Application Delivery Strategies, Meta Group. 
File:949.
4. Francis X. Diebold (2000). “Big Data Dynamic 
and Forecasting”, Discussion of Reichlin and 
Watson paper, in Economics and Econometrics, 
Eighth World Congress of the Econometric Society. 
5. Gartner (2013). Survey Analysis: Big Data Adoption 
in 2013 shows substance behind the hype. 
6. Gerrard, D., Mooney, J. , Thompson, D. (2017). 
“Digital Preservation at Big data scale: proposing a 
step - change in preservation system architectures”, 
Library Hi Tech, http://doi.org/10.1108/LHT-06-
2017-0122, truy cập ngày 17/10/2019.
7. Howe D. (2008). “The future of biocuration”, 
Nature 455, P. 47-50.
8. Kenneth Cukier (2010). “Data, data everywhere”, A 
special report on managing information, Economist 
Newspaper, Volume 394.
9. Kim Young Seok (2017). “Big data analysis of 
chernoff face method”, Journal of Documentation, 
Vol. 73, No. 3, P. 466-480.
10. Li Shuqing; Jiao Fúen; Zhang Yong; Xu Xia 
(2019). “Problems and Changes in Digital Libraries 
services”, Journal of Academic Librarianship, Vol, 
45.
11. Michael Cox, David Ellsworth (1997). “Application 
- Controlled Demand Paging for Out - of - Core 
Visualization”, Report NAS-97-010, NASA Ames 
Research Center.
12. Marydee Ojala (2018). “Big Data and AI: 
technology, transparency, and trust”, http://www.
infotoday.com, truy cập ngày 14/11/2019.
13. Randal E. Bryant, Randy H. Katz, và Edward D. 
Lazowska (2008). “Big - Data Computing: Creating 
in 
Commerce, 
Science and Society”, Computing Community 
Consortium, http://www.cra.org/ccc/initiatives, truy 
cập ngày 14/11/2019.
14. Simovic Aleksandar (2018). “A Big Data smart 
institution”, Library Hi Tech, Bradford, Vol. 36, Iss. 
3, tr.498-523
15. Steve Bryson, David Kenwright, Michael 
Cox, David Ellsworth, Robert Haimes (1999). 
“Visually exploring gigabyte data sets in real”, 
Communications of the ACM, Vol. 42, No.8, tr.83-
90.
16. Tan Jee Toon (2014). “Dữ liệu lớn, nhân 
tố thay đổi “cuộc chơi” của doanh nghiệp”, 
http://vneconomy.vn/cuoc-song-so/du-lieu-
lon-nhan-to-thay-doi-cuoc-choi-cua-doanh-
nghiep-20140422025542917.htm, truy cập ngày 
14/11/2019.
17. Waqar Ahmed, Kanwal Ameen (2017). “Defining 
the field of information and library management”, 
Library Hi Tech News, p. 21-24.
18. Ye Chunlei (2017). “Research on the key 
technology of big data service in university library”, 
the Institude of Electrical and Electronics Engineers, 
Inc. Conference Proceedings, Piscataway.
19. Yu Jen Chien (2016). “Library Data, Big Data or 
Better Data: Challenges from the Field”, ASIST 
Meeting, Proceeding of ASIST annual meeting, 
Vol 53, No. 1.
20. Zhan Ming, Widén Gunilla (2018). “Public 
libraries: roles in big data”, The Electronic library, 
Vol. 36, No.1, P. 133-145.
(Ngày Tòa soạn nhận được bài: 26-12-2019; 
Ngày phản biện đánh giá: 10-03-2020; Ngày chấp 
nhận đăng: 15-03-2020).
NGHIÊN CỨU - TRAO ĐỔI

Removed lines from big-data.pdf:
Adding Value to Manufacturing, Retail, Supply Chain, and 
Oklahoma State University, Stillwater, OK 74078 
The concept of big data has been around for many years.  Only in the last few years have organizations 
started to understand how they can use big data to gain insightful knowledge about their business operations, 
which is enabling them to make better business decisions. While there is no single definition, big data 
usually works on the principles of four Vs - Volume, Velocity, Variety, and Veracity. As the name suggests, 
big data is really big, meaning a huge amount of data is being generated daily, reaching the scale of 
petabytes. This data comes in all forms -  structured, semi-structured, and unstructured and is pouring in 
from all directions and generated by many systems and devices, such as transactional systems, log files, 
GPS devices, smartphones, RFID readers, surveillance cameras, sensor networks, Internet of Things (IoT), 
and social media. Finally, as big data becomes an important asset for enterprises, the focus is also on the 
trustworthiness of data and its sources. 
According to Gartner, Inc., “Big data is high-volume, high-velocity and high-variety information assets that 
demand cost-effective, innovative forms of information processing for enhanced insight and decision 
making.”a In this article, we first elaborate on the big data concept and present the storage and processing 
technologies that have been developed to deal with big data.  We then briefly discuss the evolution of 
traditional analytical processing to today’s big data analytics.   Through several applications and use cases, 
we illustrate how big data analytics is adding value to manufacturing, retail, supply chain, and logistics 
operations.  Finally, we conclude by discussing key challenges that businesses have to face as the use of 
big data analytics becomes more widespread. 
Regardless of the decision to be made - optimized production/work schedules, accurate forecasts, customer 
preferences - data nowadays has the potential to help businesses succeed more than ever before.  From an 
organizational perspective, big data is a holistic approach of obtaining actionable insight to create a 
competitive advantage over others.1 There are two distinct approaches to applying big data - improve the 
propositions. A challenge that organizations increasingly face is finding and working with trusted data. 
Working with inaccurate and untrusted data can be worse than having no data at all. As data requirements 
and regulations become more complex, organizations must be aware of where all their data is coming from, 
where it is getting stored, and who is interacting with this data as conclusions are drawn.2 
a https://www.gartner.com/it-glossary/big-data/ 
2 
What is Big Data? 
of data that needs to be handled and tracked, the speed at 
which the information is flowing into online systems, and 
regular basis. Because of the changes happening in the Web 
environment, new definitions for big data have been 
proposed, with a focus on technologies that handle this data. 
O’Reilly defines big data as “Big data is data that exceeds 
the processing capacity of conventional database systems. 
The data is too big, moves too fast, and doesn’t fit the 
structures of traditional database architectures. To gain 
value from this data, organizations must choose an 
alternative way to process it.”3 
To understand how big data is transforming businesses, we 
focus on the size of data in storage.4 Size is important but 
there are other aspects to big data namely variety, volume, 
and more recently, veracity.2 Together they are called the 4 
Vs of big data: Volume, Velocity, Variety, and Veracity. 
databases, data warehouses, and data marts.6 Here, the data 
is uploaded to operational data stores using Extract, 
Transform, and Load (ETL) tools which extract data from 
internal and external sources, transform the data to fit the 
operational needs, and finally load the data into the data 
warehouse. The key point is that the data is getting cleaned, 
transformed, and cataloged before being made available for 
data mining and online analytical functions. This traditional 
data warehouse approach discourages the incorporation of 
new data sources until they are cleansed and integrated.  
Since data is ubiquitous these days, big data storage 
environments need to be “magnetic” in nature, attracting 
data from all sources. Hence, big data calls for Magnetic, 
Agile, and Deep (MAD) analysis skills, which differs from 
the traditional data warehousing approach. Given the growing number of data sources and the sophisticated 
tools for data analysis, big data storage should allow analysts to easily process and use data rapidly. 
Solutions like distributed file systems and Massive Parallel Processing (MPP) databases are available 
nowadays for providing high query performance and platform scalability. Non-relational databases such as 
Not Only SQL (NoSQL) were developed for storing and managing unstructured data.7 These newer 
technologies aim for scalability, data model flexibility, and simplified application development and 
deployment. They separate data management and data storage and focus on high performance scalable data 
Volume. The ability to process a large 
social media, from Internet of Things to 
system logs, etc. 
Velocity. The rate at which data is 
getting created every second of the day. 
contributor, 
more 
data 
is 
generated and logged than ever before.5 
Also, the rapid adoption of social 
created a deluge of data. Advances in 
useful now. 
Variety.  It is the diversity of data 
which organizations are witnessing. 
processing a limited set of data, such as 
and 
logs. 
includes images, voice recordings, 
videos, and texts generated from 
media to deliver new insight. 
Veracity.  It is not just the quality of 
data, but also the trustworthiness of 
data sources.  Basic issues are the 
accuracy and applicability of data.  
uncertainty due to inconsistencies, 
incompleteness, ambiguities, etc. 
3 
storage, allowing management tasks to be written in the application layer instead of having it written in 
database specific languages.   
Why Big Data? 
When organizations adopt big data as a part of their business model, the first tangible question is usually 
what value this big data will provide to the company.7 Data must be used to make better decisions, to 
optimize resource consumption, and improve process quality and performance. It should also aim to 
perform precise customer segmentation, optimize customer satisfaction, and increase customer loyalty. 
from existing products and create additional revenue from new products. 
Newer Data Sources, Newer Opportunities 
The new sources of big data include industries which are taking a big step step towards digitization, and as 
a result, data growth in the past few years has been phenomenal. Some of the areas where data is coming 
from include social media, internet browsing pattern data, advertising response data, financial forecasts, 
location information, driving patterns, vehicle diagnostics, and traffic and weather data from sensors, 
monitors, and forecast systems. Other sources of data include data from healthcare, where the healthcare 
industry is implementing electronic medical records and digital imaging, which is used for short-term public 
health monitoring and long-term research programs.  Similarly, low cost gene sequencing can generate 
effectiveness in life sciences.8 Another area is data from video surveillance which is transitioning from 
patterns for security and service enhancement. Transportation and logistics industry has been generating 
and storing enormous amount of data coming from sensors, GPS transceivers, RFID tag readers, smart 
meters, cell phones, material handling equipment enabled with sensors, etc. This data can be used to 
opportunities. 
information contained therein.9 It involves applying algorithmic processes to derive insights. Analytics is 
used to extract previously unknown, useful, valid, and hidden patterns and information from large data 
sets.6 While the focus of analytics has been on inference, it can also provide prescriptive insights as 
explained later in this section.  Hence, analytics has a significant impact on research and technology, as 
businesses recognize its great potential in helping them gain competitive advantage. 
“Big data analytics is the use of advanced analytic techniques against very large and diverse data sets that 
include structured, semi-structured, and unstructured data from different sources, and in different sizes from 
terabytes to zettabytes.”b It helps in uncovering hidden patterns, unknown correlations, market trends, 
customer preferences and other useful information. Advanced analytics can help organizations discover 
what has changed and how they should react.  Analytics is the best way to discover new customer segments, 
identify the best suppliers, associate products of affinity, understand sales seasonality and so on.4 
Organizations are implementing specific forms of analytics tools and techniques which include data mining, 
statistical analysis, data visualization, artificial intelligence, machine learning, and other data capabilities 
b https://www.ibm.com/analytics/hadoop/big-data-analytics 
4 
which support analytics4. Though these techniques have been around for many years, organizations are 
using them now as most of these techniques adapt well to very large, multi-petabyte data sets. 
Big data’s worth is only realized when businesses can indulge in decision making using this data. To enable 
such data-driven decision making, organizations must use efficient processes to turn the high volume of 
fast moving and diverse data into meaningful insights. Analyzing big data allows researchers and businesses 
harness their data and use it to identify new opportunities which in turn leads to better and smarter business 
moves, more efficient operations, higher profits and satisfied customers and an overall competitive 
advantage.6 Big data analytics could be viewed as a sub-process in the complete process of knowledge 
extraction from big data.  
As organization began to adopt data analytics in the late 1990s and early 2000s, they faced many hurdles.  
professionals. Analysts used to spend more time collecting and preparing data than analyzing it.  They 
focused on finding more accurate and reliable solutions to business problems, while keeping the solutions 
simple at the same time so that business users could understand it.  Some examples of tools used during 
this time period are SAS, a tool for building backend data inference and modeling; Oracle and Teradata, 
detailed solution suites for easy development of solutions; IBM CPLEX, a tool for solving large 
optimization problems; and Cognos and MicroStrategy, tools for visualization, mostly in the form of 
reports. 
In late 2000s, social media giants like Google and Facebook and other internet-based companies in general 
started uncovering, collecting, and analyzing newer types of data which later evolved into big data. In 
addition to the data generated by companies in their internal operations and transactions, newer data was 
brought in from external sources including public data sources, social media, and mobile devices. Analysts 
realized this new data was qualitatively different (e.g., unstructured text, pictures, audio, and video) along 
with the much larger volumes as compared to internal company data.  This led to the development of newer 
tools and technologies, examples of which are Hadoop, a pioneer in distributed data storage and processing 
with low cost, flexibility, and scalability; Python and R, open source programming languages with vast and 
ever-evolving libraries for statistical data analysis; Tableau, Looker, and Microsoft Power BI, popular 
visualization products to develop, customize, and build visually appealing and interactive web dashboards. 
Descriptive, Predictive, and Prescriptive Analytics 
Analyzing data is not limited to deriving insights from the past, but it can also help businesses in predicting 
future outcomes and optimizing business performance. Currently organizations use three types of analytics 
at different stages in their decision-making process - Descriptive, Predictive, and Prescriptive analytics as 
shown in Figure 1. The latter two are also referred to collectively as advanced analytics. 
Descriptive analytics does exactly as the name suggests, ‘describe’ or summarize the data and convert it 
into something useful. It is the most basic type of analytics and almost 90% of the organizations today use 
this technique. Descriptive analytics is the analysis of historical data using data aggregation or data mining 
and lies at the bottom of the big data analytics value chain. However, it is extremely valuable because it 
organization’s future.  
Descriptive analytics is an important step to make raw data understandable to its users, and it helps in 
answering questions like “What is happening?” Consider for example, a metric that companies get from 
5 
web servers using Google Analytics tools, namely page views.  It can be used to determine if a strategy was 
a success or not. The main objective in descriptive analytics is to find the reasons behind the previous 
it can help the organization in strategizing.  
The majority of the statistics we use comes from descriptive analytics – e.g., calculations as simple as 
averages and standard deviations. Descriptive models use basic mathematical and statistical techniques to 
derive key performance indicators that can highlight the historical trends in data. STATA, MS Excel, and 
SPSS represent the older generation of descriptive analytics tools, while R and Python are quickly becoming 
the preferred tools in industry because of vast open-source libraries and the ease of development and 
deployment. Descriptive analytics can yield historical insights into an organization’s production, inventory 
levels, sales, operations, financials, and customer behavior.  
Figure 1. Analytics Framework by Tom Davenport26 
Predictive analytics can be defined as the ability to “predict” what might happen and a better understanding 
of future outcomes. It is one of the more sophisticated types of analytics techniques and employs statistical 
techniques and machine learning. It is used to detect clusters, tendencies, and exceptions, and to predict 
future trends, making it a valuable tool for forecasting. The foundation of predictive analytics is probability. 
It takes the data which the user has and tries to fill in the missing data values with best guesses. It helps in 
finding the answer to ‘What could happen?’ With properly tuned models, predictive analytics can support 
complex forecasting in marketing and sales. This helps an organization to set realistic goals for business, 
restrain expectations, and do effective planning. 
Tools used to apply predictive modeling vary by the nature of model’s complexity, but some commonly 
used tools are SAS, MATLAB, R, Python, among others. The common functionality of these tools is that 
they combine historical data found in POS, ERP, CRM, and HR systems to identify patterns in the data and 
apply algorithms such as random forest and Generalized Linear Model (GLM) for prediction, and K-means 
clustering for identifying clusters. Finally, simulation can be employed to statistically predict the outcomes 
of specific decision scenarios.   
6 
An application of predictive analytics is to produce credit scores, which are used by financial services to 
determine the probability of customer making timely payments. Other business uses include, how sales 
might close at the end of a year, inventory level forecasts, predicting what items a customer might purchase 
together and other customer purchasing patterns. Despite all the advantages that predictive analytics brings 
to the table, it is important to understand that forecasting is just an estimation, and its accuracy depends on 
the quality and stability of data.  
Prescriptive analytics is the most sophisticated analytics approach which makes use of optimization 
techniques to explore a given set of options and prescribe the best possible solution for a given scenario. 
As the name suggests, it “prescribes” a solution to a specific problem. One approach is machine learning 
neurons using training data sets.  Once trained, the neural network model can suggest the optimal course of 
action supporting the business objective for a given set of business inputs. Simulation, a predictive analytics 
tool at its core, can also be part of a powerful prescriptive analytics approach when combined with 
appropriate search or optimization techniques.  Prescriptive analytics not only predicts ‘What will happen?’, 
but also determines “What the company should do?” It provides recommendations for the actions to be 
taken to achieve optimal business performance. Because it has power to suggest optimal solutions, 
prescriptive analytics is the ultimate frontier for advanced analytics. 
Prescriptive analytical models are complex in nature. However, when implemented efficiently, prescriptive 
analytics can have a significant impact on the decision-making effectiveness of the organization. Technical 
advancements such as cloud computing have made deployment of these complex models much easier. 
prescriptive analytics to optimize production and inventory decisions in supply chains, optimize customer 
experience, and to make sure that the right product is being delivered at the right time.  Airline systems use 
travel factors, demand levels, purchasing patterns, timings, etc., in order to maximize the revenue generated.  
hence, they are ensuring to choose the right kind of analytics solutions to reduce operational cost, enhance 
service quality, and increase ROI. 
Big Data Analytics Applications and Use Cases 
Supply chain activities produce a huge amount of data, which is being continuously generated by systems 
and devices such as POS, ERP, SCM, RFID, GPS, blogs, and wiki entries, not to mention the unlimited 
data generated from sources like CCTVs, digital clickstreams, imagery, social media posts, and discussions 
on various forum platforms. Advanced connected devices and technologies which support today’s supply 
chain such as sensors, smart devices, and tags are continuously gathering real-time data and providing an 
end-to-end visibility in the supply chain. It becomes the task of supply chain managers to tap and process 
this data to make insightful decisions which could help boost productivity and reduce costs. 
7 
Application 
Area
Technique/Technology/
Supplier Insight Program Greater insight into suppliers' financial stability, 
performance, and ability to provide services.
Achieved product excellence, reduction in time-to-
market through co-development and co-production. 
Better alignment between engineers, suppliers, and 
customers.
Better service level; accurate prediction of customer 
needs and demand; automated planning and 
forecasting operations.
Inventory management with streaming analytics, real-
time data delivery and updates every few hours, and 
accurate performance analysis of each store.
Machine learning-based 
accuracy and greater profit margins.
application
Accurate forecasts, reduction in delivery time by 
upto 50%, and better service levels.
& Co.
Analytics application 
using Intel's Trusted 
Better tracking of in-store items using RFID tags; 
updating item location and inventory; helping 
salesperson track misplaced item to avoid lost sales.
Data-intensive 
Increase in forcasting accuracy; reduced inventory, 
stockouts, and obsolescence; better access to 
company's logistics needs.
Anticipatory shipping
before actual customer orders.
and packing area.
Drone-based delivery
Goods delivered to locations less than 30 minutes 
Cloud-based 3D 
Optimize picking accuracy, inventory turns, and 
warehouse productivity in real-time using inputs 
from sensors, such as  shelf weight and weight on 
forklift.
Quality early-warning 
Reduced rework, increased productivity and cost 
savings, higher quality standards, and improved 
service levels, by detecting and prioritizing quality 
related issues much sooner in the supply chain.
Greater demand and supply visibility, better 
distribution channel management, better service level, 
and improved inventory management.
collection.
Co.
Real-time monitoring and 
Greater visibility for customers, better pallet 
management, optimized space utilization, greater 
labor productivity, inventory accuracy of 99.9%, and 
improved customer satisfaction.
UPS
Optimized 55,000 delivery routes in North America, 
saving close to $400 million annually. Reducton in 
transportation.
shipment information, reduction in mileage and cost, 
and improved CO2 efficiency.
Resilience 360
Accuracy in risk detection, prevent production 
inefficiencies and revenue losses, maintain service 
levels, and reduce emergency cost by efficiently re-
routing shipments in case of unforeseen events.
analyze potential business opportunities. Real-time 
in a given location.
quality of address information is poor. Real-time 
ddress verification to optimize route planning.
DHL
Applications of Big Data Analytics
IBM
Supply Chain 
and 
8 
Applications and Use Cases in Manufacturing 
Raytheon, a major U.S. based defense contractor and industrial corporation, made use of data analytics to 
reduce costs within their supply chain and production operations. They developed a Supplier Insight 
program, which integrated structured and unstructured data from internal and external sources.10 With more 
than 10,000 suppliers, they needed a platform that could provide rapid, data-driven decision making 
capability. With this new system, they could track suppliers’ financial stability, performance, and their 
ability to provide services in the face of disruptive events. Raytheon was able to immediately identify if a 
supplier could provide what they needed, and quickly made decisions that reduced any adverse impact on 
their customers. Supplier Insight has allowed them to negotiate the cost better, by engaging in long-term 
contracts with suppliers for multiple programs.10 They now have an ability to look across all their suppliers 
and programs to achieve cost reductions. Raytheon has also developed smart factories which have the 
capacity to handle big data coming from different sources like sensors, instruments, CAD models, internet 
transactions, simulations, and digital records in the company, which equips them with real-time control of 
various elements of the production processes. For example, their Immersive Design Center (IDC) makes 
use of a 3-D immersive environment to achieve product excellence and decrease time-to-market through 
co-development and co-production of products by immersive data visualization and interaction.10 This also 
resulted in better alignment between their engineers, suppliers, and customers. They work together to refine 
the design and detect potential problems without the work and rework associated with expensive prototypes, 
resulting in reduced costs.10  
Lennox International, a U.S. based cooling and heating devices manufacturing company, integrated 
their expansion throughout North America.11 With the help of machine learning algorithms, they accurately 
predicted customer needs, while understanding customer demand better. It also helped the company to 
automate its planning and forecasting operations. 
Many companies gather data on supplier information and purchasing volumes for annual supplier 
performance review, spend analysis, and cost savings analysis functions to support strategic decisions.  For 
example, a pharmaceutical company created a database of all the bids submitted for packaging.12 This data 
was then evaluated to understand the cost structure of suppliers and to create detailed cost models for 
different packaging options. Such models can help in the selection of the most cost-effective supplier for 
new packaging.12 Another example is how IoT with its network of sensors embedded in millions of devices 
can enable new opportunities in manufacturing. For example, real-time information on a machine’s 
condition can initiate a production order for a spare part, which then can be shipped using a drone to the 
plant engineer for replacing the faulty or near faulty part.12 It also helps in determining when and how 
critical maintenance is required by a specific machine, thereby avoiding costly equipment breakdowns and 
improving the overall production efficiency.  
Daily production needs to be monitored to maintain the efficiency and output of a company. Big data 
analytics uses the data collected from operational machines, employee records, and data logs of the number 
of units produced, to provide insights to the operations manager, helping him/her to make changes that are 
profitable for the company. Manufacturers are also exploring predictive analytics to realize significant 
savings in product testing and improving product quality. Since different products and parts require 
different tests, instead of performing numerous quality tests on each part, data mining and pattern 
recognition can be used to determine the type and number of tests truly needed for each part or product.13  
9 
Applications and Use Cases in Retail 
Walmart, the number one fortune 500 company, has the world’s largest private cloud, which helps support 
real-time data feeds to its decision makers. Walmart’s Data Café based at their Bentonville, Arkansas 
headquarters takes care of most of this cloud architecture.14 Their original data infrastructure only enabled 
managers to get weekly reports, which prevented them from making decisions based on real-time market 
conditions. Also, the reports were standardized with little room for customization. Data café, which was 
built on SAP’s HANA in-memory analytics engine, enabled inventory management with streaming 
analytics, and provided an enterprise view of timely information flow for a large cross-sectional staff 
looking to resolve every-day business issues.14 The data delivered through this system is almost real-time 
and updated every few hours.  Furthermore, the system was designed to be responsive to providing reports 
and queries required by managers in the given time frame, which helped them gain timely insight and make 
better decisions. These insights are derived from “200 streams of internal and external data which includes 
40 petabytes of recent transactional data, and can be manipulated, modeled, and visualized.”14 The 
importance of near real-time insights is crucial since it helps managers respond to challenges in real-time 
as they arise. For example, on Black Friday, Walmart’s Data Café provides near real-time insights on the 
performance of east-coast stores, which enables Walmart to make pricing adjustments for west-coast stores 
before they open.14 During a recent Halloween, sales analysts were able to see that two stores were not 
selling a novelty cookie that was very popular in most stores. Using near real-time data from Data Café, it 
was discovered that simple stocking oversight led to the cookies not being put on shelves in these stores.14 
The company was able to react in real-time to avoid additional lost sales. Data Café also provides automated 
alerts to managers when a metric falls below a threshold in a department. This tool has reduced the problem-
solving time from weeks to minutes using reliable internal and external sources of data. 
Levi Strauss & Co, a leading American clothing company, provides better in-store shopping experience for 
items using IoT technology coupled with advanced analytics. Levi’s in collaboration with Intel® 
implemented a solution using Intel’s Trusted Analytics Platform (TAP), which helped salespersons to 
quickly find misplaced items in the store.15 This application made use of RFID tags woven into clothing 
items, in-store antenna sensors installed in the ceiling of the store to continuously track the RFID tags, and 
cloud-based analytical tool built on TAP for detailed analysis. This technology helped determine when 
items are no longer in their correct place or no longer available at that time. TAP algorithms use data 
collected overnight to determine the exact location of various groups of items, and during store hours 
sensors track the location of items and an algorithm determines if an item is in its assigned location. If an 
item is placed in its assigned group location, no action is generated by the algorithm.  Suppose a pair of 
jeans is lying in the T-shirt section or left in the fitting room, the TAP algorithm will generate an alert on 
the mobile application instructing the salesperson to put the item back in its assigned location.15 This helps 
the salesperson to keep the item where it belongs and avoid lost sales. Levi’s also aims to generate customer 
insight using big data analytics with the data collected from sensors tracking customers’ in-store behavior 
to better understand their preferences.15 
Groupe Danone, a French multinational food-product corporation, found itself making accurate predictions 
only 30 percent of the time for responses to promotional offers, which was resulting in significant losses to 
the company.11 When they implemented machine learning in their planning architecture, they saw 
significant improvement in both sales and forecasting. Similarly, Granarolo, an Italian dairy company, used 
machine learning to increase its forecasting accuracy by 5 percent, decreased delivery times by up to 50 
percent of the original time, which resulted in better service levels.11 Morrisons, one of UK’s largest food 
10 
retailers, was able to dramatically improve same store sales and achieve a 30% reduction in shelf gap and 
from Blue Yonder, which uses AI technology to “improve demand planning and reinvigorate replenishment 
based on customer behavior in every store.”16 Blue Yonder’s data-intensive forecasting methods deployed 
as cloud-based services is making such advanced capabilities accessible to other retailer’s as well.12 
Applications and Use Cases in Supply Chains and Warehouses 
In supply chain operations, planning and forecasting are among the most data-driven operations, which use 
an array of supply chain planning tools supported by ERP systems. With the use of supply chain analytics, 
it is now possible to re-envision the planning processes by using external and internal data sources to make 
real-time decisions based on market trends, uncertainty, seasonality, and other fluctuations.  
IBM understood the value of big data analytics early and employed it in optimizing their supply chain 
operations. They have used various analytical tools to solve a range of problems, and a few of them are 
discussed here.17 IBM’s Quality Early-Warning System (QEWS) was typically deployed upstream at 
suppliers, IBM’s operations, and in the field.  QEWS detects and prioritizes quality related issues much 
sooner than the traditional quality control processes. Analyzing big data coming from across their supply 
chain, IBM was able to reduce rework, increase productivity, ensure higher quality standards, and improve 
customer satisfaction, leading to significant cost savings. For a company like IBM, ensuring correct 
inventory levels with so many business partners was challenging. They made use of IBM Buying Analysis 
Tool, which not only provided demand and supply visibility, but ensured better distribution channel 
management, delivery of the right product at the right time to meet customer demand, while maintaining 
proper inventory levels. IBM also used a tool named Accounts Receivable, which uses advanced analytics 
to optimize the resources needed to collect revenues. They also make use of supply chain social listening, 
disrupt the supply chain.17 It also helps them obtain timely information and feedback on their products. As 
an early adopter, IBM has been using predictive and prescriptive analytics in its supply chain over the last 
several years. 
Warehousing is another area where big data analytics is creating new opportunities.  Logivations, a German 
supply chain solutions provider, offers a cloud-based 3D warehouse layout planning and optimization tool, 
camera-guided AGVs and tracking, and various other supply chain analytics solutions.18 Such technologies 
existing warehouse by simulating new configurations. Another example is the analysis of images and videos 
captured by AGVs, and sensor inputs including shelf weight and weight on the forklift, to monitor picking 
accuracy, inventory turns, and warehouse productivity in real-time.12 Also, forklift drive picking 
productivity and route optimization can be achieved by analyzing the route choices and driving behaviors.12  
A leading forklift provider is looking into all these opportunities, and figuring out how a forklift truck can 
be used as a big data hub - collecting  real-time data to identify additional sources of waste in the warehouse 
operations, using a hybrid of analytics and ERP and WMS data. Amazon is another warehouse automation 
pioneer, deploying Kiva robots that bring the items (racks) to the picking and packing area in their 
fulfillment centers. With increasing pressure to reduce order-to-delivery times, warehouses are turning to a 
flexible automation strategy by using autonomous technologies such as Amazon’s Kivac robots and 
GreyOrange’s Butlerd system to increase their picking efficiency.  Amazon has also tried to deliver goods 
c https://www.amazonrobotics.com/#/  
d https://www.greyorange.com/butler-goods-to-person-system  
11 
to people living less than 30 minutes away from an Amazon warehouse or distribution center via a drone. 
Amazon has also patented an “Anticipatory Shipping” technology to identify which orders should be 
packed and pushed into the logistics network before the actual customer orders are placed.12 
Merchandise Warehouse Co. (MW), a logistics provider of multi-temperature warehouse services in the US 
mid-west, provides services such as tempering, inspection, blast freezing, temperature monitoring, labeling, 
import/export, and packaging.19 With such operations there is little room for error, since clients’ food 
products could get spoiled if they are not maintained at correct temperatures.19 MW needed real-time 
enable quality assurance with comprehensive traceability.  They wanted this for all operations including 
inspections and holds.19 Technologies such as CCTV, WMS, electronic data interchange, mobile 
computers, and scanners were employed to help track and analyze data to get real-time information in the 
warehouse and manage inventory. It helped MW’s customers gain visibility by having on-line access to 
temperatures, activity reports, and information about inventory levels. MW’s solution also includes tools 
for pallet management for tracing every pallet from the time it arrives in the warehouse to until it leaves. 
inventory to marked temperature zones were provided by the new system. It also ensures greater labor 
productivity and accuracy using workflow-based warehouse management and could automate processes 
designed for specific customer needs. MW reaped various other benefits from this initiative like accurately 
capturing billing events in real-time resulting in reduced labor used for billing and paperwork. The system 
helped the company deal with the issue of “catch weight”, where the actual weight of the product,  especially 
meat, varies when it hits the retail shelves, a common problem in cold storage warehouses and food 
industry.19  Increased customer satisfaction levels were also achieved, since clients had real-time access to 
information and reports when needed. The solution helped MW achieve an inventory accuracy of 99.9 
percent from a previous 98.6 percent.19 
Applications and Use Cases in Logistics 
Logistics companies need to keep the goods moving at all times, even in the face of disruptions such as 
storms, cargos getting stranded due to ship crashes, and geopolitical events in order to keep the businesses 
running.  A Netherlands based logistics management company uses big data analytics on Microsoft’s Azure 
cloud to keep its customers informed about the number of goods in each container, their location at a given 
time, and expected delivery times.20 Purchase orders are tracked using mobile applications to identify 
challenges which could delay the delivery of an order. Tariff calculations and fees related to the movement 
of shipping containers are calculated by another application which can be accessed by the client, giving 
them a greater insight into financial risks.20 These mobile applications make use of big data analytics in 
internal supply chain operations to provide actionable business insights. Previously the time it took to 
identify a challenge and develop a solution to address it could be anywhere from 3 to 9 months. With the 
use of big data technologies, this time has been brought down to a couple of weeks depending upon how 
complex the problem is.20  
Companies managing their own supply chains and those outsourcing to third-party logistics providers 
manage a massive flow of freight, goods, and products daily while at the same time creating vast data sets. 
Millions of shipments are tracked daily from origins to destinations, generating information such as the 
content, weight, size, location, and route of each individual shipment, across a large number of networks. 
Companies are exploiting and analyzing these large data sets to improve their operational efficiencies, 
effectiveness, and customer service. A study by the Council of Supply Chain Management Professionals 
shows that 93 percent of shippers and 98 percent of 3PL providers feel that data-driven decision making is 
12 
a crucial supply chain activity.21 Also, 71 percent of these believe big data improves performance and 
quality. Logistics companies can utilize big data analytics to consolidate, interpret, and store the data 
coming from various sources for immediate or future use based on their requirements.  
Courier and delivery companies like UPS use real-time routing of deliveries using the trucks’ geo-location 
and traffic information data. UPS spent almost 10 years developing its On-Road Integrated Optimization 
and Navigation system (ORION) to optimize close to 55,000 routes in North America in its delivery 
network.22 This system saves the company $300 million to $400 million annually by saving about 100 
million miles per year, which is a reduction of 10 million gallons of fuel consumed and reducing CO2 
emissions by almost 100,000 metric tons.22  Data mining techniques also help logistics companies deliver 
services with fewer delivery attempts, by using predictive analytics to predict when a customer is more 
likely to be available at home.12 Costs and carbon emissions can also be reduced by selecting the right mode 
of transportation for deliveries. An example is the use of supply chain analytics to understand the priority 
and can be delivered by rail.12  
Better transportation planning can be achieved with the use of Transportation Management System (TMS) 
which can help identify future shipping patterns, optimize routes, carrier selection, or loads, and secure 
necessary capacity. This is achieved by tracking shipment frequency and identifying the endpoints of supply 
chains by studying precise inbound and outbound statistics. Direct application of predictive analytics is 
helping logistics providers make real-time decisions which result in reduced costs, greater reliability, and 
improved customer satisfaction. For example, data streams produced by sensors on delivery trucks, beacons 
which broadcast their presence to nearby devices such as computers and smartphones, radar devices, and 
employing simulation models.23 When a shipment is going to be late, a carrier can make real-time 
adjustments to prevent bottlenecks further down the supply chain.23 
DHL, a global logistics provider, has extensively explored big data analytics in their supply chain activities 
and is currently employing several smart systems around their services. Increasing the last mile efficiencies 
is often the most expensive step in the supply chain.24 Last mile optimization is an extensively studied area 
and researchers have found promising applications of big data analytics here. Data analytics is applied to 
achieve real-time optimization of delivery routes, where streams of data are processed to maximize the 
performance of the delivery fleet. Rapid processing of real-time information supports the goal of route 
optimization on the last mile, saving time in the delivery process. When the vehicles are loaded and 
unloaded, manual sequencing of shipments is eliminated by the use of sensors, and dynamic calculations 
are used to find the optimal delivery sequence. Based on real-time traffic conditions on the road, telematic 
databases are used to change the delivery route automatically. DHL’s SmartTruck uses data mining, 
machine learning, and other data analytics techniques to optimize the initial tour planning based on 
incoming shipment on a daily basis.24 Dynamic routing system recalculates the routes depending on the 
traffic situations and delivery times. This also results in cost reduction and improved CO2 efficiency by 
reducing the miles travelled. 
It is vital for robust supply chains to be able to cope with unforeseen events in today’s rapidly changing 
world. Apart from being flexible and resilient, businesses need accurate risk detection systems to keep 
running smoothly. Big data analytics and complex event processing algorithms are used to alert businesses 
when a pattern falls in the set of critical conditions such as tornadoes or floods in an area, or breakdown of 
fleet. These alert systems send a report on the probability and impact of the risk and provide suitable 
actionable insight to alleviate potential interruption. With this information on hand, customers can re-route 
their shipments or manage supplies from other distribution locations. DHL’s Resilience 360 risk 
13 
management solutions aims to provide such functionalities.24 It is equipped with two components, a risk 
assessment portion and supply chain monitoring instruments, both operating in real time. This improves the 
resilience of the supply chain and prevents production inefficiencies and revenue losses. Resilience 360 is 
designed to maintain prescribed service levels, protect sales and operations, and reduce emergency costs, 
creating a competitive advantage for the company.24 
Future economic development is often modeled on global transportation of goods and services. The type of 
goods shipped indicate the local demand and supply preferences. Logistics providers make use of big data 
shipments by their distribution networks. These shipment records are a valuable resource for market 
intelligence research, and logistics providers refine this data to substantiate existing market research. 
Regression analysis techniques are used to produce demand and supply forecasts with the use of the 
shipment records and market research outcomes. The primary target group for these advanced data analytics 
services are small and medium-sized enterprises, which lack capacity to conduct their own market research. 
The results from regression-based analytics have high predictive value, which can help these enterprises 
serve a larger customer base, and generate accurate forecasts based on industry, geography, and product 
category. DHL Geovista is one such online geo-marketing tool available for small and medium-sized 
enterprises to analyze potential business opportunities.24  
shipments more accurately.24 Customer’s delivery address verification is a fundamental requirement for 
any logistics provider. This can be troublesome in developing countries and other remote areas, where the 
area. Address Management uses daily freight and parcel delivery data and matches this data with reference 
data and returns the incorrect incoming data with validated data from the database, in order to verify the 
address in real-time and optimize route planning for retailers and public sector entities.  
Other Applications 
There are several other applications of big data analytics which a company can encounter on a regular basis. 
Locating a new store is a strategic decision for a company, and big data analytics could play an important 
role here. Extensive data analysis is performed by the analysts in exploring customer data, demographic 
factors, retailer network, location of other competitors in the area, and market potential. A recent example 
of this is the location for Amazon’s HQ2. Visualizing the growth of a company has become easier with the 
use of data analytics, since it is now possible to quickly compare the performance matrix of different sites 
and identify the reasons behind such results. Predictive analytics comes in handy in analyzing the market 
and gaining insight on questions related to global growth strategy, site relocation, new product introduction, 
and supplier selection. 
business profitable.25 Data analytics tools simplify the process of price formation, which not only accounts 
for the cost of production of an item, but also the spending capacity of the customers and presence of 
competitors in the market. Price flexibility, buying patterns of the customers, competitors’ prices, and 
seasonality are analyzed using the data coming from various sources. Machine learning algorithms help 
to changes in prices. Furthermore, using real-time price optimization techniques, retailers can attract new 
customers and retain existing customers by adjusting the price as per market trends. Recommendation 
engines is another great way of predicting customers’ behavior, since they give a retailer insight into 
customers’ reviews and opinions. It also helps the retailers to increase sales and stay abreast with trends. 
14 
Based on machine learning algorithms, recommendation engines make adjustments depending on customer 
preferences, previous shopping and browsing experience, demographic data, need, and usefulness. 
Collaborative or content-based data filtering is used in this process to gain useful insight which gives 
leverage to retailers on customers’ opinions.  
Companies often fail to understand what big data is, its benefits, and more importantly the computing and 
the human infrastructure required to realize its true potential. Without a clear understanding of the concept 
of big data, adopting and implementing a project using big data tools can seriously challenge its success. 
Having discussed various applications and use cases of implementing big data technologies in 
manufacturing, retail, supply chain, and logistics, it is important to understand the associated challenges. 
they decide to invest in technologies using big data.  
reducing their dependence on legacy systems. Even though the industry is shifting its focus to the digital 
age with adoption of IoT and artificial intelligence, it is still a long way before the full potential of big data 
is realized. Industry has to develop an awareness of the various elements of the big data landscape, which 
include sensors to social media that collect data, in-memory to cloud for data storage, data mining to deep 
learning to convert data into useful business insights or actions.  Any new business solution will involve a 
significantly. Most people are resistant to change, and it shows in companies when workers stick to to an 
old way of thinking and doing work. An example is the use of Excel, which to the present day remains one 
of the popular tools in many companies, despite having  many limitations when compared to newer tools.27 
While there is a need to educate industry to change this legacy mentality, there is no need for an abrupt or 
complete shift to newer tools. A viable option is to slowly augment existing systems with big data analytics 
tools and capabilities.   
With the phenomenal increase in the size of data, the problem of storage space for big data has become a 
real problem for many companies. Cloud storage is soon becoming the only viable alternative with the ever-
increasing need for storage space.  With the maturity of the cloud computing infrastructure, which includes 
storage, applications, and computing platforms, companies are beginning to consider shifting to the cloud 
infrastructure for most of their computing needs. But transitioning from the traditional in-house computing 
infrastructure to the cloud infrastructure has its own challenges.  According to McAfee, “Most organizations 
that have been around awhile have a hodgepodge of hardware, operating systems, and applications, often 
described as ‘legacy spaghetti’.”28 First, companies have to address legacy system issues and simplify their 
system before moving to the cloud. For the most part, cloud is cost-effective compared to building and 
running an IT infrastructure.  However, a company needs to carefully evaluate the cost factor based on their 
specific needs, for example, in-house applications requiring continuous transfer of large data sets. 
analytics with specialized MS degrees in Data Science.  These degree programs are housed mostly in 
business schools or computer science departments.  Engineering schools to a large extent are still lagging 
in providing adequate training in data science to their graduates.   Data science professionals can manage 
and analyze large volumes of real-time data coming from multiple sources and in different formats. With 
several new technologies such as the NoSQL data management framework, Hadoop, cloud computing, and 
in-memory analytics, their skills are vital for the rapidly changing computing landscape. Given that 
engineering schools are still looking for the right curriculum mix (e.g., minors, degree options, and 
15 
certificates) to train engineers in data science, training employees at entry level is a challenging and 
expensive proposition for companies dealing with these newer technologies. When industry hires data 
science professionals, akin to software developers and programmers, they need guidance from subject 
matter experts (SMEs) to build the right tools and techniques that can help industry harness the power of 
big data in the long-run.  Industry needs to quickly educate SMEs to understand the big data analytics 
professionals.   
As seen in recent times, data privacy has become one of the major concerns of organizations. With recent 
threats like hacking of personal data, individuals and companies have become apprehensive about linking 
data from multiple sources as it may compromise an individual’s privacy. Also, with an increase in the 
number of connected devices within the industry, data security has also become a big concern and presently 
this risk is greater than ever. Big data analysis uses huge amounts of data for analysis and mining purposes 
to reach some meaningful conclusion, and security of this big data can be enhanced by using techniques 
such as authentication, authorization, and encryption.   
Effective flow and sharing of information among supply chain partners is critical to the success of today’s 
digital supply chains.  Unauthorized disclosure and data leakage of information shared among supply chain 
partners have been identified as two main threats in today’s digital supply chains.29 Visibility needed within 
a supply chain and consumers’ demand for transparency seem to be at odds with security requirements.  
With newer, secure technologies such as blockchain and data cleanroom, it is possible to achieve both 
visibility and transparency.30  Data cleanroom is a shared environment between two or more supply chain 
visibility to their data. Blockchain, a decentralized, distributed database is one of the most secure options 
available for supply chain partners for real-time information tracking.  Another important, but often 
overlooked challenge is the ethical use of data.  The legal infrastructure has not kept up with the rapid 
development in technology, which is able to collect and store vast amounts of consumer data with or without 
their knowledge.  While it may be legal, certain use of the data may be considered unethical.  Such actions 
may have a negative impact on a company as today’s consumers are more educated and have experienced 
negative consequences of such unethical usage. 
In a recent survey of supply chain professionals conducted by APQC, “lack of people with the needed 
skills” was identified as the biggest barrier to advanced analytics applications in industry.31  In addition, 
these employees need “a good understanding of the business to provide solid advice.”29  Resistance to 
change and lack of access to data across disparate systems were the second and third biggest barriers, 
respectively.  In addition to lack of access to data, issues such as inconsistent and unorganized data are also 
issues in some cases as different companies record their data in different formats, platforms, and systems.27 
useful insights.  
As companies make a push for big data analytics applications, they should first establish a clear business 
need such as “solving a problem or seizing an opportunity.”7  According to Watson, “big data initiatives 
should start with a specific or narrowly defined set of objectives rather than a ‘build it and they will come’ 
approach.”7 Pilot schemes are a good way to demonstrate the value of big data analytics.32  It is common to 
focus the initial business case for big data analytics on customer-centric objectives.7 The various 
applications and uses cases discussed earlier cover many different areas that have benefited from big data 
analytics. Whatever be the area, it is desirable that the pilot project address a problem tied to a specific 
16 
business outcome.  The pilot project should not only help solve a business problem, but also demonstrate 
the effectiveness of big data analytics for the organization and its stakeholders. Finally, for successful big 
data initiatives it is essential to have strong, committed sponsorship and alignment between the business 
and analytics strategies.7 In the early stages of adoption, the sponsor could be the CIO and then shifting to 
function-specific executives as business opportunities are identified.   
To benefit from big data analytics companies must also establish a data-driven decision-making culture, 
which calls for acting on insights from data rather than on pure managerial intuition.32 Promotion of data-
sharing practices, increased availability of training in data analytics, and communication of the benefits of 
data-driven decision making are some of the strategies for promoting a data-drive culture.7 While workforce 
training needs to focus on improving technological and digital proficiency, the future work environment 
also demands training in certain soft skills.  The work environment is changing with the rapid introduction 
of AI, automation, and analytics-driven solutions.  Workers need to be open to new ways of working and 
have openness to agility, adaptability, and working in teams to cope with a constantly changing external 
environment.  In the long-run, big data needs to become an integral part of the organization’s operating 
model. There also needs to be clear ownership for big data in the organization with leadership positions 
such as a chief analytics officer.32 Data science should become another established skill in the organization.   
during the development of this white paper.  We would like convey our appreciation to Scott Wahl for his 
guidance and feedback during the formative stages of this effort.  We would also like to thank John 
Ashodian, John Hill, Ying Tat Leung, Juan Ma, Hari Padmanabhan, and John Paxton for carefully reading 
an earlier version of this white paper and providing several constructive suggestions and feedback, which 
have helped us greatly improve the quality of the white paper. 
17 
1. Morten Brinch, Jan Stentoft, and Jesper K. Jensen, “Big Data and its Applications in Supply Chain 
Management: Findings from a Delphi Study,” Proceedings of the 50th Hawaii International Conference 
on System Sciences, 2017: 1351-1360.  
2. IBM Corporation, “The Path to Data Veracity,” IBM Big Data and Analytics Hub, May 2018, 
https://www.ibmbigdatahub.com/whitepaper/path-data-veracity 
3. DataStax Corporation, “Big Data: Beyond the Hype,” October 2013, 
https://www.datastax.com/resources/whitepapers/bigdata 
4. Phillip Russom, “Big Data Analytics,” TDWI Research, 2011, 
https://tdwi.org/research/2011/09/best-practices-report-q4-big-data-
analytics.aspx?tc=page0&m=1  
5. DXC Technology Company, “Five Industries Where Big Data is Making a Difference,” November 
2015, https://assets1.dxc.technology/analytics/downloads/DXC-Analytics-
Five_Industries_Where_Big_Data_is_Making_a_Difference-4AA5-6292ENW.pdf 
6. Nada Elgendy and Ahmed Elragal, “Big Data Analytics: A Literature Review Paper,” In: Perner P. 
(eds) Advances in Data Mining. Applications and Theoretical Aspects. ICDM 2014. Lecture Notes in 
Computer Science, vol 8557, Springer, Cham., 2014, https://link.springer.com/chapter/10.1007/978-
3-319-08976-8_16  
7. Hugh J. Watson, "Tutorial: Big Data Analytics: Concepts, Technologies, and Applications," 
Communications of the Association for Information Systems, 34 (2014), Article 65. 
http://aisel.aisnet.org/cais/vol34/iss1/65  
8. Richard L. Villars, Carl W. Olofson, and Matthew Eastwood, “Big Data: What It Is and Why You 
Should Care,” International Data Corporation, 2011. 
http://www.tracemyflows.com/uploads/big_data/idc_amd_big_data_whitepaper.pdf 
9. Sunil Tiwari, H.M. Wee, and Yosef Daryanto, “Big Data Analytics in Supply Chain Management 
Between 2010 and 2016: Insights to Industries,” Computers and Industrial Engineering, 115 (2017): 319-
330.  
10. Bob Trebilcock, “Supply Chain, Data Analytics, and Big Data,” Logistics Management, August 2015.  
https://www.logisticsmgmt.com/article/supply_chain_data_analytics_and_big_data  
11. Kaushik Pal, “How Machine Learning Can Improve Supply Chain Efficiency,” Techopedia, February-
2018.  https://www.techopedia.com/2/31846/trends/big-data/how-machine-learning-can-improve-
supply-chain-efficiency  
12.  McKinsey & Company, “Big Data and the Supply Chain: The Big-Supply-Chain Analytics 
Landscape: Part 1,” February 2016,  https://www.mckinsey.com/business-functions/operations/our-
insights/big-data-and-the-supply-chain-the-big-supply-chain-analytics-landscape-part-1#  
13. Lorenzo Romano, “Big Data Analytics: A Key Ingredient for Agility in Manufacturing,” May 2019, 
https://www.orange-business.com/en/blogs/big-data-analytics-key-ingredient-agility-manufacturing  
18 
14. Joe McKendrick, “Walmart’s Gigantic Private Cloud for Real-Time Inventory Control,” RT 
Insights.com, January 2017. https://www.rtinsights.com/walmart-cloud-inventory-management-
real-time-data/  
15. RT Insights team, “Levi’s Real-Time Tracking of Jeans: RFID in Retail,” RT Insights.com, April 
2016. https://www.rtinsights.com/rfid-in-retail-customer-experience-levis/ 
16.  JDA, “Store Replenishment at Morrisons,” 2017, https://jda.com/knowledge-center/collateral/by-
morrisons-case-study  
17.  Hans W. Ittmann, “The Impact of Big Data and Business Analytics on Supply Chain Management,” 
Journal of Transport and Supply Chain Management, 9, no. 1 (2015). 
https://jtscm.co.za/index.php/jtscm/article/view/165/331  
18. Logivation, https://www.logivations.com/en/solutions/plan/design_efficiency.php  
19. RT Insights team, “Using Mobile Device for a Real-Time Warehouse,” 2016, 
https://www.rtinsights.com/zebra-omnii-xt15-datek-real-time-warehouse/  
 20. Motifworks, “How Big Data Analytics Can Benefit Supply Chain and Logistics Industry,” 2017. 
https://motifworks.com/2017/02/23/how-big-data-analytics-can-benefit-supply-chain-logistics-
industry/  
 21. “2017 Third-Party Logistics Study,” https://jda.com/-/media/jda/knowledge-center/thought-
leadership/2017stateoflogisticsreport_new.ashx  
 22. UPS, “ORION Backgrounder,” 2019, 
https://www.pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=Factsheet
s&id=1426321616277-282  
23. “Data-Driven Logistics: The Growing Use of Predictive Analytics,” July 2018, https://www.smith-
howard.com/data-driven-logistics-the-growing-use-of-predictive-analytics/ 
 24. Martin Jeske, Moritz Grüner, and Frank Weiẞ, “Big Data in Logistics – A DHL Perspective on How 
to Move Beyond the Hype,” December 2013. 
http://www.dhl.com/content/dam/downloads/g0/about_us/innovation/CSI_Studie_BIG_DATA.pdf  
25.  McKinsey & Company, “Big Data, Analytics, and the Future of Marketing and Sales,” March 2015, 
https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/Marketing%20and%20Sales/Our
%20Insights/EBook%20Big%20data%20analytics%20and%20the%20future%20of%20marketing%20sal
es/Big-Data-eBook.ashx 
26. Gurobi Optimization, “The Power of Analytics,” accessed September 8, 2019. 
http://www.gurobi.com/resources/prescriptive-analytics,  
27.  Transmetrics, “ Big Data and Big Roadblocks:  How the Logistics Industry can Overcome its Big 
Data Challenges,” March 2018, https://www.youredi.com/blog/logistics-industry-can-overcome-big-data-
28. Andrew McAfee, “What Every CEO Needs to Know About the Cloud,” Harvard Business Review, 
Nov. 2011: 124-132. 
19 
29. Bharat Bhargava, Rohit Ranchal, and Lotfi Ben Othmane, “Secure Information Sharing in Digital 
Supply Chains,” 3rd IEEE International Advanced Computing Conference, May 2013, 
https://www.cs.purdue.edu/homes/bb/Bhargava-Supply_Chain-Feb2013-india.pdf  
30. Megan Ray Nicholas, “How to Share Data Safely Across your Supply Chain,” 
https://www.smartdatacollective.com/share-data-safely-across-supply-chain/  
31. APQC, “APQC Quick Poll:  The Current State of Big Data & Advanced Analytics in Supply Chain,” 
May 2019, 
https://www.scmr.com/article/apqc_quick_poll_the_current_state_of_big_data_advanced_analytics_in_su
pply  
32. David Meer, “A Call to Action on Big Data,” Forbes, October 2014, 
https://www.forbes.com/sites/strategyand/2014/10/28/a-call-to-action-on-big-data/#6a4b6c22314  

Removed lines from KimAnh-HTKhoa.pdf:
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381804857
Tích Hợp Big Data Và Điện Toán Đám Mây: Động Lực Thúc Đẩy Thay Đổi Cho
Doanh Nghiệp.
Conference Paper · June 2024
CITATIONS
0
READS
376
1 author:
Vo Thi Kim Anh
Ton Duc Thang University
28 PUBLICATIONS   2 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Vo Thi Kim Anh on 29 June 2024.
The user has requested enhancement of the downloaded file.
KỶ YẾU
HỘI THẢO KHOA HỌC
KHOA CÔNG NGHỆ THÔNG TIN
LẦN 6
2024
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
i 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
KHOA CÔNG NGHỆ THÔNG TIN 
-------------- 
KỶ YẾU HỘI THẢO 
KHOA HỌC CÔNG NGHỆ LẦN 6 
Thành Phố Hồ Chí Minh, tháng 06 năm 2024 
(Lưu hành nội bộ) 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
ii 
BAN BIÊN TẬP 
1. TS. Nguyễn Hà Giang - Trưởng Khoa CNTT 
2. TS. Văn Thị Thiên Trang - Phó Trưởng Khoa CNTT 
3. ThS. Nguyễn Minh Tuấn - Phó Trưởng Khoa CNTT 
4. ThS. Trần Thành Công - Trợ lý Trưởng Khoa, Trưởng Ngành TMĐT 
5. ThS. Hoàng Văn Hiếu - Trưởng Ngành CNTT 
6. ThS. Võ Đình Ngà - Trưởng Ngành TKĐH 
7. ThS. Nguyễn Thị Hoài Linh - Trưởng Ngành KHDL 
8. ThS. Ngô Văn Công Bằng - Trưởng Bộ môn THUD 
9. ThS. Trương Nhã Bình - Trưởng Bộ môn Toán 
THƯ KÝ 
1. KS. Phạm Hữu Kỳ – Giảng viên Khoa CNTT 
2. Trần Thị Phương Anh – Thư ký Khoa CNTT 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iii 
LỜI GIỚI THIỆU 
Công nghệ thông tin đã và đang là yếu tố cốt lõi thúc đẩy nền kinh tế - xã hội phát triển 
mạnh mẽ, đặc biệt trong thời đại kỹ thuật số ngày nay. Sự bùng nổ của các công nghệ mới 
và ứng dụng tiên tiến đã thay đổi cách chúng ta sống, làm việc và tương tác. Với mục đích 
tạo ra một diễn đàn để các nhà nghiên cứu, học giả, giảng viên, cũng như các chuyên gia, 
trao đổi kết quả nghiên cứu, chia sẻ kiến thức, thảo luận quan điểm, ý tưởng về các xu 
hướng mới nhất trong lĩnh vực công nghệ thông tin và ứng dụng, Khoa Công nghệ thông 
tin, Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh (UEF) tổ chức hội thảo 
với chủ đề “Hội thảo khoa học công nghệ Khoa CNTT lần 6 năm 2024”.  
Hội thảo không chỉ nhằm mục đích nâng cao năng lực nghiên cứu mà còn thúc đẩy các 
phát minh, đổi mới và chuyển giao công nghệ trong lĩnh vực công nghệ thông tin. Đây là 
cơ hội để các chuyên gia đầu ngành, nhà nghiên cứu, giảng viên và sinh viên gặp gỡ, học 
hỏi và hợp tác, cùng nhau phát triển và ứng dụng các thành tựu khoa học kỹ thuật vào thực 
tiễn. Qua đó, hội thảo mong muốn góp phần nâng cao chất lượng giáo dục, nghiên cứu và 
thực hành trong lĩnh vực công nghệ thông tin. 
Do thời gian chuẩn bị có hạn, việc biên tập Kỷ yếu này không tránh khỏi những thiếu 
sót. Ban biên tập rất mong ý kiến đóng góp cũng như sự lượng thứ từ quý độc giả để các 
kỳ hội thảo sau được tổ chức ngày một tốt hơn, hiệu quả hơn 
Trân trọng! 
Tp. Hồ Chí Minh, tháng 6 năm 2024 
BAN BIÊN TẬP 
KHOA CÔNG NGHỆ THÔNG TIN 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
141-145 ĐIỆN BIÊN PHỦ, P.15, Q.BÌNH THẠNH, TP.HCM 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iv 
TỐI ƯU HÓA TRUY VẤN TRONG SQL SERVER: PHƯƠNG PHÁP VÀ ỨNG 
DỤNG..........................................................................................Trang 1 
Nguyễn Minh Tuấn - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KÊ CỦA CÔNG CỤ CHATGPT.......................................................Trang 14 
Nguyễn Văn Vinh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
RANSOMWARE: MỐI ĐE DỌA TRONG THỜI ĐẠI SỐ........................Trang 24 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
CHỮ KÝ.....................................................................................Trang 29 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC THÚC ĐẨY 
THAY ĐỔI CHO DOANH NGHIỆP.................................................Trang 35 
Võ Thị Kim Anh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
NHÂN CHO SINH VIÊN NGÀNH THIẾT KẾ ĐỒ HỌA.........................Trang 44 
Võ Đình Ngà - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
ỨNG DỤNG BÀI TOÁN VẬN TẢI: TỐI ƯU CHI PHÍ THU GOM RÁC SINH 
HOẠT CỦA CÁC BỆNH VIỆN..........................................................Trang 59 
Trương Nhã Bình - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
Ngô Thuận Dủ - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
DOANH CỦA DOANH NGHIỆP.......................................................Trang 70 
Hoàng Văn Hiếu - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
SÁNG TẠO NỘI DUNG AI: CÁCH MẠNG HÓA TƯƠNG LAI CỦA TIẾP THỊ NỘI 
DUNG .......................................................................................Trang 85 
Trần Thành Công - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 35 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC 
THÚC ĐẨY THAY ĐỔI CHO DOANH NGHIỆP. 
INTEGRATION OF BIG DATA AND CLOUD COMPUTING: A 
1Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh, anhvtk@uef.edu.vn  
Tóm tắt: Kỷ nguyên số mang đến sự bùng nổ dữ liệu, tạo ra cả thách thức và cơ hội cho doanh nghiệp. 
Sự hội tụ của Big Data và điện toán đám mây nổi lên như giải pháp mạnh mẽ, cách mạng hóa cách 
thức xử lý và khai thác dữ liệu. Bài viết này khám phá tác động biến đổi của sự kết hợp này, đồng thời 
đề xuất những cân nhắc thực tế cho doanh nghiệp bắt đầu áp dụng Big Data trên nền tảng đám mây. 
Từ khóa: Kỷ nguyên số, Big Data, điện toán đám mây, biến đổi, doanh nghiệp. 
Abstract: The digital era has ushered in an unprecedented surge of data, presenting both challenges 
and opportunities for businesses. The convergence of big data and cloud computing has emerged as a 
powerful solution, revolutionizing the way data is processed and harnessed. This paper delves into the 
embarking on their big data on cloud journey. 
Key words: Digital Era, Big Data, Cloud Computing, Transformation, Business 
1. Sự kết hợp mạnh mẽ giữa Big Data và 
liệu, mang đến cả thách thức và cơ hội cho 
doanh nghiệp. Khái niệm Big Data, với đặc 
trưng khối lượng, tốc độ và sự đa dạng, lần 
đầu tiên được giới thiệu bởi Laney (2001) [1] 
và khai thác thông tin. Tuy nhiên, việc quản 
minh là rất phức tạp. 
Sự xuất hiện của điện toán đám mây [2] 
Data, cung cấp giải pháp mạnh mẽ để giải 
quyết thách thức này. Điện toán đám mây 
internet, giúp doanh nghiệp tận dụng tối đa 
linh hoạt. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 36 
Dikaiakos et al. (2009) [3] nhấn mạnh về khả 
năng mở rộng, hiệu quả chi phí và khả năng 
truy cập. 
Đối với khả năng mở rộng: cơ sở hạ tầng 
trên nhu cầu xử lý, loại bỏ nhu cầu đầu tư ban 
đầu tốn kém vào phần cứng. Doanh nghiệp chỉ 
cần trả tiền cho các tài nguyên họ sử dụng, tối 
tức đầu tư [3]. 
Về hiệu quả về chi phí: doanh nghiệp chỉ 
trả tiền cho các tài nguyên họ sử dụng, tối ưu 
đầu tư [3]. 
Còn đối với khả năng truy cập: các giải 
năng truy cập mọi lúc, mọi nơi, thúc đẩy cộng 
tác và sự linh hoạt. 
động, phát triển sản phẩm mới, gia tăng lợi thế 
doanh đầy biến động (xem thống kê Bảng 1).
Bảng 1: Lợi ích của Big Data và Điện toán đám mây  
Tự động mở rộng/thu hẹp tài nguyên, tối ưu hóa chi phí. 
Chỉ trả tiền cho tài nguyên sử dụng. 
Truy cập mọi lúc, mọi nơi, thúc đẩy cộng tác. 
Tự động hóa quy trình, cải thiện ra quyết định, tối ưu hóa 
chuỗi cung ứng. 
Xác định xu hướng thị trường và nhu cầu khách hàng. 
Đưa ra quyết định sáng suốt và nhanh chóng dựa trên dữ liệu. 
Phân tích dữ liệu để dự đoán rủi ro và nắm bắt cơ hội mới. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 37 
2. Ứng dụng thực tiễn của Big Data  
giới thực. Các doanh nghiệp đang tận dụng 
động, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Dưới đây là một số ví dụ cụ 
thể:  
Trước tiên, đó là ở ngành bán lẻ: Các gã 
nền tảng đám mây để quản lý hàng tồn kho, 
thông tin chi tiết về khách hàng [4]. Ví dụ, câu 
chuyện về Amazon retail (Amazon.com). 
Ngày 10 tháng 11 năm 2010 là ngày 
Amazon.com tắt máy chủ web vật lý cuối 
cùng 
trong 
trung 
tâm 
dữ 
liệu 
của 
Amazon.com ([17]). Amazon.com là nhà bán 
lẻ trực tuyến lớn nhất thế giới. Do 
Amazon.com tạo ra rất nhiều dữ liệu, trước 
trữ dữ liệu đó. Nhưng khi Amazon.com phát 
triển lớn hơn, kích thước cơ sở dữ liệu Oracle 
cùng khó khăn. Điều này khiến họ phải cân 
AWS. Bằng cách chuyển sang AWS, họ đã 
trải nghiệm cải thiện hiệu suất gấp 12 lần và 
giảm thời gian khôi phục từ khoảng 15 giờ 
xuống 2,5 giờ ([18]). Amazon.com đã vượt 
qua chi phí cao, hiệu suất chậm và quản lý tốn 
AWS. Họ tận dụng Amazon S3 vì tính tiết 
kiệm chi phí, khả năng mở rộng, bảo mật và 
lưu trữ bền vững, giúp sao lưu và khôi phục 
nhanh hơn đáng kể. Ngoài ra, việc di chuyển 
mạch. Nhìn chung, việc chuyển sang AWS 
giúp giảm chi phí, cải thiện hiệu quả và cung 
phát triển của Amazon (Bảng 3, 4). 
Trong ngành chăm sóc sức khỏe: Ngành 
Data. Nghiên cứu của [5] cho thấy các tổ chức 
mây để phân tích dữ liệu bệnh nhân, từ đó cải 
sáng kiến nghiên cứu. Ví dụ, Mayo Clinic sử 
điều trị mới, chẩn đoán bệnh chính xác hơn và 
cải thiện hiệu quả chăm sóc.  
Và trong ngành dịch vụ tài chính: Phân 
chính. Các nghiên cứu điển hình của [6] cho 
để xác định các giao dịch gian lận, đánh giá 
rủi ro tín dụng và quản lý danh mục đầu tư. Ví 
dụ, JPMorgan Chase sử dụng Big Data để 
phát hiện các trường hợp rửa tiền, ngăn chặn 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 38 
đầu tư.  
vực. Các giải pháp Big Data dựa trên nền tảng 
doanh đầy biến động. Bảng 2 sau đây thống 
kê các ứng dụng:
Bảng 2: Ứng dụng thực tiễn của Big Data 
Quản lý hàng tồn kho, cá nhân hóa 
chiến dịch tiếp thị, thu thập thông tin 
khách hàng, đề xuất sản 
[4, 18] 
Phân tích dữ liệu bệnh nhân, cải thiện 
chất lượng chăm sóc, thúc đẩy nghiên 
cứu 
phương pháp điều trị mới, 
[5] 
Quản lý rủi ro, phát hiện gian lận, 
đánh giá rủi ro tín dụng, quản lý danh 
rửa tiền, ngăn chặn gian lận 
thẻ tín dụng, tối ưu hóa danh 
[6] 
Bảng 3: Bảng so sánh Lưu trữ truyền thống vs Lưu trữ đám mây Amazon S3 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Chi phí trả trước cao cho phần cứng băng, dung 
lượng trung tâm dữ liệu và giấy phép phần mềm. 
Mô hình trả tiền theo nhu cầu, loại 
bỏ chi phí trả trước. 
liệu ngày càng tăng. 
của Amazon. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 39 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Sao lưu và phục hồi chậm do thời gian đọc băng. 
kể so với băng. 
liệu, dễ bị lỗi phần cứng. 
11 số chín (99.999999999%). 
tầng băng. 
cầu quản lý tối thiểu. 
Bảng 4: Bảng so sánh Máy chủ cục bộ vs AWS EC2 đám mây 
Máy chủ On-premises 
trung tâm dữ liệu cục bộ. 
chuyên dụng để giao tiếp liền mạch. 
máy chủ web, cơ sở dữ liệu và 
các công cụ. 
AWS. 
3. Giải quyết thách thức và triển khai hiệu 
toán đám mây mang lại nhiều lợi ích, nó cũng 
thận. Bảo mật dữ liệu là một trong những 
mối quan tâm hàng đầu. Pearson (2013) [7] 
vệ dữ liệu nhạy cảm trên đám mây. Các biện 
pháp này bao gồm: mã hóa dữ liệu, kiểm soát 
quyền truy cập, và tuân thủ các quy định. 
truy cập trái phép. Kiểm soát quyền truy cập 
vào dữ liệu và mức độ truy cập của dữ liệu đó. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 40 
Đối với việc tuân thủ các quy định, như là, 
bảo mật dữ liệu, chẳng hạn như quy định về 
bảo vệ dữ liệu (GDPR) của Liên minh châu 
Âu. 
pháp Big Data trên nền tảng đám mây. 
Achanta (2023) [8] và Setiyawan & Patel 
(2019) [9] đã nêu bật tầm quan trọng của 
việc: chất lượng dữ liệu, và tuân thủ quy định. 
việc xem xét dữ liệu tính chính xác, đầy đủ 
và nhất quán để có thể phân tích hiệu quả. 
quản lý dữ liệu, chẳng hạn như Sarbanes-
Oxley Act (SOX) của Hoa Kỳ.  
Ngoài ra, còn có một số thách thức khác 
Big Data trên nền tảng đám mây, bao gồm: sự 
tương tác, kỹ năng nhân sự, và chi phí triển 
khai – vận hành. Về khả năng tương tác, thì 
doanh nghiệp. Đối với các kỹ năng, thì doanh 
đám mây. Còn lại, đối với quản lý chi phí, thì 
toán đám mây. 
thức và cân nhắc này, các doanh nghiệp có thể 
tranh (Bảng 3). 
Bảng 5: Giải quyết thách thức và triển khai hiệu quả Big Data dựa trên điện toán đám mây 
Mã hóa mạnh mẽ, kiểm soát quyền truy 
cập, tuân thủ quy định 
[7] 
Đảm bảo chất lượng dữ liệu, tuân thủ 
[8, 9] 
[10] 
[11, 12] 
[13, 14] 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 41 
4. Tương lai của việc ra quyết định dựa 
mây. Các xu hướng mới nổi như điện toán 
lý và sử dụng dữ liệu. 
Theo Banjanovic & Husaković (2023) 
[15], điện toán biên tích hợp phân tích Big 
liệu thời gian thực tại ranh giới của mạng. 
nguồn khác nhanh chóng và hiệu quả hơn. 
McGrath & Brenner (2017) [16] cho rằng 
liệu. Nhờ vậy, doanh nghiệp có thể thúc đẩy 
đổi mới và tăng trưởng nhanh hơn. 
Sự kết hợp của Big Data, điện toán đám 
tâm trong việc ra quyết định, đổi mới và tăng 
trưởng (Bảng 4). Doanh nghiệp cần nắm bắt 
đại dữ liệu. 
Bảng 6: Tương lai của việc ra quyết định dựa trên dữ liệu 
biên 
mạng 
liệu nhanh chóng, hiệu quả 
[15] 
chủ 
[16] 
năng lưu trữ, xử lý và phân tích dữ liệu mạnh 
mẽ. Nhờ đó, doanh nghiệp có thể nâng cao 
hiệu quả hoạt động, hiểu rõ hơn về khách 
hàng, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Việc nắm bắt sức mạnh của Big 
số. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 42 
mây hiệu quả, doanh nghiệp cần xác định rõ 
mục tiêu, lựa chọn giải pháp phù hợp, xây 
dựng chiến lược quản trị dữ liệu, đầu tư đào 
án nhỏ đến mở rộng dần. Việc áp dụng thành 
doanh nghiệp thành công trong kỷ nguyên số. 
hiệu quả, doanh nghiệp có thể khai thác sức 
đạt được những lợi ích to lớn. 
[1] Laney, D. (2001) 3D Data Management: 
Controlling Data Volume, Velocity and Variety. 
META Group Research Note, 6. 
[2] Armbrust, M., Griffith, R., Joseph, A. D., Katz, 
R., Konwinski, A., Lee, G., Patterson, D., et al. 
(2010). 
A 
view 
of 
computing. 
Communications of the ACM, 53(4), 50-58. 
ACM. 
[3] Dikaiakos, M., Katsaros, D., Mehra, P., Pallis, G., 
& Vakali, A. (2009). “Cloud computing: 
scientific research”. IEEE Internet Computing, 
13(5), 10-13. 
[4] Chen, W., Li, J., & Jin, X. J. (2016). The 
replenishment policy of agri-products with 
stochastic demand in integrated agricultural 
supply chains. Expert Systems with Applications, 
48, 55-66. 
[5] Halamka, J. (2014). The Argonaut Project 
Charter. Life as a Healthcare CIO. 
[6] Rizvi, S. (2021). Role of big data in financial 
institutions for financial fraud. SSRN Electronic 
Journal, 4, 35. 
[7] Pearson, S. (2013). Privacy, Security and Trust in 
Cloud Computing. In: Pearson, S., Yee, G. (eds) 
Privacy and Security for Cloud Computing. 
and 
Networks. 
Springer, London. https://doi.org/10.1007/978-1-
4471-4189-1_1  
[8]  Achanta, M. (2023). Data governance in the age of 
cloud computing: Strategies and considerations. 
(IJSR), 12, 1338-1343. 
[9]  Setiyawan, D., & Patel, C. (2019). A proposed 
and data management in higher education. SSRN 
Electronic Journal, 6, 19-25. 
[10] Agrawal, D. & Das, S. & Abbadi, A. (2011). Big 
Data and Cloud Computing: Current State and 
Opportunities. 
ACM 
Series. 
530-533. 
10.1145/1951365.1951432. 
[11] Ghaleb, E.A.A.; Dominic, P.D.D.; Fati, S.M.; 
Muneer, A.; Ali, R.F. 2021. The Assessment of Big 
Data Adoption Readiness with a Technology–
Organization–Environment 
Framework: 
A 
Employees. 
2021, 
13, 
8379. 
https://doi.org/10.3390/su13158379 
[12] Shamim, S., Zeng, J., Choksy, U.S. & Shariq, S. 
M. 2020. Connecting big data management 
employee level, International Business Review, 
Volume 29, Issue 6, 101604, ISSN 0969-5931, 
https://doi.org/10.1016/j.ibusrev.2019.101604. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 43 
[13] Muniswamaiah, M., Agerwala, T. & Tappert, C. 
(2019). Big data in cloud computing review and 
opportunities.  International Journal of Computer 
Science & Information Technology (IJCSIT) Vol 
11, 
No 
4. 
https://arxiv.org/ftp/arxiv/papers/1912/1912.108
21.pdf 
[14] El-Seoud, S. A., El-Sofany, H. F., Abdelfattah, 
M. A. F., & Mohamed, R. (2017). Big Data and 
Cloud Computing: Trends and Challenges. 
(iJIM), 
11(2), 
pp. 
34–52. 
https://doi.org/10.3991/ijim.v11i2.6561 
[15] Banjanovic, M. L., & Husaković, A. (2023). Edge 
AI: Reshaping the Future of Edge Computing 
with 
Intelligence. 
10.5644/PI2023.209.07. 
[16] McGrath, G., & Brenner, P. R. (2017). 
"Serverless 
Computing: 
Design, 
Implementation, and Performance," 2017 IEEE 
Computing Systems Workshops (ICDCSW), 
Atlanta, GA, USA, 2017, pp. 405-410, doi: 
10.1109/ICDCSW.2017.36. 
[17] [Amazon Web Services]. (2012, December 10). 
AWS re: Invent ENT 205- Drinking Our Own 
[Video]. 
Https://www.Youtube.com/User/AmazonWebSe
rvices/Cloud 
. 
https://www.youtube.com/watch?v=f45Uo5rw6
YY  
[18] Chavan, A. (2020, September 21). How Amazon 
retail (Amazon.Com) uses the AWS cloud. 
Medium. 
June 
7, 
2024, 
from 
https://ankush-chavan.medium.com/how-
amazon-retail-amazon-com-uses-the-aws-cloud-
View publication stats

Removed lines from sybca-bigdata-ppt.pdf:
Introduction to Big Data
What is Data?
The quantities, characters, or symbols on which operations are performed by a computer, 
magnetic, optical, or mechanical recording media.
What is Big Data?
Big Data is also data but with a huge size. Big Data is a term used to describe a 
collection of data that is huge in volume and yet growing exponentially with time. In 
tools are able to store it or process it efficiently.
“Extremely large data sets that may be analyzed computationally to reveal patterns , 
trends and association, especially relating to human behavior and interaction are 
known as Big Data.”

Following are some the examples of Big Data-
The New York Stock Exchange generates about one terabyte of new trade data per day.
Social Media
The statistic shows that 500+terabytes of new data get ingested into the databases of social 
media site Facebook, every day. This data is mainly generated in terms of photo and video 
uploads, message exchanges, putting comments etc.
A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many 
thousand flights per day, generation of data reaches up to many Petabytes.
Name
Size(In Bytes)
Bit
1/8
1/2 (rare)
Byte
1
1024 bytes
1024
1, 024kilobytes
1, 048, 576
1, 024 megabytes
1, 073, 741, 824
1, 024 gigabytes
1, 099, 511, 627, 776
1, 024 terrabytes
1, 125, 899, 906, 842, 624
1, 024 petabytes
1, 152, 921, 504, 606, 846, 976
1, 024 exabytes
1, 180, 591, 620, 717, 411, 303, 424
1, 024 zettabytes
1, 208, 925, 819, 614, 629, 174, 706, 176
Characteristics Of Big Data
•
The following are known as “Big Data Characteristics”.
1. Volume
2. Velocity
3. Variety
4. Veracity
1. Volume:
Volume means “How much Data is generated”. Now-a-days, 
very vast amount of Data say TB(Tera Bytes) to PB(Peta Bytes) to Exa
Byte(EB) and more.
2. Velocity:
Velocity means “How fast produce Data”. Now-a-days, Organizations or 
fast rate.
3. Variety:
Variety means “Different forms of Data”. Now-a-days, Organizations or 
rate in different formats. We will discuss in details about different formats of 
Data soon.
4. Veracity
Veracity means “The Quality or Correctness or Accuracy of Captured Data”. 
Out of 4Vs, it is most important V for any Big Data Solutions. Because without 
Correct Information or Data, there is no use of storing large amount of data at 
fast rate and different formats. That data should give correct business value.
Types of Digital Data
1. Structured
2. Unstructured
3. Semi-structured
Structured

Any data that can be stored, accessed and processed in the form of fixed format is 
termed as a 'structured' data. 

Over the period of time, talent in computer science has achieved greater success in 
developing techniques for working with such kind of data (where the format is well 
known in advance) and also deriving value out of it.

However, nowadays, we are foreseeing issues when a size of such data grows to a huge 
extent, typical sizes are being in the range of multiple zettabytes.
Do you know? 1021 bytes equal to 1 zettabyte or one billion terabytes forms a zettabyte.
given and imagine the challenges involved in its storage and processing.
Do you know? Data stored in a relational database management system is one 
example of a 'structured' data.
• Examples Of Structured Data
An 'Employee' table in a database is an example of Structured Data
2365
Male
650000
3398
650000
7465
Male
500000
7500
Male
500000
7699
550000
Unstructured

Any data with unknown form or the structure is classified as unstructured data.

In addition to the size being huge, un-structured data poses multiple challenges in terms 
of its processing for deriving value out of it.

combination of simple text files, images, videos etc. 

Now day organizations have wealth of data available with them but unfortunately, they 
don't know how to derive value out of it since this data is in its raw form or unstructured 
format.
• Examples Of Un-structured Data
The output returned by 'Google Search'
Semi-structured

Semi-structured data can contain both the forms of data. 

We can see semi-structured data as a structured in form but it is actually not defined 
with e.g. a table definition in relational DBMS.

Example of semi-structured data is a data represented in an XML file.
Examples Of Semi-structured Data
Personal data stored in an XML file-
<rec><name>Prashant Rao</name><sex>Male</sex><age>35</age></rec>
<rec><name>Seema R.</name><sex>Female</sex><age>41</age></rec>
<rec><name>Satish Mane</name><sex>Male</sex><age>29</age></rec>
<rec><name>Subrato Roy</name><sex>Male</sex><age>26</age></rec>
<rec><name>Jeremiah J.</name><sex>Male</sex><age>35</age></rec>
Big Data Analytics
Big Data Analytics: 

Big Data analytics is the process of collecting, organizing and analyzing 
large sets of data (called Big Data) to discover patterns and other useful 
information.

that is most important to the business and future business decisions. 
from analyzing the data.
High-Performance Analytics Required: 

To analyze such a large volume of data, Big Data analytics is typically 
performed using specialized software tools and applications for predictive 
analytics, data mining, text mining, forecasting and data optimization. 

high-performance analytics.

large volumes of data that a business has collected to determine which data is 
relevant and can be analyzed to drive better business decisions in the future.
The Challenges:

For most organizations, Big Data analysis is a challenge. Consider the sheer 
volume of data and the different formats of the  
data(both structured and unstructured data) that is collected across the entire 
combined, contrasted and analyzed to find patterns and other useful business 
information.

organization stores in different places and often in different systems. 

easily as structured data. 

This massive volume of data is typically so large that it's difficult to process 
using traditional database and software methods.
How Big Data Analytics is Used Today:

data improves, business can be transformed in all sorts of ways. 

Today's advances in analyzing big data allow researchers to decode human DNA in 
minutes, predict where terrorists plan to attack, determine which gene is mostly likely 
to be responsible for certain diseases and, of course, which ads you are most likely to 
respond to on Facebook.

Another example comes from one of the biggest mobile carriers in the world.

France's Orange launched its Data for Development project by releasing subscriber 
data for customers in the Ivory Coast.

The 2.5 billion records, which were made anonymous, included details on calls and 
text messages exchanged between 5 million users.

as the foundation for development projects to improve public health and safety.

cell phone data to map where people went after emergencies; another showed how to 
use cellular data for disease containment. (source)
The Benefits of Big Data Analytics:

data. Many big data projects originate from the need to answer specific 
business questions. With the right big data analytics platforms in place, an 
enterprise can boost sales, increase efficiency, and improve operations, 
customer service and risk management.

Webopedia parent company, QuinStreet, surveyed 540 enterprise decision-
companies plan to use Big Data analytics to improve operations. About half 
of all respondents said they were applying big data analytics to improve 
customer retention, help with product development and gain a competitive 
advantage.

Notably, the business area getting the most attention relates to increasing 
efficiency and optimizing operations. Specifically, 62 percent of respondents 
said that they use big data analytics to improve speed and reduce complexity.
Application of Big Data 
Here is the list of top Big Data applications in today’s world:
•
•
•
Big Data in E-commerce
•
•
•
•
•
Let’s discuss the applications of Big Data in detail.
1. Big Data in Retail

The retail industry is the one that faces the most fierce competition of all. Retailers 
constantly hunt for ways that will give them a competitive edge over others. 
Customers are the real king sounds legit for the retail industry in particular.

For retailers to thrive in this competitive world, they need to understand their 
customers in a better way. If they are aware of their customers’ needs and how to 
fulfill those needs in the best possible way, then they know everything.

– Big Data in Retail.

Through advanced analysis of their customer’s data, retailers are now able to 
understand them from every angle possible. They gather this data from various 
sources such as social media, loyalty programs, etc.

Even a minute detail about any customer has now become significant for them. They are 
now closer to their customers than they have ever been. This empowers them to provide 
customers with more personalized services and predict their demands in advance.

This helps them in building a loyal customer base. Some of the biggest names in the retail 
world like Walmart, Sears and Holdings, Costco, Walgreens, and many more now have Big 
Data as an integral part of their organizations.

are responsible for as much as 30% of retail annual sales.
2. Big Data in Healthcare

Big Data and healthcare are an ideal match. It complements the healthcare industry better 
than anything ever will. The amount of data the healthcare industry has to deal with is 
unimaginable.
Gone are the days when healthcare practitioners were incapable of harnessing this data. 
From finding a cure to cancer to detecting Ebola and much more, Big Data has got it all 
under its belt and researchers have seen some life-saving outcomes through it.

medications. Data analysts are harnessing this data to develop more and more effective 
treatments. Identifying unusual patterns of certain medicines to discover ways for 
developing more economical solutions is a common practice these days.

Explore how Big Data helps to speed up the treatment process – Big Data in 
Healthcare.

people of all age groups. This generates massive amounts of real-time data in the 
form of alerts which helps in saving the lives of the people.
3. Big Data in Education

When you ask people about the use of the data that an educational institute gathers, the 
need it for future references.

Even you had the same perception about this data, didn’t you? But the fact is, this data 
holds enormous importance. Big Data is the key to shaping the future of the people and 
has the power to transform the education system for better.

curriculum. Additionally, universities can even track the dropout rates of the students 
and are taking the required measures to reduce this rate as much as possible.
4. Big Data in E-commerce

One of the greatest revolutions this generation has seen is that of E-commerce. It is now part 
and parcel of our routine life. Whenever we need to buy something, the first thought that 
provokes our mind is E-commerce. And not your surprise, Big Data has been the face of it.

Some of the biggest E-commerce companies of the world like Amazon, Flipkart, Alibaba, and 
popularity Big Data has gained in recent times.

Big Data is now as important as anyone else in these organizations. Amazon, the biggest E-
commerce firm in the world and one of the pioneers of Big Data and analytics, has Big Data as 
the backbone of its system. Flipkart, the biggest E-commerce firm in India, has one of the most 
robust data platforms in the country.

See how Flipkart used Big Data to have one of the most robust data platforms.

Big Data’s recommendation engine is one of the most amazing applications the Big Data world 
has ever witnessed. It furnishes the companies with a 360-degree view of its customers.

Companies then suggest customers accordingly. Customers now experience more personalized 
services than they have ever had. Big Data has completely redefined people’s online shopping 
experiences.
5. Big Data in Media and Entertainment

sheer piece of art. Art and science are often considered to be the two completely 
contrasting domains but when employed together, they do make a deadly duo and Big 
Data’s endeavors in the media industry are a perfect example of it.

Viewers these days need content according to their choices only. Content that is 
relatively new to what they saw the previous time. Earlier the companies 
broadcasted the Ads randomly without any kind of analysis.

But after the advent of Big Data analytics in the industry, companies now are 
aware of the kind of Ads that attracts a customer and the most appropriate time to 
broadcast it for seeking maximum attention.

Customers are now the real heroes of the Media and entertainment industry -
courtesy to Big Data and Analytics.
6. Big Data in Finance

data is one of the toughest challenges any financial firm faces. Data has been the second most 
important commodity for them after money.

Even before Big Data gained popularity, the finance industry was already conquering the 
technical field. In addition to it, financial firms were among the earliest adopters of Big Data 
and Analytics.

has been at the heart of it. Big Data is bossing the key areas of financial firms such as fraud 
detection, risk analysis, algorithmic trading, and customer contentment.

This has brought much-needed fluency in their systems. They are now empowered to focus 
more on providing better services to their customers rather than focussing on security issues. 
Big Data has now enhanced the financial system with answers to its hardest of the challenges.
7. Big Data in Travel Industry

with it, the travel industry was a bit late to realize its worth. Better late than never though. 
Having a stress-free traveling experience is still like a daydream for many.

And now Big Data’s arrival is like a ray of hope, that will mark the departure of all the 
hindrances in our smooth traveling experience.
See how Big Data is revolutionizing the travel & tourism sector.

Through Big Data and analytics, travel companies are now able to offer more 
customized traveling experience. They are now able to understand their customer’s 
requirements in a much-enhanced way.

From providing them with the best offers to be able to make suggestions in real-time, 
Big Data is certainly a perfect guide for any traveler. Big Data is gradually taking the 
window seat in the travel industry.
8. Big Data in Telecom

The telecom industry is the soul of every digital revolution that takes place around the world. 
With the ever-increasing popularity of smartphones, it has flooded the telecom industry with 
massive amounts of data.

And this data is like a goldmine, telecom companies just need to know how to dig it properly. 
Through Big Data and analytics, companies are able to provide the customers with smooth 
connectivity, thus eradicating all the network barriers that the customers have to deal with.
Companies now with the help of Big Data and analytics can track the areas with the lowest as 
well as the highest network traffics and thus doing the needful to ensure hassle-free network 
connectivity.
Big Data alike other industries have helped the telecom industry to understand its customers 
pretty well. 
Telecom industries now provide customers with offers as customized as possible.

Big Data has been behind the data revolution we are currently experiencing.
9. Big Data in Automobile

“A business like an automobile, has to be driven, in order to get results.” B.C. Forbes

smoothly. Big Data is driving the automobile industry towards some unbelievable and never 
before results.

wings to it. Big Data has helped the automobile industry achieve things that were beyond our 

From analyzing the trends to understanding the supply chain management, from taking care 
of its customers to turning our wildest dream of connected cars a reality, Big Data is well 
and truly driving the automobile industry crazy.

Removed lines from TNG_QUAN_V_D_LIU_LN_BIGDATA.pdf:
1 
TỔNG QUAN Vӄ DӲ LIӊU LӞN (BIGDATA) 
Ks. Nguyễn Công Hoan 
Trung Tâm Thông tin Khoa học thống kê (Viện KHTK) 
Trước đây, chúng ta mới chỉ biết đến dữ liệu có cấu trúc (structure data), ngày 
nay, với sự kết hợp của dữ liệu và internet, đã xuất hiện một dạng khác của dữ liệu -  Big 
data (dịch là “dữ liệu lớn”). Dữ liệu này có thể từ các nguồn như: hồ sơ hành chính,giao 
dịch điện tử, dòng trạng thái (status), chia sẻ hình ảnh, bình luận, nhắn tin...của chính 
chúng ta, nói cách khác chúng là dữ liệu được sản sinh qua quá trình chia sẻ thông tin 
trực tuyến liên tục của người sử dụng. Để cung cấp cái nhìn tổng quan, chúng tôi xin giới 
liệu lớn mang lại.    
1.  Khái niӋm, đặc trưng của dӳ liӋu lӟn và sự khác biӋt vӟi dӳ liӋu truyӅn thống 
1.1. Khái niệm về dữ liệu lớn 
- Theo wikipedia: Big data là một thuật ngữ chỉ bộ dữ liệu lớn hoặc phức tạp mà các 
phương pháp truyền thống không đӫ các ứng dөng để xử lỦ dữ liệu này. 
-  Theo Gartner: Dữ liệu lớn là những nguồn thông tin có đặc điểm chung khối lượng lớn,  
tốc độ nhanh và dữ liệu định dạng dưới nhiều hình thức khác nhau, do đó muốn khai thác 
được  đòi hỏi phải có hình thức xử lỦ mới để đưa ra quyết định, khám phá và tối ưu hóa 
quy trình.  
1.2. Nguồn hình thành dữ liệu và phương pháp khai thác và quản lý dữ liệu lớn. 
Qua thống kê và tổng hợp, nguồn dữ liệu lớn được hình thành chӫ yếu từ 6 nguồn: 
(1) Dữ liệu hành chính (phát sinh từ chương trình cӫa một tổ chức, có thể là chính phӫ 
hay phi chính phӫ). Ví dө, hồ sơ y tế điện tử ở bệnh viện, hồ sơ bảo hiểm, hồ sơ ngân 
hàng...; (2) Dữ liệu từ hoạt động thương mại (phát sinh từ các giao dịch giữa hai thực 
thể). Ví dө, các giao dịch thẻ tín dөng, giao dịch trên mạng, bao gồm cả từ các thiết bị di 
động; (3) Dữ liệu từ các thiết bị cảm biến như thiết bị chөp hình ảnh vệ tinh, cảm biến 
đường, cảm biến khí hậu; (4) Dữ liệu từ các thiết bị theo dõi, ví dө theo dõi dữ liệu từ 
điện thoại di động, GPS; (5) Dữ liệu từ các hành vi, ví dө như tìm kiếm trực tuyến về 
(một sản phẩm, một dịch vө hay thông tin khác), đọc các trang mạng trực tuyến...; (6) Dữ 
liệu từ các thông tin về  Ủ kiến, quan điểm cӫa các cá nhân, tổ chức, trên các phương tiện 
thông tin xã hội. 
theo các nguồn hình thành dữ liệu lớn. Mỗi nguồn dữ liệu lớn khác nhau sẽ có phương 
pháp khai thác và quản lỦ dữ liệu lớn khác nhau. Tuy nhiên, hiện nay phần lớn các tổ 
dữ liệu lớn. 
1.3. Đặc trưng 5V cͯa dữ liệu lớn 
Dữ liệu lớn có 5 đặc trưng cơ bản như sau (mô hình 5V):   
(1) Khối lượng dữ liệu (Volume)  
2 
Đây là đặc điểm tiêu biểu nhất cӫa dữ liệu lớn, khối lượng dữ liệu rất lớn. Kích cỡ 
cӫa Big Data đang từng ngày tăng lên, và tính đến năm 2012 thì nó có thể nằm trong 
khoảng vài chөc terabyte cho đến nhiều petabyte (1 petabyte = 1024 terabyte) chỉ cho 
một tập hợp dữ liệu. Dữ liệu truyền thống có thể lưu trữ trên các thiết bị đĩa mềm, đĩa 
cứng. Nhưng với dữ liệu lớn chúng ta sẽ sử dөng công nghệ “đám mây” mới đáp ứng khả 
năng lưu trữ được dữ liệu lớn. 
 (2) Tốc độ (Velocity) 
Tốc độ có thể hiểu theo 2 khía cạnh: (a) Khối lượng dữ liệu gia tăng rất nhanh (mỗi 
giây có tới 72.9 triệu các yêu cầu truy cập tìm kiếm trên web bán hàng cӫa Amazon); (b) 
Xử lỦ dữ liệu nhanh ở mức thời gian thực (real-time), có nghĩa dữ liệu được xử lỦ ngay 
tức thời ngay sau khi chúng phát sinh (tính đến bằng mili giây). Các ứng dөng phổ biến 
trên lĩnh vực Internet, Tài chính, Ngân hàng, Hàng không, Quân sự, Y tế – Sức khỏe như 
hiện nay phần lớn dữ liệu lớn được xử lỦ real-time. Công nghệ xử lỦ dữ liệu lớn ngày nay 
đã cho phép chúng ta xử lỦ tức thì trước khi chúng được lưu trữ vào cơ sở dữ liệu. 
(3) Đa dạng (Variety) 
Đối với dữ liệu truyền thống chúng ta hay nói đến dữ liệu có cấu trúc, thì ngày nay 
hơn 80% dữ liệu được sinh ra là phi cấu trúc (tài liệu, blog, hình ảnh, vi deo, bài hát, dữ 
liệu từ thiết bị cảm biến vật lỦ, thiết bị chăm sóc sức khỏe...). Big Data cho phép liên kết 
và phân tích nhiều dạng dữ liệu khác nhau. Ví dө, với các bình luận cӫa một nhóm người 
dùng nào đó trên Facebook với thông tin video được chia sẻ từ Youtube và Twitter. 
(4) Độ tin cậy/chính xác (Veracity) 
Một trong những tính chất phức tạp nhất cӫa Dữ liệu lớn là độ tin cậy/chính xác cӫa 
dữ liệu. Với xu hướng phương tiện truyền thông xã hội (Social Media) và mạng xã hội 
(Social Network) ngày nay và sự gia tăng mạnh mẽ tính tương tác và chia sẻ cӫa người 
dùng Mobile làm cho bức tranh xác định về độ tin cậy & chính xác cӫa dữ liệu ngày một 
khó khăn hơn. Bài toán phân tích và loại bỏ dữ liệu thiếu chính xác và nhiễu đang là tính 
chất quan trọng cӫa BigData. 
 (5) Giá trị (Value) 
Giá trị là đặc điểm quan trọng nhất cӫa dữ liệu lớn, vì khi bắt đầu triển khai xây 
thông tin mang lại như thế nào, khi đó chúng ta mới có quyết định có nên triển khai dữ 
liệu lớn hay không. Nếu chúng ta có dữ liệu lớn mà chỉ nhận được 1% lợi ích từ nó, thì 
không nên đầu tư phát triển dữ liệu lớn. Kết quả dự báo chính xác thể hiện rõ nét nhất về 
giá trị cӫa dữ liệu lớn mang lại. Ví dө, từ khối dữ liệu phát sinh trong quá trình khám, 
chữa bệnh sẽ giúp dự báo về sức khỏe được chính xác hơn, sẽ giảm được chi phí điều trị 
và các chi phí liên quan đến y tế.  
1.4. Sự khác biệt giữa dữ liệu lớn với dữ liệu truyền thống  
3 
Dữ liệu lớn khác với dữ liệu truyền thống (ví dө, kho dữ liệu - Data Warehouse) ở 4 
điểm cơ bản: Dữ liệu đa dạng hơn; lưu trữ dữ liệu lớn hơn; truy vấn nhanh hơn; độ chính 
xác cao hơn. 
(1) Dữ liệu đa dạng hơn: Khi khai thác dữ liệu truyền thống (Dữ liệu có cấu trúc), 
chúng ta thường phải trả lời các câu hỏi: Dữ liệu lấy ra kiểu gì? định dạng dữ liệu như thế 
nào? Đối với dữ liệu lớn, không phải trả lời các câu hỏi trên. Hay nói khác, khi khai thác, 
chúng; điều quan tâm là giá trị mà dữ liệu mang lại có đáp ứng được cho công việc hiện 
tại và tương lai hay không. 
(2) Lưu trữ dữ liệu lớn hơn: Lưu trữ dữ liệu truyền thống vô cùng phức tạp và luôn 
đặt ra câu hỏi lưu như thế nào? dung lượng kho lưu trữ bao nhiêu là đӫ? gắn kèm với câu 
hỏi đó là chi phí đầu tư tương ứng. Công nghệ lưu trữ  dữ liệu lớn hiện nay đã phần nào 
có thể giải quyết được vấn đề trên nhờ những công nghệ lưu trữ đám mây, phân phối lưu 
xác và xử lỦ nhanh trong thời gian thực. 
(3) Truy vấn dữ liệu nhanh hơn: Dữ liệu lớn được cập nhật liên tөc, trong khi đó 
tin đáp ứng theo yêu cầu. 
(4) Độ chính xác cao hơn: Dữ liệu lớn khi đưa vào sử dөng thường được kiểm định 
lại dữ liệu với những điều kiện chặt chẽ, số lượng thông tin được kiểm tra thông thường 
rất lớn, và đảm bảo về nguồn lấy dữ liệu không có sự tác động cӫa con người vào thay 
đổi số liệu thu thập. 
2. Bͱc tranh tổng thể ͱng dụng dữ liệu lớn  
 Dữ liệu lớn đã được ứng dөng trong nhiều lĩnh vực như: hoạt động chính trị; giao 
thông; y tế; thể thao; tài chính; thương mại; thống kê... dưới đây là một số ví dө về ứng 
dөng dữ liệu lớn. 
2.1. Ͱng dụng dữ liệu lớn trong hoạt động chính trị 
cӫa mình. Ông xây dựng một đội ngũ nhân viên chuyên đi 
triển khai về dữ liệu lớn. Đội ngũ nhân viên này thu thập tất 
cả thông tin về người dân ở các khu vực, sau đó phân tích và 
chỉ ra một số thông tin quan trọng về người dân Mỹ như: 
Thích đọc sách gì, thích mua loại thuốc gì, thích sử dөng phương tiện gì... Thậm chí còn 
biết được cả thông tin về mẹ cӫa cử tri đó đã bỏ phiếu tín nhiệm ai ở lần bầu cử trước. 
Trên cơ sở những thông tin này, Tổng thống Obama đã đưa ra kế hoạch vận động phù 
hợp, giúp ông tái đắc cử Tổng thống nước Mỹ lần thứ 2. 
4 
dөng như: Hệ thống chính phӫ điện tử; phân tích quy định và việc tuân thӫ quy định; 
phân tích, giám sát, theo dõi và phát hiện gian lận, mối đe dọa, an ninh mạng. 
2.2. Ͱng dụng dữ liệu lớn trong giao thông 
dòng giao thông trong thành phố vào các giờ cao điểm, từ đó có 
những kế hoạch phân luồng giao thông chi tiết, hợp lỦ giúp giảm 
thiểu kẹt xe. Ngoài ra còn đưa ra thông tin cho người tham gia 
đi vào giờ nào để tránh kẹt xe, hoặc đi đường nào là ngắn nhất.v.v. Ngoài ra dữ liệu lớn 
còn giúp phân tích định vị người dùng thiết bị di động, ghi nhận chi tiết cuộc gọi trong 
thời gian thực; và giảm thiểu tình trạng ùn tắc giao thông. 
2.3. Ͱng dụng dữ liệu lớn trong y tế 
để đưa ra dự đoán về nguy cơ mắc bệnh. Đồng thời cũng đưa ra 
được xu hướng lây lan cӫa bệnh. Ví dө, ứng dөng Google Flu 
dөng này dựa trên từ khóa tìm kiếm ở một khu vực nào đó, sau đó 
kiếm đó, sau cùng là đưa ra dự báo về xu hướng dịch cúm tại khu 
vực đó. Qua đó cho biết tình hình cúm tại khu vực đó sẽ diễn ra như thế nào để đưa ra các 
giải pháp phòng tránh. Những kết quả mà Google Flu Trend đưa ra, hoàn toàn phù hợp 
với báo cáo cӫa Tổ chức y tế thế giới WHO về tình hình bệnh cúm tại các khu vực đó. 
2.4. Ͱng dụng dữ liệu lớn trong thể thao 
cӫa đội tuyển Đức (hình bên) đã đưa ra những điểm bất hợp lỦ 
trong cấu trúc cӫa đội tuyển Đức, từ đó giúp cho đội tuyển Đức 
khắc phөc được điểm yếu và đã dành được World cup 2014. 
2.5. Ͱng dụng dữ liệu lớn trong tài chính 
Từ những dữ liệu chính xác, kịp thời thu thập được thông qua các giao dịch cӫa 
khách hàng, tiến hành phân tích, xếp hạng và quản lỦ các rӫi ro trong đầu tư tài chính, tín 
dөng. 
2.6. Ͱng dụng dữ liệu lớn trong thương mại 
sau: Phân khúc thị trường và khách hàng; phân tích hành vi khách hàng tại cửa hàng; tiếp 
thị trên nền tảng định vị; phân tích tiếp thị chéo kênh, tiếp thị đa kênh; quản lỦ các chiến 
dịch tiếp thị và khách hàng thân thiết; So sánh giá; Phân tích và quản lỦ chuỗi cung ứng; 
Phân tích hành vi, thói quen người tiêu dùng. 
2.7. Ͱng dụng dữ liệu lớn trong thống kê 
5 
thức, Ӫy ban Thống kê Liên hợp quốc cũng như các tổ chức thống kê khu vực và cơ quan 
thống kê quốc gia cӫa nhiều nước đã triển khai hàng loạt các hoạt động về Bigdata như: 
Hàn Quốc sử dөng ảnh vệ tinh để thống kê nông nghiệp và một số lĩnhvực khác;Australia 
sử dөng ảnh vệ tinh để thống kê diện tích đất nông nghiệp và năng suất; Italia sử dөng dữ 
liệu điện thoại di động để thống kê di cư; Bhutan dùng thiết bị di động để tính toán chỉ số 
giá tiêu dùng; Estonia dùng điện thoại di động định vị vệ tinh để thống kê du lịch; 
3. Nhӳng cơ hội và thách thͱc khi ͱng dụng Big data trong thống kê chính thͱc 
3.1 Cơ hội  
(1) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp cho chúng ta có thêm phương án 
giải quyết, xử lỦ và đối phó với những thách thức đối sản xuất số liệu thống kê chính thức 
trong hiện tại và tương lai. Những nghiên cứu thực nghiệm cần phải được tiến hành để 
khám phá những ứng dөng tiềm năng cӫa dữ liệu lớn trong số liệu thống kê chính thức, 
và nghiên cứu thực nghiệm đó phải là một phần trong quy trình sản xuất số liệu thống kê. 
(2) Nghiên cứu về dữ liệu lớn cần phải có cơ sở hạ tầng công nghệ thông tin hiện 
đại, đáp ứng các yêu cầu xử lỦ khối lượng lớn dữ liệu và nhanh, đồng thời có thể tập hợp 
dữ liệu từ nhiều nguồn khác nhau. Thực hiện được điều này chúng ta có được đội ngũ 
qua kinh nghiệm thực tế. 
(3) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp chúng ta có được những văn bản 
được khai thác dữ liệu thông qua hồ sơ hành chính, ngoài ra dữ liệu cũng được bảo đảm 
và giữ bí mật nhờ những văn bản pháp lỦ bổ sung này. 
(4) Sử dөng dữ liệu lớn đem lại niềm tin cӫa cộng đồng với thống kê chính thức do 
tác động chӫ Ủ cӫa con người. 
3.2 Thách thͱc  
(1)Tài chính 
Nhiều đơn vị, tổ chức không đo lường được vấn đề sẽ phát sinh trong quá trình triển 
khai thực hiện, dự toán kinh phí chưa chính xác, do vậy dự án không thực hiện được. Để 
triển khai được thành công, yếu tố tài chính có Ủ nghĩa rất quan trọng, một số tập đoàn 
Big data như IBM, website bán hàng thương mại điện tử Amazon ... 
(2) Chính sách, quy định Luật pháp về truy cập và sử dụng dữ liệu 
Việc sử dөng và khai thác dữ liệu lớn phө thuộc vào luật quy định cӫa mỗi quốc gia. 
1 Xem Báo cáo “Thống kê chính thức với Big data: Kinh nghiệm quốc tế và định hướng của Thống kê Việt Nam. 
6 
  Ví dө: ở Canada người dùng có thể được tiếp cận dữ liệu từ cả hai tổ chức chính 
phӫ và phi chính phӫ, nhưng ở những nước khác như Ireland thì phải được sự cho phép 
từ các cơ quan chính phӫ. Điều này có thể dẫn đến những hạn chế để truy cập vào một số 
loại dữ liệu lớn. 
 (3) Trình độ khai thác và quản lý dữ liệu  
quản lỦ là cũng khác nhau tuy nhiên, Một vấn đề liên quan đến quản lỦ thông tin hiện nay 
là nguồn nhân lực. Khoa học dữ liệu lớn đang phát triển mạnh trong những tổ chức tư 
nhân, trong khi đó bộ phận này chưa được liên kết với những tổ chức cӫa chính phӫ một 
cách chặt chẽ dẫn đến việc quản lỦ vẫn còn nhiều vướng mắc.. 
(4) Hạ tầng Công nghệ thông tin 
sử dөng giao diện ứng dөng cӫa Chương trình chuyên sâu tiêu chuẩn (API) để truy cập 
dữ liệu. Bằng cách này, nó có thể kết nối các ứng dөng cho dữ liệu thu về và xử lỦ dữ liệu 
trực tiếp với dữ liệu hành chính. Ngoài ra hệ thống khai thác dữ liệu lớn cũng cần phải 
được tính toán để có thể kết nối vào được kho cơ sở dữ liệu truyền thống, đó cũng là một 
trong những thách thức lớn cần được giải quyết.  
data, những lợi ích mà Big data mang lại cho chúng ta. Bên cạnh đó cũng chỉ ra những 
thách thức khi triển khai áp dөng khai thác Big data. 
cung cấpthông tin để chung ta xử lỦ được tình huống nhanh nhất, chính xác nhất và giá trị 
cӫa Big data mang lại luôn có tính định hướng đến tương  lai ? giải đáp những câu hỏi tại 
sao việc ấy lại xảy ra?;  Sau chuyện đó thì điều gì sẽ sảy ra? và chúng ta nên ứng phó như 
thế nào trong hoàn cảnh đó? 
1. Tài liệu cơ hội và thách thức với bigdata –E cӫa Liên Hợp Quốc: 
http://unstats.un.org/unsd/statcom/doc14/2014-11-BigData-E.pdf 
2. Báo cáo Hội thảo về tương lai cӫa Thống kê học London: 
https://statistics.stanford.edu/statistics-and-science-london-workshop-report 
3. Tài liệu về các khái niệm và đặc trưng cӫa Big data: 
https://viblo.asia/dovv/posts/3OEqGjWwv9bL 

Removed lines from what-is-big-data-ebook-4421383.pdf:
What is Big Data? 
04 
  08 
Big Data Use Cases                                                                      10 
                                                             13 
15 
18 
4 
5 
What exactly is big data? 
To put it simply: big data is larger, more 
complex data sets, especially from new data 
sources. These data sets are so voluminous that 
traditional data processing software just can’t 
manage them. But these massive volumes of 
you wouldn’t have been able to tackle before. 
To really understand big data, it’s helpful to have 
some historical background. Here’s Gartner’s 
defnition, circa 2001(which is still the go-to 
defnition): 
“Big data is data that contains greater variety 
arriving in increasing volumes and with ever 
higher velocity. This is known as the three Vs.” 
• Volume.The amount of data matters. With 
big data, you’ll have to process high volumes 
of low-density, unstructured data. This can be 
data of unknown value, such as Twitter data 
feeds, clickstreams on a webpage or a mobile 
app, or sensor-enabled equipment. For some 
organizations, this might be tens of 
terabytes of data. For others, it may be 
hundreds of petabytes. 
Velocity. Velocity is the fast rate at which data 
is received and (perhaps) acted on. Normally, 
memory versus being written to disk.  Some 
internet-enabled smart products operate in real 
real-time evaluation and action. 
• Variety. In today’s big data world, data 
comes in new unstructured data types. 
Unstructured and semi-structured data types, 
such as text, audio, and video require addition 
support metadata. 
6 
Volume
1 
2 
3 
THE VALUE—AND TRUTH—OF 
Since 2001, two more Vs have become apparent: 
value and veracity. Data has intrinsic value. But 
it’s of no use until that value is discovered. 
Equally important: How truthful is your data—and 
how much can you rely on it? 
Today, big data has become capital. Think of 
some of the world’s biggest tech companies. A 
their data, which they’re constantly analyzing to 
new products. 
and compute, making it easier and less expensive 
to store more data than ever before. With an 
increased volume of big data now cheaper and 
more accessible, you can make more accurate 
and precise business decisions. 
Finding value in big data isn’t only about 
analyzing it (which is a whole other beneft). 
It’s an entire discovery process that requires 
insightful analysts, business users, and 
executives who ask the right questions, recognize 
patterns, make informed assumptions, and 
predict behavior. 
But how did we get here? 
7 
8 
Around 2005, people began to realize just how 
much data users generated through Facebook, 
YouTube, and other online services. Hadoop (an 
open-source framework created specifcally to 
store and analyze big data sets) was developed 
that same year. NoSQL also began to gain 
popularity during this time. 
The development of open-source frameworks, 
such as Hadoop (and more recently, Spark) was 
store. In the years since then, the volume of big 
data has skyrocketed. Users are still generating 
huge amounts of data—but it’s not just humans. 
With the advent of Internet of Things (IoT) , more 
objects and devices are connected to the internet, 
product performance. The emergence of machine 
learning has produced still more data. 
While big data has come far, its popularity is only 
just beginning. Cloud computing has expanded 
big data possibilities even further. 
The cloud offers a truly elastic scalability, where 
test around a subset of data. It’s an exciting time 
to see what’s going to happen next. 
THE VALUE OF BIG DATA COMES IS TWOFOLD: 
1. Big data makes it possible for you to gain 
2. More complete answers means more 
confdence in the data–which means 
a completely different approach to 
9 
10 
cases that you haven’t been able to fully delve 
into before. Here are just a few.  (More use cases 
are on our solutions page): 
Companies like Netfix and Procter & Gamble 
use big data to anticipate customer demand. 
products or services, and then modeling the 
commercial success of the offerings, they build 
predictive models for new products and services. 
In addition, P&G uses data and analytics from 
focus groups, social media, test markets, and 
early store rollouts to plan, produce, and launch 
new products. 
be deeply buried in structured data, such as the 
equipment year, make, and model, as well as 
entries, senor data, error messages, and engine 
temperature., By analyzing these indications of 
potential issues before the problems happen, 
equipment uptime. 
The race for customers is on. A clearer view of 
ever before. Big data enables you to gather data 
from social media, web visits, call logs, and 
experience and maximize the value delivered. 
Start delivering personalized offers, reduce 
customer churn, and handle issues proactively. 
When it comes to security, it’s not just a few 
rogue hackers; you’re up against entire expert 
teams. Security landscapes and compliance 
requirements are constantly evolving. Big 
indicate fraud and aggregate large volumes of 
much faster. 
now. And data—specifcally big data—is one of 
the reasons why. It’s only recently that we’ve 
them. And the availability of big data to train 
machine-learning models makes that happen. 
11 
news, but it’s an area in which big data is having 
the most impact. With big data, you can analyze 
and assess production, customer feedback and 
returns, and other factors to reduce outages and 
anticipate future demands. Big data can also be 
used to improve decision-making in line with 
current market demand. 
interdependencies between humans, institutions, 
entities, and process and then determining new 
ways to use those insights. Use data insights to 
considerations. Examine trends and what 
services. Implement dynamic pricing. There are 
endless possibilities. 
While big data holds a lot of promise, it is not 
without its challenges.  
First, big data is... big. Although new 
technologies have been developed to store data, 
data volumes are doubling in size around every 
two years. Organizations still struggle to keep 
store it. 
But it’s not enough to just store the data. Data 
must be used to be valuable, and that depends 
on curation. Clean data, or data that’s relevant 
meaningful analysis requires a lot of work. 
Data scientists spend 50 to 80 percent of their 
actually be used. 
Finally, big data technology is changing at a fast 
pace. A few years ago, Apache Hadoop was the 
popular technology used to handle big data. That 
is, until Apache Spark was introduced in 2014. 
Today, a combination of the two frameworks 
appears to be the best approach. Keeping up with 
big data technology is an ongoing challenge. 
12 
13 
Oracle Cloud for  Big  Data  Analytics 
Data 
Enterprise Apps 
Data 
new opportunities and business models. Getting 
started involves three key actions:   
disparate sources and applications. Traditional 
data integration mechanisms, such as ETL 
(extract, transform, and load) generally aren’t 
up to the task. It requires new strategies and 
technologies to analyze big data sets at terabyte, 
or even petabyte, scale. At the same time, big 
data has the same requirements for quality, 
governance, and confdence as traditional data 
sources. During integration, you need to bring in 
the data, process it, and make sure it’s formatted 
analysts can get started with. 
Big data requires storage. Your storage solution 
can be in the cloud, on-premises, or both. 
on an on-demand basis. Many people choose 
data is currently residing. The cloud is gradually 
gaining popularity because it supports your 
to spin up resources as needed. 
analyze and act on your data. Get new clarity with 
a visual analysis of your varied data sets. Explore 
the data further to make new discoveries. Share 
your fndings with others. Build data models with 
machine learning and artifcial intelligence. Put 
your data to work. 
To help you on your big data journey, we’ve put 
in mind. Here are our guidelines for building a 
successful big data foundation. 
14 
15 
#1: ALIGN BIG DATA WITH 
new discoveries. To that end, it is important to 
base new investments in skills, organization, 
or infrastructure with a strong business-
investments and funding. To determine if 
you are on the right track, ask how big data 
supports and enables your top business and IT 
priorities. Examples include understanding how 
behavior, deriving sentiment from social 
media and customer support interactions, and 
and their relevance for customer, product, 
manufacturing, and engineering data. 
#2: EASE SKILLS SHORTAGE 
shortage. You can mitigate this risk by ensuring 
that big data technologies, considerations, 
governance program. 
Standardizing your approach will allow you 
to manage costs and leverage resources. 
proactively identify any potential skill gaps. These 
can be addressed by training/cross-training 
existing resources, hiring new resources, and 
leveraging consulting frms. 
#3: OPTIMIZE KNOWLEDGE 
Use a Center of Excellence approach to share 
knowledge, control oversight, and manage 
project communications. Whether big data is a 
new or an expanding investment, the soft and 
hard costs can be shared across the enterprise. 
Leveraging this approach can help increase 
systematic way. 
#4:TOP PAYOFF IS ALIGNING 
own.But you can bring even greater business 
already using today. 
16 
Whether you are capturing customer, product, 
equipment, or environmental big data, the goal 
core master and analytical summaries, leading 
to better conclusions. For example, there is 
sentiment from that of only your best customers. 
capabilities, data warehousing platform, and 
information architecture. 
processes and models can be both human- and 
machine-based. Big data analytical capabilities 
include statistics, spatial analysis, semantics, 
interactive discovery, and visualization. Using 
analytical models, you can correlate different 
and meaningful discoveries. 
#5: PLAN YOUR DISCOVERY LAB 
straightforward. Sometimes we don’t even 
know what we’re looking for. That’s expected. 
Management and IT needs to support this “lack 
of direction” or “lack of clear requirement.” 
At the same time, it’s important for analysts and 
requirements. To accommodate the interactive 
statistical algorithms, you need high-performance 
work areas. Be sure that sandbox environments 
have the power they need—and are 
properly governed. 
#6: ALIGN WITH THE CLOUD 
jobs. A big data solution includes all data 
realms including transactions, master data, 
reference data, and summarized data. Analytical 
sandboxes should be created on demand. 
control of the entire data fow including pre- 
and post-processing, integration, in-database 
summarization, and analytical modeling. A well-
supporting these changing requirements. 
17 
18 
Clearly, big data has tremendous potential. 
customers, make more accurate decisions, and 
create new growth opportunities. Contact us to 
learn more. 
See how Oracle can help your big data journey. 
Start your free trial today. 
Contact us URL: 
https://www.oracle.com/marketingcloud/contact­
sales.html 
Free Trial URL: 
https://go.oracle.com/LP=50758/? 
19 
Oracle Corporation 
Copyright © 2019, Oracle and/or its affiliates. All rights reserved. This document is provided for information purposes only, and the contents hereof are subject 
to change without notice. This document is not warranted to be error-free, nor subject to any other warranties or conditions, whether expressed orally or 
implied in law, including implied warranties and conditions of merchantability or fitness for a particular purpose. We specifically disclaim any liability with 
500 Oracle Parkway 
respect to this document, and no contractual obligations are formed either directly or indirectly by this document. This document may not be reproduced or 
transmitted in any form or by any means, electronic or mechanical, for any purpose, without our prior written permission. 
CA 94065 
Oracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners. 
USA 
Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or 
registered trademarks of SPARC International, Inc. AMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks or registered trademarks of 
Advanced Micro Devices. UNIX is a registered trademark of The Open Group. 
Phone: +1.650.506.7000 
+1.800.ORACLE1 
Fax: 
+1.650.506.7200 
oracle.com 

Removed lines from 2_iis_2015_81-90.pdf:
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
81 
BIG DATA ANALYTICS 
Jasmine Zakir, Minot State University,	  jasminezakir@outlook.com 
Tom Seymour, Minot State University,	  tom.seymour@minotstateu.edu 
Kristi Berg, Minot State University,	  kristi.berg@minostatateu.edu 
ABSTRACT 
Today Big Data draws a lot of attention in the IT world. The rapid rise of the Internet and the digital economy has 
fuelled an exponential growth in demand for data storage and analytics, and IT department are facing tremendous 
challenge in protecting and analyzing these increased volumes of information. The reason organizations are 
collecting and storing more data than ever before is because their business depends on it. The type of information 
being created is no more traditional database-driven data referred to as structured data rather it is data that 
include documents, images, audio, video, and social media contents known as unstructured data or Big Data. Big 
Data Analytics is a way of extracting value from these huge volumes of information, and it drives new market 
opportunities and maximizes customer retention. This paper primarily focuses on discussing the various 
technologies that work together as a Big Data Analytics system that can help predict future volumes, gain insights, 
take proactive actions, and give way to better strategic decision-making. Further this paper analyzes the adoption, 
usage and impact of big data analytics to the business value of an enterprise to improve its competitive advantage 
using a set of data algorithms for large data sets such as Hadoop and MapReduce.  
Keywords: Big Data, Analytics, Hadoop, MapReduce 
Big Data is an important concept, which is applied to data, which does not conform to the normal structure of the 
traditional database. Big Data consists of different types of key technologies like Hadoop, HDFS, NoSQL, 
MapReduce, MongoDB, Cassandra, PIG, HIVE, and HBASE that work together to achieve the end goal like 
extracting value from data that would be previously considered dead. According to a recent market report published 
by Transparency Market Research, the total value of big data was estimated at $6.3 billion as of 2012, but by 2018, 
it’s expected to reach the staggering level of $48.3 billion that’s almost a 700 percent increase [29]. Forrester 
Research estimates that organizations effectively utilize less than 5 percent of their available data. This is because 
the rest is simply too expensive to deal with. Big Data is derived from multiple sources. It involves not just 
traditional relational data, but all paradigms of unstructured data sources that are growing at a significant rate. For 
instance, machine-derived data multiplies quickly and contains rich, diverse content that needs to be discovered. 
Another example, human-derived data from social media is more textual, but the valuable insights are often 
overloaded with many possible meanings.  
Big Data Analytics reflect the challenges of data that are too vast, too unstructured, and too fast moving to be 
managed by traditional methods. From businesses and research institutions to governments, organizations now 
routinely generate data of unprecedented scope and complexity. Gleaning meaningful information and competitive 
advantages from massive amounts of data has become increasingly important to organizations globally. Trying to 
efficiently extract the meaningful insights from such data sources quickly and easily is challenging. Thus, analytics 
increase their market share. The tools available to handle the volume, velocity, and variety of big data have 
improved greatly in recent years. In general, these technologies are not prohibitively expensive, and much of the 
software is open source. Hadoop, the most commonly used framework, combines commodity hardware with open-
source software. It takes incoming streams of data and distributes them onto cheap disks; it also provides tools for 
analyzing the data. However, these technologies do require a skill set that is new to most IT departments, which will 
need to work hard to integrate all the relevant internal and external sources of data. Although attention to technology 
isn’t sufficient, it is always a necessary component of a big data strategy. This paper discusses some of the most 
commonly used big data technologies mostly open source that work together as a big data analytics system for 
leveraging large quantities of unstructured data to make more informed decisions.  
https://doi.org/10.48009/2_iis_2015_81-90
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
82 
Big Data is a data analysis methodology enabled by recent advances in technologies that support high-velocity data 
capture, storage and analysis. Data sources extend beyond the traditional corporate database to include emails, 
mobile device outputs, and sensor-generated data where data is no longer restricted to structured database records 
but rather unstructured data having no standard formatting [30]. Since Big Data and Analytics is a relatively new 
and evolving phrase, there is no uniform definition; various stakeholders have provided diverse and sometimes 
contradictory definitions. One of the first widely quoted definitions of Big Data resulted from the Gartner report of 
2001. Gartner proposed that, Big Data is defined by three V’s volume, velocity, and variety. Gartner expanded its 
definition in 2012 to include veracity, representing requirements about trust and uncertainty pertaining to data and 
the outcome of data analysis. In a 2012 report, IDC defined the 4th V as value—highlighting that Big Data 
applications need to bring incremental value to businesses. Big Data Analytics is all about processing unstructured 
information from call logs, mobile-banking transactions, online user generated content such as blog posts and 
tweets, online searches, and images which can be transformed into valuable business information using 
computational techniques to unveil trends and patterns between datasets. 
Another dimension of the Big Data definition involves technology. Big Data is not only large and complex, but it 
requires innovative technology to analyze and process. In 2013, the National Institute of Standard and Technology 
(NIST) Big Data workgroup proposed the following definition of Big Data that emphasizes application of new 
technology; Big Data exceed the capacity or capability of current or conventional methods and systems, and enable 
novel approaches to frontier questions previously inaccessible or impractical using current or conventional methods. 
Business challenges rarely show up in the appearance of a perfect data problem, and even when data are abundant, 
practitioners have difficulties to incorporate it into their complex decision-making that adds business value. In 2012, 
McKinsey & Company conducted a survey of 1,469 executives across various regions, industries and company 
sizes, in which 49 percent of respondents said that their companies are focusing big data efforts on customer 
insights, segmentation and targeting to improve overall performance [10] An even higher number of respondents 60 
percent said their companies should focus efforts on using data and analytics to generate these insights. Yet, just 
one-fifth said that their organizations have fully deployed data and analytics to generate insights in one business unit 
or function, and only 13 percent use data to generate insights across the company. As these survey results show, the 
question is no longer whether big data can help business, but how can business derive maximum results from big 
data. 
Predictive Analytics is the use of historical data to forecast on consumer behavior and trends [18]. It is the use of 
past/historical data to predict future trends. This analysis makes use of the statistical models and machine learning 
algorithms to identify patterns and learn from historical data [25]. Predictive Analysis can also be defined as a 
process that uses machine learning to analyze data and make predictions [22].  
future, and 68% sight competitive advantage as the prime benefit of predictive analysis [17]. Broadly speaking, 
predictive analysis can be applied in ecommerce for product recommendation, price management, and predictive 
search. Typically a large e-commerce site offers thousands of product and services for sale. Navigating and 
searching for a product out of thousands on a website could be a major setback to consumers. However, with the 
invention of recommender system, an E-Commerce site/application can quickly identify/predict products that 
closely suit the consumer’s taste [24].  
Using a technology called Collaborative Filtering a database of historical user preferences is created. When a new 
customer access the ecommerce site, the customer is matched with the database of preferences, in order to discover a 
preference class that closely matches the customer taste. These products are then recommended to the customer [24]. 
Another technology that is used in ecommerce is the clustering algorithm. Clustering algorithm works by identifying 
groups of users that have similar preferences. These users are then clustered into a single group and are given a 
unique identifier.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
83 
New customers cluster are predicted by calculating the average similarities of the individual members in that cluster. 
Hence a user could be a partial member of more than one cluster depending of the weight of the user’s average 
opinion [24]. Advanced analytics is defined as the scientific process of transforming data into insight for making 
better decisions. As a formal discipline, advanced analytics have grown under the Operational Research domain. 
There are some fields that have considerable overlap with analytics, and also different accepted classifications for 
the types of analytics [2].  
Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large 
amounts of log data from many different sources to a centralized data store. Flume deploys as one or more agents, 
each contained within its own instance of the Java Virtual Machine (JVM). Agents consist of three pluggable 
components: sources, sinks, and channels. Flume agents ingest incoming streaming data from one or more sources. 
Data ingested by a Flume agent is passed to a sink, which is most commonly a distributed file system like Hadoop. 
agent to be the sink of another. Flume sources listen and consume events. Events can range from newline-terminated 
strings in stdout to HTTP POSTs and RPC calls — it all depends on what sources the agent is configured to use. 
Flume agents may have more than one source, but at the minimum they require one. Sources require a name and a 
type; the type then dictates additional configuration parameters. 
Channels are the mechanism by which Flume agents transfer events from their sources to their sinks. Events written 
to the channel by a source are not removed from the channel until a sink removes that event in a transaction. This 
allows Flume sinks to retry writes in the event of a failure in the external repository (such as HDFS or an outgoing 
network connection). For example, if the network between a Flume agent and a Hadoop cluster goes down, the 
channel. Sink is an interface implementation that can remove events from a channel and transmit them to the next 
agent in the flow, or to the event’s final destination and also sinks can remove events from the channel in 
transactions and write them to output. Transactions close when the event is successfully written, ensuring that all 
events are committed to their final destination.  
Apache Sqoop is a CLI tool designed to transfer data between Hadoop and relational databases. Sqoop can import 
been transformed using MapReduce. Sqoop also has the ability to import data into HBase and Hive. Sqoop connects 
imported. Both import and export utilize MapReduce, which provides parallel operation as well as fault tolerance. 
During import, Sqoop reads the table, row by row, into HDFS. Because import is performed in parallel, the output in 
HDFS is multiple files.  
Apache’s Pig is a major project, which is lying on top of Hadoop, and provides higher-level language to use 
Hadoop’s MapReduce library. Pig provides the scripting language to describe operations like the reading, filtering 
and transforming, joining, and writing data which are exactly the same operations that MapReduce was originally 
designed for. Instead of expressing these operations in thousands of lines of Java code which uses MapReduce 
directly, Apache Pig lets the users express them in a language that is not unlike a bash or Perl script.  
Pig was initially developed at Yahoo Research around 2006 but moved into the Apache Software Foundation in 
2007. Unlike SQL, Pig does not require that the data must have a schema, so it is well suited to process the 
unstructured data. But, Pig can still leverage the value of a schema if you want to supply one. PigLatin is relationally 
complete like SQL, which means it is at least as powerful as a relational algebra. Turing completeness requires 
conditional constructs, an infinite memory model, and looping constructs.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
84 
SQL for querying. Being a SQL dialect, HIVEQL is a declarative language. In PigLatin, you specify the data flow, 
but in Hive we describe the result we want and hive figures out how to build a data flow to achieve that result. 
Unlike Pig, in Hive a schema is required, but you are not limited to only one schema. Like PigLatin and SQL, 
HiveQL itself is a relationally complete language but it is not a Turing complete language.  
Apache Zoo Keeper is an effort to develop and maintain an open-source server, which enables highly reliable 
distributed coordination. It provides a distributed configuration service, a synchronization service and a naming 
registry for distributed systems. Distributed applications use ZooKeeper to store and mediate updates to import 
configuration information. ZooKeeper is especially fast with workloads where reads to the data are more common 
than writes. The ideal read/write ratio is about 10:1. ZooKeeper is replicated over a set of hosts (called an ensemble) 
and the servers are aware of each other and there is no single point of failure.   
Figure 1. Intel Manager for Hadoop [3] 
MongoDB is an open source, document-oriented NoSQL database that has lately attained some space in the data 
industry. It is considered as one of the most popular NoSQL databases, competing today and favors master-slave 
replication. The role of master is to perform reads and writes whereas the slave confines to copy the data received 
from master, to perform the read operation, and backup the data. The slaves do not participate in write operations 
but may select an alternate master in case of the current master failure. MongoDB uses binary format of JSON-like 
documents underneath and believes in dynamic schemas, unlike the traditional relational databases. The query 
system of MongoDB can return particular fields and query set compass search by fields, range queries, regular 
expression search, etc. and may include the user-defined complex JavaScript functions. As hinted already, 
MongoDB practice flexible schema and the document structure in a grouping, called Collection, may vary and 
common fields of various documents in a collection can have disparate types of the data. 
The MongoDB is equipped with the suitable drivers for most of the programming languages, which are used to 
develop the customized systems that use MongoDB as their backend player. There is an increasingly demand of 
using MongoDB as pure in-memory database; in such cases, the application dataset will always be small. Though, it 
is probably are easy for maintenance and can make a database developer happier; this can be a bottle neck for 
complex applications that require tremendous database management capabilities. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
85 
In order to efficiently address the challenges of Big Data, the leading vendor developed the Oracle NoSQL database. 
It was built by Oracle Berkeley DB team and the Berkeley DB Java Edition is the building block of Oracle NoSQL. 
Berkeley DB is a robust and scalable key-value store and used as the underlying storage for several popular data 
model such as Amazon Dynamo, GenieDB, MemcacheDB and Voldemort [28].  
scalability, throughput, and reliability with little tuning efforts. It is an efficient and a resilient transaction model that 
significantly eases the development process of applications, involving Big Data. It is a distributed, scalable yet 
simple key-value pair data model that fully supports the ACID transactions and JSON format and integrated with 
Oracle Database and Hadoop. It offers scalable throughput with bounded latency. The model very well 
accommodates the horizontal scaling with dynamic annexation of new capacity, citing high availability; the design 
architecture of Oracle NoSQL does not support single point of failure, and lucid load balancing. Actually, the goals 
of high availability, rapid failover in the event of a node failure, etc. are achieved by replicating the storage nodes. 
which is able to handle big data requirements. It is a highly scalable and high-performance distributed database 
management system that can handle real-time big data applications that drive key systems for modern and successful 
businesses. It has a built-for-scale architecture that can handle petabytes of information and thousands of concurrent 
users/operations per second as easily as it can manage much smaller amount of data and user traffic. It has a peer to 
peer design that offers no single point of failure for any database process or function, in addition to the location 
independence capabilities that equate to a true network-independent method of storing and accessing data, data can 
be read and written anywhere. Apache Cassandra is also equipped with flexible/dynamic schema design that 
accommodates all formats of big data applications, including structured, semi-structured, and unstructured data. 
online.  
clusters of computers. It is designed to scale up from single servers to thousands of machines, with each offering 
local computation and storage. The basic notion is to allow a single query to find and collect results from all the 
cluster members, and this model is clearly suitable for Google's model of search support. One of the largest 
technological challenges in software systems research today is to provide mechanisms for storage, manipulation, and 
information retrieval on large amount of data. Web services and social media produce together an impressive 
amount of data, reaching the scale of petabytes daily (Facebook, 2012). These data may contain valuable 
information, which sometimes is not properly explored by existing systems. Most of this data is stored in a non-
structured manner, using different languages and format, which, in many cases, are in compatible. 
large datasets. Over the last years, commodity hardware became part of clusters, since the x86 platform cope with 
the need of having an overall better cost/performance ratio, while decreasing maintenance cost. Apache Hadoop is a 
framework developed to take advantage of this approach, using such commodity clusters for storage, processing and 
manipulation of large amount of data. The framework was designed over the MapReduce paradigm and uses the 
HDFS as a storage file system. Hadoop presents key characteristics when performing parallel and distributed 
computing, such as data integrity, availability, scalability, exception handling, and failure recovery.   
Hadoop is a popular choice when you need to filter, sort, or pre-process large amounts of new data in place and 
distill it to generate denser data that theoretically contains more information. Pre-processing involves filtering new 
data sources to make them suitable for additional analysis in a data warehouse.  Hadoop is a top-level open source 
project of the Apache Software Foundation. Several suppliers, including Intel, offer their own commercial Hadoop 
distributions, packaging the basic software stack with other Hadoop software projects such as Apache Hive, Apache 
Pig, and Apache Sqoop. These distributions must integrate with data warehouses, databases, and other data 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
86 
process or query.	  
Figure 2. Data Architecture with Hadoop Integrated with existing data system [12] 
MapReduce is the original massively scalable, parallel processing framework commonly used with Hadoop and 
other components such as the Hadoop Distributed File System (HDFS) and YARN. YARN can be described as a 
large-scale, distributed operating system for big data implementations. As Hadoop has matured, the batch-oriented, 
disk-intensive MapReduce’ s limitations have become more apparent as Big Data analytics moves to more real-time, 
streaming processing and advanced implementations such as the aforementioned machine learning.  
MapReduce is the model of distributed data processing introduced by Google in 2004. The fundamental concept of 
MapReduce is to divide problems into two parts: a map function that processes source data into sufficient statistics 
and a reduce function that merges all sufficient statistics into a final answer. By definition, any number of 
concurrent map functions can be run at the same time without intercommunication. Once all the data has had the 
map function applied to it, the reduce function can be run to combine the results of the map phases.  For large scale 
batch processing and high speed data retrieval, common in Web search scenarios, MapReduce provides the fastest, 
most cost-effective and most scalable mechanism for returning results. Today, most of the leading technologies for 
managing "big data" are developed on MapReduce. With MapReduce there are few scalability limitations, but 
leveraging it directly does require writing and maintaining a lot of code. 
Splunk is a general-purpose search, analysis and reporting engine for time-series text data, typically machine data. 
Splunk software is deployed to address one or more core IT functions: application management, security, 
compliance, IT operations management and providing analytics for the business. The Splunk engine is optimized for 
quickly indexing and persisting unstructured data loaded into the system. Specifically, Splunk uses a minimal 
schema for persisted data – events consist only of the raw event text, implied timestamp, source (typically the 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
87 
filename for file based inputs), source type (an indication of the general type of data) and host (where the data 
originated).  
Once data enters the Splunk system, it quickly proceeds through processing, is persisted in its raw form and is 
indexed by the above fields along with all the keywords in the raw event text. Indexing is an essential element of the 
canonical “super-grep” use case for Splunk, but it also makes most retrieval tasks faster. Any more sophisticated 
processing on these raw events is deferred until search time. This serves four important goals: indexing speed is 
increased as minimal processing is performed, bringing new data into the system is a relatively low effort exercise as 
no schema planning is needed, the original data is persisted for easy inspection and the system is resilient to change 
as data parsing problems do not require reloading or re-indexing the data. 
Apache Spark an open source big data processing framework built around speed, ease of use, and sophisticated 
analytics. It was originally developed in 2009 in UC Berkeley’s AMP Lab, and open sourced in 2010 as an Apache 
project. Hadoop as a big data processing technology has been around for ten years and has proven to be the solution 
of choice for processing large data sets. MapReduce is a great solution for one-pass computations, but not very 
efficient for use cases that require multi-pass computations and algorithms. Each step in the data processing 
workflow has one Map phase and one Reduce phase and you'll need to convert any use case into MapReduce pattern 
to leverage this solution. Spark takes MapReduce to the next level with less expensive shuffles in the data 
processing. With capabilities like in-memory data storage and near real-time processing, the performance can be 
several times faster than other big data technologies.  
Spark also supports lazy evaluation of big data queries, which helps with optimization of the steps in data processing 
workflows. It provides a higher-level API to improve developer productivity and a consistent architect model for big 
data solutions. Spark holds intermediate results in memory rather than writing them to disk, which is very useful 
especially when you need to work on the same dataset multiple times. It’s designed to be an execution engine that 
works both in-memory and on-disk. Spark operators perform external operations when data does not fit in memory. 
Spark can be used for processing datasets that larger than the aggregate memory in a cluster. Spark will attempt to 
store as much as data in memory and then will spill to disk. It can store part of a data set in memory and the 
remaining data on the disk. You have to look at your data and use cases to assess the memory requirements. With 
this in-memory data storage, Spark comes with a great performance advantage. 
Spark is written in Scala Programing Language and runs on the Java Virtual machine. It currently supports 
programming languages like Scala, java, python, Clojure and R. Other than Spark Core API, there are additional 
libraries that are part of the Spark ecosystem and provide additional capabilities in Big Data analytics. Spark 
Streaming is one among the spark library that can be used for processing the real-time streaming data. This is based 
on micro based on micro batch style of computing and processing. Spark SQL provides the capabilities to expose the 
visualization tools. MLlib, GraphX are some other libraries from spark. 
Thomas H. Davenport was perhaps the first to observe in his Harvard Business Review article published in January 
2006 (“Competing on Analytics”) how companies who orientated themselves around fact based management 
approach and compete on their analytical abilities considerably out-performed their peers in the marketplace. The 
reality is that it takes continuous improvement to become an analytics-driven organization. In a presentation given at 
the Strata New York conference in September 2011, McKinsey & Company showed the eye opening; 10-year 
category growth rate differences (see Figure 7, below) between businesses that smartly use their big data and those 
that do not.  
Amazon uses Big Data to monitor, track and secure 1.5 billion items in its inventory that are laying around 200 
fulfillment centers around the world, and then relies on predictive analytics for its ‘anticipatory shipping’ to predict 
when a customer will purchase a product, and pre-ship it to a depot close to the final destination. Wal-Mart handles 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
88 
more than a million customer transactions each hour [23], imports information into databases to contain more than 
2.5 petabytes and asked their suppliers to tag shipments with radio frequency identification (RFID) systems [8] that 
can generate 100 to 1000 times the data of conventional bar code systems. UPS deployment of telematics in their 
freight segment helped in their global redesign of logistical networks [6]. Amazon is a big data giant and the largest 
online retail store. The company pioneered e-commerce in many different ways, but one of its biggest successes was 
the personalized recommendation system, which was built from the big data it gathers from its millions of 
customers’ transactions. 
The U.S. federal government collects more than 370,000 raw and geospatial datasets from 172 agencies and sub 
agencies.  It leverages that data to provide a portal to 230 citizen-developed apps, with the aim of increasing public 
access to information not deemed private or classified. Professional social network LinkedIn uses data from its more 
than 100 million users to build new social products based on users’ own definitions of their skill sets. Silver Spring 
Networks deploys smart, two-way power grids for its utility customers that utilize digital technology to deliver more 
help manage energy use and maximize efficiency. Jeffrey Brenner and the Camden Coalition mapped a city’s crime 
trends to identify problems with its healthcare system, revealing services that were both medically ineffective and 
expensive. 
Today’s technology landscape is changing fast. Organizations of all shapes and sizes are being pressured to be data-
driven and to do more with less. Even though big data technologies are still in a nascent stage, relatively speaking, 
the impact of the 3V’s of big data, which now is 5v’s cannot be ignored. The time is now for organizations to begin 
planning for and building out their Hadoop-based data lake. Organizations with the right infrastructures, talent and 
vision in place are well equipped to take their big data strategies to the next level and transform their businesses. 
They can use big data to unveil new patterns and trends, gain additional insights and begin to find answers to 
pressing business issues. The deeper organizations dig into big data and the more equipped they are to act upon 
what’s learned, the more likely they are to reveal answers that can add value to the top line of the business. This is 
where the returns on big data investments multiply and the transformation begins. Harnessing big data insight 
delivers more than cost cutting or productivity improvement but it definitely reveals new business opportunities. 
Data-driven decisions always tend to be better decisions. 
1. Apache Software Foundation. (2010). Apache ZooKeeper. Retrieved April 5, 2015 from 
https://zookeeper.apache.org 
2. Chae, B., Sheu, C., Yang, C. and Olson, D. (2014). The impact of advanced analytics and data accuracy on 
operational performance: A contingent resource based theory (RBT) perspective, Decision Support Systems, 59, 
119-126. 
3. Chambers, C., Raniwala, A., Adams, S., Henry, R., Bradshaw, R., and Weizenbaum, N. (2010). Flume Java: 
Easy, Efficient Data-Parallel Pipelines. Google, Inc. Retrieved April 1, 2015 from 
http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf 
4. Cisco Systems. Cisco UCS Common Platform Architecture Version 2 (CPA v2) for Big Data with 
Comprehensive Data Protection using Intel Distribution for Apache Hadoop. Retrieved March 15, 2015, from 
http://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/Cisco_UCS_CPA_for_Big_Data_wi
th_Intel.html 
5. DATASTAX Corporation. (2013, October). Big Data: Beyond the Hype - Why Big data Matters to you [White 
paper]. Retrieved March 15, 2015 from https://www.datastax.com/wp-content/uploads/2011/10/WP-DataStax-
BigData.pdf 
6. Davenport, T & Patil, D. (2012). Data Scientist: The Sexiest Job of the 21st Century. Harvard Business Review, 
90, 70-76. 
7. Dhawan, S & Rathee, S. (2013). Big Data Analytics using Hadoop Components like Pig and Hive. American 
International Journal of Research in Science, Technology, Engineering & Mathematics, 88, 13-131. Retrieved 
from http://iasir.net/AIJRSTEMpapers/AIJRSTEM13-131.pdf 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
89 
8. Edwards, P., Peters, M. and Sharman, G. (2001). The Effectiveness of Information Systems in Supporting the 
Extended Supply Chain, Journal of Business Logistics 22 (1), 1-27 
9. EMC Corporation. (2013, January). EMC Accelerates Journey to Big Data with Business Analytics-as-a-
Service [White paper]. Retrieved from http://www.emc.com/collateral/white-papers/h11259-emc-accelerates-
journey-big-data-ba-wp.pdf 
10. EMC Corporation. Big Data, Big Transformations [White paper]. Retrieved from 
http://www.emc.com/collateral/white-papers/idg-bigdata-umbrella-wp.pdf 
11. EMC Solutions Group. (2012, July). Big Data-as-a-Service [White paper]. Retrieved from 
https://www.emc.com/collateral/software/white-papers/h10839-big-data-as-a-service-perspt.pdf 
12. Enterprise Hadoop: The Ecosystem of Projects. Retrieved from http://hortonworks.com/hadoop/ 
13. George, L. (2014, September). Getting Started with Big Data Architecture. Retrieved April 5, 2015, from 
http://blog.cloudera.com/blog/2014/09/getting-started-with-big-data-architecture/ 
14. IBM Corporation. IBM Big Data Platform. Retrieved from http://www-
01.ibm.com/software/in/data/bigdata/enterprise.html 
15. Intel Corporation. Big Data Analytics - Extract, Transform, and Load Big data with Apache Hadoop [White 
paper]. Retrieved April 3, 2015 from https://software.intel.com/sites/default/files/article/402274/etl-big-data-
with-hadoop.pdf 
16. McClary, D. (2013, June). Acquiring Big Data Using Apache Flume. Retrieved March 3, 2015 from 
http://www.drdobbs.com/database/acquiring-big-data-using-apache-flume/240155029 
17. Millard, S. (2013). Big Data Brewing Value in Human Capital Management – Ventana Research. Retrieved 
April 2, 2015 from http://stephanmillard.ventanaresearch.com/2013/08/28/big-data-brewing-value-in-human-
capital-management 
18. Mosavi, A. and Vaezipour, A. (2013). Developing Effective Tools for Predictive Analytics and Informed 
Decisions. Technical Report. University of Tallinn.  
19. Oracle Corporation. (2013, March). Big Data Analytics - Advanced Analytics in Oracle Database [White 
paper]. Retrieved March 5, 2015 from http://www.oracle.com/technetwork/database/options/advanced-
analytics/advanced-analytics-wp-12c-1896138.pdf?ssSourceSiteId=ocomen 
20. Oracle Enterprise Architecture. (2015, April). An Enterprise Architect's Guide to Big Data - Reference 
Architecture Overview [White paper]. Retrieved from 
http://www.oracle.com/technetwork/topics/entarch/articles/oea-big-data-guide-1522052.pdf 
21. Penchikala, S. (2015, January). Big Data Processing with Apache Spark - Part 1: Introduction. Retrieved from 
http://www.infoq.com/articles/apache-spark-introduction 
22. Puri, R. (2013). How Online Retailers Use Predictive Analytics To Improve Your Shopping Experience. 
Retrieved April5, 2015 from http://blogs.sap.com/innovation/analytics/how-online-retailers-use-predictive-
analytics-to-improve-your-shopping-experience-0108060 
23. Sanders, N.R. (2014). Big Data Driven Supply Chain Management: A Framework for Implementing Analytics 
and Tuning Information into Intelligence, 1st Edition, Pearson, NJ 
24. Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. (2002). Recommendation systems for large e-commerce: 
Scalable neighborhood formation using clustering. In Proceedings of the fifth international conference on 
computer and information technology, 1.  
25. Shmueli, G. & Koppius, O. (2011). Predictive Analytics in Information Systems Research. MIS Quarterly, 
35(3), pp. 553-72. 
26. Sorkin, S. (2011). Splunk Technical Paper: Large-Scale, Unstructured Data Retrieval and Analysis Using 
Splunk. Retrieved April 15, 2015 from https://www.splunk.com/content/dam/splunk2/pdfs/technical-
briefs/splunk-and-mapreduce.pdf 
27. The Bloor Group. IBM and the Big Data Information Architecture. Retrieved April 3, 2015 from 
http://insideanalysis.com/wp-content/uploads/2014/08/BDIAVendor-IBMv01.pdf 
28. Tiwari, S. (2011). Using Oracle Berkeley DB as a NoSQL Data Store. Retrieved April 5 2015 from 
http://www.oracle.com/technetwork/articles/cloudcomp/berkeleydb-nosql-323570.htm 
29. Transparency Market Report. (May, 2015).Big Data Applications in Healthcare likely to Propel Market to 
US$48.3 Bn by 2018. Retrieved June 26, 2015, from 
http://www.transparencymarketresearch.com/pressrelease/big-data-market.htm 
30. Villars, R. L., Olofson, C. W., & Eastwood, M. (2011, June). Big data: What it is and why you should care. IDC 
White Paper. Framingham, MA: IDC. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
90 
31. Wolpe, T. (2015, March). How Facebook is speeding up the Pesto SQL query engine. Retrieved April 3, 2015, 
from http://www.zdnet.com/article/how-facebook-is-speeding-up-the-presto-sql-query-engine 
32. Zahari et al. (2010). Spark: Cluster Computing with Working Sets. Retrieved April 7, 2015, from 
http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf 

Removed lines from 48077-157-151840-1-10-20200520.pdf:
THÔNG TIN VÀ TƯ LIỆU - 2/2020 23
BIG DATA VÀ XU HƯỚNG ỨNG DỤNG TRONG HOẠT ĐỘNG THÔNG TIN - THƯ VIỆN
ThS Nguyễn Lê Phương Hoài
Viện Thông tin Khoa học xã hội 
● Tóm tắt: Big Data là một thuật ngữ được sử dụng để chỉ những bộ dữ liệu khổng lồ, chủ yếu không 
có cấu trúc, được thu thập từ nhiều nguồn khác nhau. Big Data có nhiều tác động, ứng dụng và được 
xem như một yếu tố quyết định đến việc phát triển, mang lại lợi thế cạnh tranh cho tổ chức. Bài viết 
tổng quan lược sử các quan điểm về Big Data, đồng thời nhấn mạnh vào các xu hướng ứng dụng 
trong hoạt động thông tin - thư viện.
● Từ khóa: Big Data; dữ liệu lớn; hoạt động thư viện.
BIG DATA APPLICATION IN LIBRARY AND INFORMATION ACTIVITIES
● Abstract: Big Data is a term used to refer to huge, mostly unstructured datasets, collected from 
a variety of sources. Big Data has many impacts, applications and is considered as a decisive 
factor in the development, bringing competitive advantages to the organization. The overview paper 
summarizes the views on Big Data and emphasizes application trends in library and information 
activities.
● Keywords: Big Data; library activities.
1. LƯỢC SỬ CÁC QUAN ĐIỂM VỀ BIG DATA
Hiện nay, chưa có một định nghĩa chính 
xác cho thuật ngữ Big Data. Big được ghi 
nhận lần đầu tiên trong báo cáo “Application-
controlled demand paging for out-of-core 
visualization” của Michael Cox và David 
thứ 8 (vào tháng 10 năm 1997). Ý tưởng đưa 
xử lý không còn đủ số lượng cần khảo sát, do 
thể phân tích được tất cả các thông tin [11].
Tháng 8 năm 1999, Steve Bryson, David 
Kenwright, Michael Cox, David Ellsworth, và 
Robert Haimes đăng bài “Visually exploring 
gigabyte data sets in real time” trên Tạp chí 
Communications of the ACM. Đây là bài viết 
đầu tiên sử dụng thuật ngữ “Big Data”. Các 
tác giả nhận định: “Những chiếc máy tính 
vực, cũng có thể là bất lợi; tính toán nhanh 
chóng tạo ra một lượng lớn dữ liệu. Nếu trước 
là lớn, thì bây giờ chúng ta có thể tìm thấy 
300 GB” [15]. 
Tháng 11 năm 2000, Francis X. Diebold 
Hiệp hội Kinh tế lượng bài viết “Big Data 
Measurement and Forecasting”. Trong bài 
viết này, tác giả khẳng định: “Gần đây, nhiều 
ngành khoa học như vật lý, sinh học, khoa 
học xã hội, vốn đang buộc phải đương đầu với 
khó khăn - đã thu được lợi từ hiện tượng Big 
Data và đã gặt hái được nhiều thành công. Big 
Data chỉ sự bùng nổ về số lượng (và đôi khi, 
chất lượng), khả năng liên kết cũng như độ 
sẵn sàng của dữ liệu, chủ yếu là kết quả của 
việc ghi lại dữ liệu và công nghệ lưu trữ” [4].
Tháng 2 năm 2001, Doug Laney - nhà 
phân tích của Tập đoàn Meta, công bố nghiên 
cứu “3D Data Managment: Controlling Data 
Volume, Velocity, and Variety”. Laney cho 
rằng, những thách thức và cơ hội nằm trong 
bằng mô hình “3Vs”: tăng về số lượng lưu trữ 
(Volume), tăng về tốc độ xử lý (Velocity) và 
tăng về chủng loại (Variety) [3]. Một thập kỷ 
sau, mô hình “3Vs” đã trở thành thuật ngữ 
dữ liệu lớn ba chiều. Nhiều công ty và tổ chức 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
24 THÔNG TIN VÀ TƯ LIỆU - 1/2020
24
dụng mô hình “3Vs” này để định nghĩa Big 
Data.
Tháng 12 năm 2008, Randal E. Bryant, 
Randy H. Katz, và Edward D. Lazowska 
công bố bài viết “Big-Data Computing: 
Commerce, Science and Society”, trong đó 
miêu tả: “Cũng như công cụ tìm kiếm đã làm 
thay đổi cách chúng ta tiếp cận thông tin, các 
ty, các nhà nghiên cứu khoa học, các học 
viên y tế, quốc phòng và tình báo,... Sử dụng 
nghệ máy tính suốt một thập kỷ qua. Chúng 
nó trong việc thu thập, sắp xếp và xử lý dữ 
liệu của tất cả các tầng lớp xã hội. Một khoản 
sẽ thúc đẩy phát triển và mở rộng nó” [13].
Tháng 2 năm 2010, Kenneth Cukier đăng 
“Data, data everywhere”. Cukier viết: “... thế 
mức không tưởng, và càng ngày càng được 
nhân rộng với tốc độ nhanh hơn bao giờ hết... 
Hiệu quả đã được thể hiện ở khắp mọi nơi, từ 
kinh doanh đến khoa học, từ chính phủ đến 
nghệ thuật. Các nhà khoa học và kỹ sư máy 
tượng này: Big Data” [8].
Tháng 5 năm 2012, Danah Boyd và Kate 
bài “Critical Question for Big Data” trên tờ 
Information, Communications and Society. 
Các tác giả định nghĩa Big Data như là “một 
hiện tượng văn hóa, công nghệ và học thuật 
dựa trên sự tương tác của: 1) Công nghệ tối 
thuật toán để thu thập, phân tích, liên kết, và 
so sánh các tập dữ liệu lớn; 2) Phân tích: tạo 
tuyên bố kinh tế, xã hội, kỹ thuật và pháp lý; 3) 
Thần thoại: niềm tin phổ biến rằng dữ liệu lớn 
biết mà trước đây không thể, với hào quang 
của sự thật, khách quan, chính xác” [2].
Sau đó, Gartner - công ty nghiên cứu và 
tư vấn công nghệ thông tin - bổ sung thêm 
rằng “Big Data ngoài 3 tính chất (số lượng, 
tốc độ xử lý và chủng loại) thì còn phải cần 
khám phá sâu vào sự vật/sự việc và tối ưu 
hóa các quy trình làm việc” [5]. Cùng quan 
điểm đó, Tan Jee Toon cho rằng Big Data 
mọi thứ xung quanh chúng ta, từ các thiết bị 
kỹ thuật số như di động, video, hình ảnh, tin 
nhắn tới các thiết bị cảm biến, các máy móc 
hội. Big Data có đặc điểm là được sinh ra với 
khối lượng (volume), tốc độ (velocity), độ đa 
dạng (variety) và tính xác thực (veracity) rất 
lớn [16].
Năm 2014, Gartner đưa ra khái niệm 
mới về Big Data bằng mô hình “5Vs”, gồm: 
Volume (khối lượng), Velocity (tốc độ), 
Variety (tính đa dạng), Veracity (tính xác 
thực) và Value (giá trị). Trong đó: Volume là 
khối lượng Big Data được tạo ra mỗi ngày. 
phân tán, nơi mà dữ liệu chỉ được lưu trữ một 
bởi phần mềm. Velocity là tốc độ dữ liệu mới 
được tạo ra và tốc độ dữ liệu chuyển động. 
giữ chúng trong các cơ sở dữ liệu. Variety là 
các kiểu khác nhau của dữ liệu. Công nghệ 
có cấu trúc truyền thống (được lưu trữ trong 
các bảng hoặc các cơ sở dữ liệu quan hệ) và 
phi cấu trúc (bao gồm các thông điệp, trao 
đổi của mạng xã hội, các hình ảnh, dữ liệu 
cảm biến, video, tiếng nói...). Veracity là tính 
hỗn độn hoặc tính tin cậy của dữ liệu. Công 
kiểm soát những loại dữ liệu này. Value là giá 
trị của dữ liệu. Việc tiếp cận Big Data sẽ chỉ 
thành những thứ có giá trị. Đây là khái niệm 
đầy đủ về 5 tính chất của Big Data [5].
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 25
THÔNG TIN VÀ TƯ LIỆU - 1/2020 25
2. XU HƯỚNG ỨNG DỤNG BIG DATA TRONG HOẠT 
ĐỘNG THÔNG TIN - THƯ VIỆN
Ngày nay, một lượng lớn dữ liệu số có thể 
các mạng xã hội. Theo Howe D. (2008): “Chỉ 
riêng trong các lĩnh vực nghiên cứu khoa học, 
trưởng theo cấp số nhân” [7]. Số lượng dữ 
nhiều lĩnh vực khác nhau và dữ liệu lớn (Big 
Data) được sử dụng rộng rãi trong các lĩnh 
vực, tổ chức với nhiều mục đích khác nhau. 
hành vi tiêu dùng của khách hàng, để đề xuất 
trên thông tin thu thập được (Ebay, Facebook, 
Google...). Các cơ sở nghiên cứu khoa học sử 
khoa học mới, ví dụ như xây dựng bản đồ 
gene của con người,... Việc sử dụng Big Data 
trong hoạt động thông tin - thư viện đã bắt 
đầu được quan tâm nghiên cứu. Campbell 
D. Grant, Cowan Scott R. (2016) phân tích 
và dữ liệu liên kết [1]. Kim Young Seok (2017) 
khuôn mặt Chernoff [9]. Gerrard D., Mooney 
J. và Thompson D. (2017) xem xét kiến trúc 
phân tích dữ liệu, các bộ tài nguyên được bảo 
thời gian tới [6]. Waqar Ahmed và Kanwal 
Ameen (2017) tổng quan các khái niệm về 
thư viện [17]. Ye Chunlei (2017) nghiên cứu 
trong thư viện đại học [18]. Zhan Ming, Widén 
Gunilla (2018) nghiên cứu vai trò của thư 
viện công cộng trong thời đại Big Data [20]. 
Li Shuqing; Jiao Fusen; Zhang Yong; Xu Xia 
(2019) nghiên cứu các vấn đề và thay đổi của 
dịch vụ người dùng tin [10],... Các nghiên cứu 
trong thời đại Big Data. Bàn về xu hướng ứng 
tiếp nào, nhưng có thể tổng hợp thành các xu 
hướng chính như sau:
Một là, tổ chức lưu trữ, bảo quản dữ liệu
Marydee Ojala nhận định: “Các thư viện 
ngày nay. Bộ sưu tập các tài nguyên số được 
các thư viện. Khối lượng và tính đa dạng dữ 
thư viện phải có phương pháp tổ chức lưu trữ, 
bảo quản dữ liệu hợp lý” [12]. Nguồn dữ liệu 
thư viện bao gồm: nguồn dữ liệu mô tả tài liệu 
thư viện, nguồn tài nguyên số hóa tài liệu thư 
viện, nguồn tài liệu số thư viện bổ sung qua 
việc mua hay sử dụng chung, nguồn dữ liệu 
khảo sát thư viện, dữ liệu định tính, dữ liệu 
tương tác xã hội,... Trước đây, các thư viện 
băng, đặt trong các cơ sở lưu trữ. Trước tác 
mạng công nghiệp lần thứ tư, các thư viện 
chi phí hiệu quả. Dữ liệu được lưu trữ theo 
hai cách, cả trên các thiết bị ngoại tuyến (thẻ 
nhớ SD, ổ cứng ngoài, ổ đĩa flash) và lưu trữ 
trực tuyến trên đám mây. Với phương thức 
kết hợp sử dụng băng từ để bảo quản lưu trữ, 
được yêu cầu, và sử dụng lưu trữ đám mây 
cho các Big Data. Các thư viện hướng đến 
thư viện (bao gồm cả tài nguyên vật chất và 
dữ liệu), xác định nhu cầu của người dùng 
thư viện. Trong thời gian tới, khi các yêu cầu 
mới thúc đẩy việc sử dụng Big Data, các thư 
viện hướng tới việc thu nhận, tổ chức lưu trữ 
dữ liệu (lưu trữ vật lý trong các máy chủ hoặc 
trong các cơ sở dữ liệu), bảo tồn dữ liệu và 
phổ biến dữ liệu, làm cho dữ liệu có sẵn trong 
qua các sản phẩm trực quan. Các thư viện 
tiến tới xây dựng, tạo lập hệ thống bảo quản 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
26
kỹ thuật số (bảo tồn cả tài nguyên số và siêu 
dữ liệu mô tả) có thể phát triển trong nhiều 
năm tới để đáp ứng với các yêu cầu mới. 
Hai là, cung cấp sản phẩm, dịch vụ thư 
viện mang tính cá nhân hóa, tùy chỉnh 
 Hiện nay, các thư viện đang có xu 
vụ trực tuyến. Nhiều thư viện đang sử dụng 
facebook, instagram để quảng bá các sản 
phẩm và dịch vụ thư viện. Các phương tiện 
thư viện. Cùng với các dữ liệu khảo sát thư 
viện, dữ liệu định tính (thông qua phỏng vấn, 
bảng trò chuyện...), dữ liệu tương tác xã hội 
(từ các trang truyền thông xã hội)... các thư 
thư viện. Từ đó, thư viện cung cấp các sản 
cầu của người dùng. Tuy nhiên, trong thời 
gian tới, “sự gia tăng của Big Data làm cho 
nhanh hơn, cho phép các thư viện vượt ra 
học tập và phân tích hiệu suất nghiên cứu” 
[19]. “Trong kỷ nguyên Big Data, không chỉ 
Big Data để đổi mới.” [10]. “Big Data có thể 
cũng sẽ thay đổi cho phù hợp” [17]. Các nhà 
có thể tác động đến hoạt động thông tin - thư 
viện, chuyển đổi phương thức cung cấp dịch 
vụ và tích hợp các hệ thống xử lý. Các hỗ trợ 
cạnh tranh để thư viện thu hút người dùng tin. 
Theo Li Shuqing, Jiao Fusen, Zhang Yong, 
Xu Xia: “Các vấn đề và tiềm năng của các thư 
dữ liệu, công nghệ, dịch vụ và người dùng 
tin. Sử dụng Big Data hiện có và xem xét các 
tại theo quan điểm của người dùng tin, thư 
viện có thể đưa ra các ý tưởng, phương pháp 
có trong các thư viện số” [10]. Đồng thời, nhu 
dùng tin. Kim Young Seok cho rằng: “Bằng 
thực, các thư viện có thể thiết kế các dịch 
tin. Big Data cung cấp thông tin chuyên sâu 
dùng tin, từ đó tạo ra trải nghiệm cá nhân 
hóa” [9]. Ví dụ, người dùng tin tìm kiếm trên 
gì người dùng tin gõ ở mục tìm kiếm, tần suất 
tìm kiếm, số lần tham khảo danh mục tài liệu, 
số lần xem mô tả tài liệu,... được thu thập và 
phân tích để tối ưu trải nghiệm, tạo cơ hội lớn 
hóa. Đặc biệt, với các công cụ phân tích dự 
báo của Big Data, thư viện sẽ nắm được thị 
hiếu, nhu cầu chính xác để cung cấp các sản 
phẩm, dịch vụ phù hợp với người dùng tin 
trong thời gian thực.
Ba là, ứng dụng dịch vụ phân tích dự báo
Giống như hầu hết các ngành khác, phân 
tích dự báo sẽ là một sự thay đổi lớn, quan 
trọng trong các cơ quan thông tin - thư viện. 
hoạt động hiệu quả hơn, đồng thời làm thay 
người dùng tin. Theo cách truyền thống, mối 
khá đơn giản. Người dùng thư viện nộp tiền, 
làm thẻ thư viện và đổi lại, họ được phục vụ 
trong các dịch vụ khác nhau của thư viện. Tuy 
nhiên, mối quan hệ này đang dần thay đổi 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 27
viện. Người dùng thư viện cung cấp dữ liệu 
hành vi người dùng. Thông qua các dữ liệu cá 
nhân như lịch sử sử dụng tài liệu thư viện, lịch 
sử tìm kiếm, cách thức, thói quen tìm kiếm,... 
các công cụ Big Data phân tích dữ liệu, đưa 
ra thông tin chi tiết, xác định khuynh hướng, 
nhu cầu sử dụng thư viện, nhu cầu tài liệu 
người dùng thư viện trong tương lai; các công 
cầu,... Simovic Aleksandar (2018) nhận định: 
“Các công cụ Big Data kết hợp với các thuật 
toán khoa học cho phép các thư viện dự đoán 
lai, giúp dự báo tốt hơn các vấn đề phát sinh 
tin tốt nhất cho người dùng tin” [14]. Về phía 
thư viện, việc sử dụng tài nguyên Big Data 
của người dùng tin, có thể đưa ra các ý tưởng 
các dịch vụ hiện có trong thư viện kỹ thuật số. 
Đồng thời, căn cứ vào các kết quả phân tích, 
dự báo, thư viện có thể xác định thời gian, 
và dịch vụ thư viện đến người dùng thư viện. 
Về phía người dùng thư viện, dựa vào các 
kết quả dự báo về hành vi tìm kiếm, tra cứu, 
sử dụng thư viện, các hệ thống khuyến nghị 
(Recommendation Engine) sẽ gửi đến người 
dùng tin các tài liệu có thể họ quan tâm.
Bốn là, mở rộng dịch vụ chăm sóc 
thư viện, đặc biệt trong môi trường thư viện 
điện tử, thư viện số. Các thư viện đang cố 
gắng để hiểu được người dùng thư viện, giúp 
các thắc mắc, nhu cầu của mình. Big Data 
hoạt, tạo ra giá trị từ quá trình xây dựng mối 
quan hệ thân thiết với người dùng thư viện. 
Cùng với Big Data, hệ thống trả lời tự động 
(như Chatbot) không cần sự trợ giúp của 
con người phát triển tương ứng, giúp tăng 
hiệu quả phân tích dữ liệu Big Data. Hiện 
nay, nhiều thư viện sử dụng Chatbot để giao 
tiếp, trao đổi với người dùng thư viện, tiếp 
các nhu cầu của người dùng. Khi người dùng 
viện, họ có rất nhiều thắc mắc và muốn được 
giải đáp. Chatbot sẽ đưa ra các gợi ý, hỗ trợ 
từng bước một, cung cấp thông tin về các 
sản phẩm, dịch vụ của thư viện cho người 
dùng. Chatbot được thiết kế và phát triển để 
đối thoại. Qua những dữ liệu người dùng thu 
thập được, công cụ phân tích dữ liệu Big Data 
tiến hành phân tích, xác định những nhu cầu, 
dùng thư viện. Bên cạnh đó, Chatbot nhắc 
viện như thời hạn trả tài liệu, thời hạn đổi thẻ 
sử dụng,... Đặc biệt, Chatbot giúp thư viện 
chủ động hỗ trợ 24/7, tăng trải nghiệm tối 
đa cho người dùng thư viện mọi lúc. Chatbot 
lưu lại lịch sử đối thoại, thông tin người dùng 
trong chính thư viện. Chatbot hỗ trợ các thư 
viện khai thác Big Data phục vụ người dùng. 
Trong tương lai, số thư viện sử dụng Chatbot 
tính năng và lợi ích mà Chatbot mang lại. 
Cùng với đó, thông qua dữ liệu người dùng, 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
28
các thư viện có thể phân tích, dự đoán các 
các sản phẩm, dịch vụ thông qua phân tích 
hiện các giải pháp kịp thời. 
Có thể thấy, Big Data giúp tối ưu hóa hoạt 
động thư viện bằng việc thu thập, phân tích 
thông tin, tăng trải nghiệm của người dùng 
tin bằng cách cá nhân hóa thư viện số. Cùng 
với đó, Big Data có thể giúp các thư viện tiến 
hành phân tích dự báo, tìm ra các đặc điểm 
chung dự báo thị hiếu đọc, tình trạng sử dụng 
các cơ sở dữ liệu. Không chỉ vậy, Big Data tạo 
dùng tin trong quá trình sử dụng thư viện.
1. Campbell D. Grant, Cowan Scott R. (2016). “The 
Paradox of privacy: revisiting a core library value in 
an age of big data and linked data”, Library trends, 
Vol. 64, No. 3, P. 492-811.
2. 
Boyd, 
Kate 
(2012). 
Critical Question for Big Data, Information, 
Communications and Society.
3. Doug Laney (2001). “3D Data Managment: 
controlling Data Volume, Velocity, and Variety”, 
Application Delivery Strategies, Meta Group. 
File:949.
4. Francis X. Diebold (2000). “Big Data Dynamic 
and Forecasting”, Discussion of Reichlin and 
Watson paper, in Economics and Econometrics, 
Eighth World Congress of the Econometric Society. 
5. Gartner (2013). Survey Analysis: Big Data Adoption 
in 2013 shows substance behind the hype. 
6. Gerrard, D., Mooney, J. , Thompson, D. (2017). 
“Digital Preservation at Big data scale: proposing a 
step - change in preservation system architectures”, 
Library Hi Tech, http://doi.org/10.1108/LHT-06-
2017-0122, truy cập ngày 17/10/2019.
7. Howe D. (2008). “The future of biocuration”, 
Nature 455, P. 47-50.
8. Kenneth Cukier (2010). “Data, data everywhere”, A 
special report on managing information, Economist 
Newspaper, Volume 394.
9. Kim Young Seok (2017). “Big data analysis of 
chernoff face method”, Journal of Documentation, 
Vol. 73, No. 3, P. 466-480.
10. Li Shuqing; Jiao Fúen; Zhang Yong; Xu Xia 
(2019). “Problems and Changes in Digital Libraries 
services”, Journal of Academic Librarianship, Vol, 
45.
11. Michael Cox, David Ellsworth (1997). “Application 
- Controlled Demand Paging for Out - of - Core 
Visualization”, Report NAS-97-010, NASA Ames 
Research Center.
12. Marydee Ojala (2018). “Big Data and AI: 
technology, transparency, and trust”, http://www.
infotoday.com, truy cập ngày 14/11/2019.
13. Randal E. Bryant, Randy H. Katz, và Edward D. 
Lazowska (2008). “Big - Data Computing: Creating 
in 
Commerce, 
Science and Society”, Computing Community 
Consortium, http://www.cra.org/ccc/initiatives, truy 
cập ngày 14/11/2019.
14. Simovic Aleksandar (2018). “A Big Data smart 
institution”, Library Hi Tech, Bradford, Vol. 36, Iss. 
3, tr.498-523
15. Steve Bryson, David Kenwright, Michael 
Cox, David Ellsworth, Robert Haimes (1999). 
“Visually exploring gigabyte data sets in real”, 
Communications of the ACM, Vol. 42, No.8, tr.83-
90.
16. Tan Jee Toon (2014). “Dữ liệu lớn, nhân 
tố thay đổi “cuộc chơi” của doanh nghiệp”, 
http://vneconomy.vn/cuoc-song-so/du-lieu-
lon-nhan-to-thay-doi-cuoc-choi-cua-doanh-
nghiep-20140422025542917.htm, truy cập ngày 
14/11/2019.
17. Waqar Ahmed, Kanwal Ameen (2017). “Defining 
the field of information and library management”, 
Library Hi Tech News, p. 21-24.
18. Ye Chunlei (2017). “Research on the key 
technology of big data service in university library”, 
the Institude of Electrical and Electronics Engineers, 
Inc. Conference Proceedings, Piscataway.
19. Yu Jen Chien (2016). “Library Data, Big Data or 
Better Data: Challenges from the Field”, ASIST 
Meeting, Proceeding of ASIST annual meeting, 
Vol 53, No. 1.
20. Zhan Ming, Widén Gunilla (2018). “Public 
libraries: roles in big data”, The Electronic library, 
Vol. 36, No.1, P. 133-145.
(Ngày Tòa soạn nhận được bài: 26-12-2019; 
Ngày phản biện đánh giá: 10-03-2020; Ngày chấp 
nhận đăng: 15-03-2020).
NGHIÊN CỨU - TRAO ĐỔI

Removed lines from big-data.pdf:
Adding Value to Manufacturing, Retail, Supply Chain, and 
Oklahoma State University, Stillwater, OK 74078 
The concept of big data has been around for many years.  Only in the last few years have organizations 
started to understand how they can use big data to gain insightful knowledge about their business operations, 
which is enabling them to make better business decisions. While there is no single definition, big data 
usually works on the principles of four Vs - Volume, Velocity, Variety, and Veracity. As the name suggests, 
big data is really big, meaning a huge amount of data is being generated daily, reaching the scale of 
petabytes. This data comes in all forms -  structured, semi-structured, and unstructured and is pouring in 
from all directions and generated by many systems and devices, such as transactional systems, log files, 
GPS devices, smartphones, RFID readers, surveillance cameras, sensor networks, Internet of Things (IoT), 
and social media. Finally, as big data becomes an important asset for enterprises, the focus is also on the 
trustworthiness of data and its sources. 
According to Gartner, Inc., “Big data is high-volume, high-velocity and high-variety information assets that 
demand cost-effective, innovative forms of information processing for enhanced insight and decision 
making.”a In this article, we first elaborate on the big data concept and present the storage and processing 
technologies that have been developed to deal with big data.  We then briefly discuss the evolution of 
traditional analytical processing to today’s big data analytics.   Through several applications and use cases, 
we illustrate how big data analytics is adding value to manufacturing, retail, supply chain, and logistics 
operations.  Finally, we conclude by discussing key challenges that businesses have to face as the use of 
big data analytics becomes more widespread. 
Regardless of the decision to be made - optimized production/work schedules, accurate forecasts, customer 
preferences - data nowadays has the potential to help businesses succeed more than ever before.  From an 
organizational perspective, big data is a holistic approach of obtaining actionable insight to create a 
competitive advantage over others.1 There are two distinct approaches to applying big data - improve the 
propositions. A challenge that organizations increasingly face is finding and working with trusted data. 
Working with inaccurate and untrusted data can be worse than having no data at all. As data requirements 
and regulations become more complex, organizations must be aware of where all their data is coming from, 
where it is getting stored, and who is interacting with this data as conclusions are drawn.2 
a https://www.gartner.com/it-glossary/big-data/ 
2 
What is Big Data? 
of data that needs to be handled and tracked, the speed at 
which the information is flowing into online systems, and 
regular basis. Because of the changes happening in the Web 
environment, new definitions for big data have been 
proposed, with a focus on technologies that handle this data. 
O’Reilly defines big data as “Big data is data that exceeds 
the processing capacity of conventional database systems. 
The data is too big, moves too fast, and doesn’t fit the 
structures of traditional database architectures. To gain 
value from this data, organizations must choose an 
alternative way to process it.”3 
To understand how big data is transforming businesses, we 
focus on the size of data in storage.4 Size is important but 
there are other aspects to big data namely variety, volume, 
and more recently, veracity.2 Together they are called the 4 
Vs of big data: Volume, Velocity, Variety, and Veracity. 
databases, data warehouses, and data marts.6 Here, the data 
is uploaded to operational data stores using Extract, 
Transform, and Load (ETL) tools which extract data from 
internal and external sources, transform the data to fit the 
operational needs, and finally load the data into the data 
warehouse. The key point is that the data is getting cleaned, 
transformed, and cataloged before being made available for 
data mining and online analytical functions. This traditional 
data warehouse approach discourages the incorporation of 
new data sources until they are cleansed and integrated.  
Since data is ubiquitous these days, big data storage 
environments need to be “magnetic” in nature, attracting 
data from all sources. Hence, big data calls for Magnetic, 
Agile, and Deep (MAD) analysis skills, which differs from 
the traditional data warehousing approach. Given the growing number of data sources and the sophisticated 
tools for data analysis, big data storage should allow analysts to easily process and use data rapidly. 
Solutions like distributed file systems and Massive Parallel Processing (MPP) databases are available 
nowadays for providing high query performance and platform scalability. Non-relational databases such as 
Not Only SQL (NoSQL) were developed for storing and managing unstructured data.7 These newer 
technologies aim for scalability, data model flexibility, and simplified application development and 
deployment. They separate data management and data storage and focus on high performance scalable data 
Volume. The ability to process a large 
social media, from Internet of Things to 
system logs, etc. 
Velocity. The rate at which data is 
getting created every second of the day. 
contributor, 
more 
data 
is 
generated and logged than ever before.5 
Also, the rapid adoption of social 
created a deluge of data. Advances in 
useful now. 
Variety.  It is the diversity of data 
which organizations are witnessing. 
processing a limited set of data, such as 
and 
logs. 
includes images, voice recordings, 
videos, and texts generated from 
media to deliver new insight. 
Veracity.  It is not just the quality of 
data, but also the trustworthiness of 
data sources.  Basic issues are the 
accuracy and applicability of data.  
uncertainty due to inconsistencies, 
incompleteness, ambiguities, etc. 
3 
storage, allowing management tasks to be written in the application layer instead of having it written in 
database specific languages.   
Why Big Data? 
When organizations adopt big data as a part of their business model, the first tangible question is usually 
what value this big data will provide to the company.7 Data must be used to make better decisions, to 
optimize resource consumption, and improve process quality and performance. It should also aim to 
perform precise customer segmentation, optimize customer satisfaction, and increase customer loyalty. 
from existing products and create additional revenue from new products. 
Newer Data Sources, Newer Opportunities 
The new sources of big data include industries which are taking a big step step towards digitization, and as 
a result, data growth in the past few years has been phenomenal. Some of the areas where data is coming 
from include social media, internet browsing pattern data, advertising response data, financial forecasts, 
location information, driving patterns, vehicle diagnostics, and traffic and weather data from sensors, 
monitors, and forecast systems. Other sources of data include data from healthcare, where the healthcare 
industry is implementing electronic medical records and digital imaging, which is used for short-term public 
health monitoring and long-term research programs.  Similarly, low cost gene sequencing can generate 
effectiveness in life sciences.8 Another area is data from video surveillance which is transitioning from 
patterns for security and service enhancement. Transportation and logistics industry has been generating 
and storing enormous amount of data coming from sensors, GPS transceivers, RFID tag readers, smart 
meters, cell phones, material handling equipment enabled with sensors, etc. This data can be used to 
opportunities. 
information contained therein.9 It involves applying algorithmic processes to derive insights. Analytics is 
used to extract previously unknown, useful, valid, and hidden patterns and information from large data 
sets.6 While the focus of analytics has been on inference, it can also provide prescriptive insights as 
explained later in this section.  Hence, analytics has a significant impact on research and technology, as 
businesses recognize its great potential in helping them gain competitive advantage. 
“Big data analytics is the use of advanced analytic techniques against very large and diverse data sets that 
include structured, semi-structured, and unstructured data from different sources, and in different sizes from 
terabytes to zettabytes.”b It helps in uncovering hidden patterns, unknown correlations, market trends, 
customer preferences and other useful information. Advanced analytics can help organizations discover 
what has changed and how they should react.  Analytics is the best way to discover new customer segments, 
identify the best suppliers, associate products of affinity, understand sales seasonality and so on.4 
Organizations are implementing specific forms of analytics tools and techniques which include data mining, 
statistical analysis, data visualization, artificial intelligence, machine learning, and other data capabilities 
b https://www.ibm.com/analytics/hadoop/big-data-analytics 
4 
which support analytics4. Though these techniques have been around for many years, organizations are 
using them now as most of these techniques adapt well to very large, multi-petabyte data sets. 
Big data’s worth is only realized when businesses can indulge in decision making using this data. To enable 
such data-driven decision making, organizations must use efficient processes to turn the high volume of 
fast moving and diverse data into meaningful insights. Analyzing big data allows researchers and businesses 
harness their data and use it to identify new opportunities which in turn leads to better and smarter business 
moves, more efficient operations, higher profits and satisfied customers and an overall competitive 
advantage.6 Big data analytics could be viewed as a sub-process in the complete process of knowledge 
extraction from big data.  
As organization began to adopt data analytics in the late 1990s and early 2000s, they faced many hurdles.  
professionals. Analysts used to spend more time collecting and preparing data than analyzing it.  They 
focused on finding more accurate and reliable solutions to business problems, while keeping the solutions 
simple at the same time so that business users could understand it.  Some examples of tools used during 
this time period are SAS, a tool for building backend data inference and modeling; Oracle and Teradata, 
detailed solution suites for easy development of solutions; IBM CPLEX, a tool for solving large 
optimization problems; and Cognos and MicroStrategy, tools for visualization, mostly in the form of 
reports. 
In late 2000s, social media giants like Google and Facebook and other internet-based companies in general 
started uncovering, collecting, and analyzing newer types of data which later evolved into big data. In 
addition to the data generated by companies in their internal operations and transactions, newer data was 
brought in from external sources including public data sources, social media, and mobile devices. Analysts 
realized this new data was qualitatively different (e.g., unstructured text, pictures, audio, and video) along 
with the much larger volumes as compared to internal company data.  This led to the development of newer 
tools and technologies, examples of which are Hadoop, a pioneer in distributed data storage and processing 
with low cost, flexibility, and scalability; Python and R, open source programming languages with vast and 
ever-evolving libraries for statistical data analysis; Tableau, Looker, and Microsoft Power BI, popular 
visualization products to develop, customize, and build visually appealing and interactive web dashboards. 
Descriptive, Predictive, and Prescriptive Analytics 
Analyzing data is not limited to deriving insights from the past, but it can also help businesses in predicting 
future outcomes and optimizing business performance. Currently organizations use three types of analytics 
at different stages in their decision-making process - Descriptive, Predictive, and Prescriptive analytics as 
shown in Figure 1. The latter two are also referred to collectively as advanced analytics. 
Descriptive analytics does exactly as the name suggests, ‘describe’ or summarize the data and convert it 
into something useful. It is the most basic type of analytics and almost 90% of the organizations today use 
this technique. Descriptive analytics is the analysis of historical data using data aggregation or data mining 
and lies at the bottom of the big data analytics value chain. However, it is extremely valuable because it 
organization’s future.  
Descriptive analytics is an important step to make raw data understandable to its users, and it helps in 
answering questions like “What is happening?” Consider for example, a metric that companies get from 
5 
web servers using Google Analytics tools, namely page views.  It can be used to determine if a strategy was 
a success or not. The main objective in descriptive analytics is to find the reasons behind the previous 
it can help the organization in strategizing.  
The majority of the statistics we use comes from descriptive analytics – e.g., calculations as simple as 
averages and standard deviations. Descriptive models use basic mathematical and statistical techniques to 
derive key performance indicators that can highlight the historical trends in data. STATA, MS Excel, and 
SPSS represent the older generation of descriptive analytics tools, while R and Python are quickly becoming 
the preferred tools in industry because of vast open-source libraries and the ease of development and 
deployment. Descriptive analytics can yield historical insights into an organization’s production, inventory 
levels, sales, operations, financials, and customer behavior.  
Figure 1. Analytics Framework by Tom Davenport26 
Predictive analytics can be defined as the ability to “predict” what might happen and a better understanding 
of future outcomes. It is one of the more sophisticated types of analytics techniques and employs statistical 
techniques and machine learning. It is used to detect clusters, tendencies, and exceptions, and to predict 
future trends, making it a valuable tool for forecasting. The foundation of predictive analytics is probability. 
It takes the data which the user has and tries to fill in the missing data values with best guesses. It helps in 
finding the answer to ‘What could happen?’ With properly tuned models, predictive analytics can support 
complex forecasting in marketing and sales. This helps an organization to set realistic goals for business, 
restrain expectations, and do effective planning. 
Tools used to apply predictive modeling vary by the nature of model’s complexity, but some commonly 
used tools are SAS, MATLAB, R, Python, among others. The common functionality of these tools is that 
they combine historical data found in POS, ERP, CRM, and HR systems to identify patterns in the data and 
apply algorithms such as random forest and Generalized Linear Model (GLM) for prediction, and K-means 
clustering for identifying clusters. Finally, simulation can be employed to statistically predict the outcomes 
of specific decision scenarios.   
6 
An application of predictive analytics is to produce credit scores, which are used by financial services to 
determine the probability of customer making timely payments. Other business uses include, how sales 
might close at the end of a year, inventory level forecasts, predicting what items a customer might purchase 
together and other customer purchasing patterns. Despite all the advantages that predictive analytics brings 
to the table, it is important to understand that forecasting is just an estimation, and its accuracy depends on 
the quality and stability of data.  
Prescriptive analytics is the most sophisticated analytics approach which makes use of optimization 
techniques to explore a given set of options and prescribe the best possible solution for a given scenario. 
As the name suggests, it “prescribes” a solution to a specific problem. One approach is machine learning 
neurons using training data sets.  Once trained, the neural network model can suggest the optimal course of 
action supporting the business objective for a given set of business inputs. Simulation, a predictive analytics 
tool at its core, can also be part of a powerful prescriptive analytics approach when combined with 
appropriate search or optimization techniques.  Prescriptive analytics not only predicts ‘What will happen?’, 
but also determines “What the company should do?” It provides recommendations for the actions to be 
taken to achieve optimal business performance. Because it has power to suggest optimal solutions, 
prescriptive analytics is the ultimate frontier for advanced analytics. 
Prescriptive analytical models are complex in nature. However, when implemented efficiently, prescriptive 
analytics can have a significant impact on the decision-making effectiveness of the organization. Technical 
advancements such as cloud computing have made deployment of these complex models much easier. 
prescriptive analytics to optimize production and inventory decisions in supply chains, optimize customer 
experience, and to make sure that the right product is being delivered at the right time.  Airline systems use 
travel factors, demand levels, purchasing patterns, timings, etc., in order to maximize the revenue generated.  
hence, they are ensuring to choose the right kind of analytics solutions to reduce operational cost, enhance 
service quality, and increase ROI. 
Big Data Analytics Applications and Use Cases 
Supply chain activities produce a huge amount of data, which is being continuously generated by systems 
and devices such as POS, ERP, SCM, RFID, GPS, blogs, and wiki entries, not to mention the unlimited 
data generated from sources like CCTVs, digital clickstreams, imagery, social media posts, and discussions 
on various forum platforms. Advanced connected devices and technologies which support today’s supply 
chain such as sensors, smart devices, and tags are continuously gathering real-time data and providing an 
end-to-end visibility in the supply chain. It becomes the task of supply chain managers to tap and process 
this data to make insightful decisions which could help boost productivity and reduce costs. 
7 
Application 
Area
Technique/Technology/
Supplier Insight Program Greater insight into suppliers' financial stability, 
performance, and ability to provide services.
Achieved product excellence, reduction in time-to-
market through co-development and co-production. 
Better alignment between engineers, suppliers, and 
customers.
Better service level; accurate prediction of customer 
needs and demand; automated planning and 
forecasting operations.
Inventory management with streaming analytics, real-
time data delivery and updates every few hours, and 
accurate performance analysis of each store.
Machine learning-based 
accuracy and greater profit margins.
application
Accurate forecasts, reduction in delivery time by 
upto 50%, and better service levels.
& Co.
Analytics application 
using Intel's Trusted 
Better tracking of in-store items using RFID tags; 
updating item location and inventory; helping 
salesperson track misplaced item to avoid lost sales.
Data-intensive 
Increase in forcasting accuracy; reduced inventory, 
stockouts, and obsolescence; better access to 
company's logistics needs.
Anticipatory shipping
before actual customer orders.
and packing area.
Drone-based delivery
Goods delivered to locations less than 30 minutes 
Cloud-based 3D 
Optimize picking accuracy, inventory turns, and 
warehouse productivity in real-time using inputs 
from sensors, such as  shelf weight and weight on 
forklift.
Quality early-warning 
Reduced rework, increased productivity and cost 
savings, higher quality standards, and improved 
service levels, by detecting and prioritizing quality 
related issues much sooner in the supply chain.
Greater demand and supply visibility, better 
distribution channel management, better service level, 
and improved inventory management.
collection.
Co.
Real-time monitoring and 
Greater visibility for customers, better pallet 
management, optimized space utilization, greater 
labor productivity, inventory accuracy of 99.9%, and 
improved customer satisfaction.
UPS
Optimized 55,000 delivery routes in North America, 
saving close to $400 million annually. Reducton in 
transportation.
shipment information, reduction in mileage and cost, 
and improved CO2 efficiency.
Resilience 360
Accuracy in risk detection, prevent production 
inefficiencies and revenue losses, maintain service 
levels, and reduce emergency cost by efficiently re-
routing shipments in case of unforeseen events.
analyze potential business opportunities. Real-time 
in a given location.
quality of address information is poor. Real-time 
ddress verification to optimize route planning.
DHL
Applications of Big Data Analytics
IBM
Supply Chain 
and 
8 
Applications and Use Cases in Manufacturing 
Raytheon, a major U.S. based defense contractor and industrial corporation, made use of data analytics to 
reduce costs within their supply chain and production operations. They developed a Supplier Insight 
program, which integrated structured and unstructured data from internal and external sources.10 With more 
than 10,000 suppliers, they needed a platform that could provide rapid, data-driven decision making 
capability. With this new system, they could track suppliers’ financial stability, performance, and their 
ability to provide services in the face of disruptive events. Raytheon was able to immediately identify if a 
supplier could provide what they needed, and quickly made decisions that reduced any adverse impact on 
their customers. Supplier Insight has allowed them to negotiate the cost better, by engaging in long-term 
contracts with suppliers for multiple programs.10 They now have an ability to look across all their suppliers 
and programs to achieve cost reductions. Raytheon has also developed smart factories which have the 
capacity to handle big data coming from different sources like sensors, instruments, CAD models, internet 
transactions, simulations, and digital records in the company, which equips them with real-time control of 
various elements of the production processes. For example, their Immersive Design Center (IDC) makes 
use of a 3-D immersive environment to achieve product excellence and decrease time-to-market through 
co-development and co-production of products by immersive data visualization and interaction.10 This also 
resulted in better alignment between their engineers, suppliers, and customers. They work together to refine 
the design and detect potential problems without the work and rework associated with expensive prototypes, 
resulting in reduced costs.10  
Lennox International, a U.S. based cooling and heating devices manufacturing company, integrated 
their expansion throughout North America.11 With the help of machine learning algorithms, they accurately 
predicted customer needs, while understanding customer demand better. It also helped the company to 
automate its planning and forecasting operations. 
Many companies gather data on supplier information and purchasing volumes for annual supplier 
performance review, spend analysis, and cost savings analysis functions to support strategic decisions.  For 
example, a pharmaceutical company created a database of all the bids submitted for packaging.12 This data 
was then evaluated to understand the cost structure of suppliers and to create detailed cost models for 
different packaging options. Such models can help in the selection of the most cost-effective supplier for 
new packaging.12 Another example is how IoT with its network of sensors embedded in millions of devices 
can enable new opportunities in manufacturing. For example, real-time information on a machine’s 
condition can initiate a production order for a spare part, which then can be shipped using a drone to the 
plant engineer for replacing the faulty or near faulty part.12 It also helps in determining when and how 
critical maintenance is required by a specific machine, thereby avoiding costly equipment breakdowns and 
improving the overall production efficiency.  
Daily production needs to be monitored to maintain the efficiency and output of a company. Big data 
analytics uses the data collected from operational machines, employee records, and data logs of the number 
of units produced, to provide insights to the operations manager, helping him/her to make changes that are 
profitable for the company. Manufacturers are also exploring predictive analytics to realize significant 
savings in product testing and improving product quality. Since different products and parts require 
different tests, instead of performing numerous quality tests on each part, data mining and pattern 
recognition can be used to determine the type and number of tests truly needed for each part or product.13  
9 
Applications and Use Cases in Retail 
Walmart, the number one fortune 500 company, has the world’s largest private cloud, which helps support 
real-time data feeds to its decision makers. Walmart’s Data Café based at their Bentonville, Arkansas 
headquarters takes care of most of this cloud architecture.14 Their original data infrastructure only enabled 
managers to get weekly reports, which prevented them from making decisions based on real-time market 
conditions. Also, the reports were standardized with little room for customization. Data café, which was 
built on SAP’s HANA in-memory analytics engine, enabled inventory management with streaming 
analytics, and provided an enterprise view of timely information flow for a large cross-sectional staff 
looking to resolve every-day business issues.14 The data delivered through this system is almost real-time 
and updated every few hours.  Furthermore, the system was designed to be responsive to providing reports 
and queries required by managers in the given time frame, which helped them gain timely insight and make 
better decisions. These insights are derived from “200 streams of internal and external data which includes 
40 petabytes of recent transactional data, and can be manipulated, modeled, and visualized.”14 The 
importance of near real-time insights is crucial since it helps managers respond to challenges in real-time 
as they arise. For example, on Black Friday, Walmart’s Data Café provides near real-time insights on the 
performance of east-coast stores, which enables Walmart to make pricing adjustments for west-coast stores 
before they open.14 During a recent Halloween, sales analysts were able to see that two stores were not 
selling a novelty cookie that was very popular in most stores. Using near real-time data from Data Café, it 
was discovered that simple stocking oversight led to the cookies not being put on shelves in these stores.14 
The company was able to react in real-time to avoid additional lost sales. Data Café also provides automated 
alerts to managers when a metric falls below a threshold in a department. This tool has reduced the problem-
solving time from weeks to minutes using reliable internal and external sources of data. 
Levi Strauss & Co, a leading American clothing company, provides better in-store shopping experience for 
items using IoT technology coupled with advanced analytics. Levi’s in collaboration with Intel® 
implemented a solution using Intel’s Trusted Analytics Platform (TAP), which helped salespersons to 
quickly find misplaced items in the store.15 This application made use of RFID tags woven into clothing 
items, in-store antenna sensors installed in the ceiling of the store to continuously track the RFID tags, and 
cloud-based analytical tool built on TAP for detailed analysis. This technology helped determine when 
items are no longer in their correct place or no longer available at that time. TAP algorithms use data 
collected overnight to determine the exact location of various groups of items, and during store hours 
sensors track the location of items and an algorithm determines if an item is in its assigned location. If an 
item is placed in its assigned group location, no action is generated by the algorithm.  Suppose a pair of 
jeans is lying in the T-shirt section or left in the fitting room, the TAP algorithm will generate an alert on 
the mobile application instructing the salesperson to put the item back in its assigned location.15 This helps 
the salesperson to keep the item where it belongs and avoid lost sales. Levi’s also aims to generate customer 
insight using big data analytics with the data collected from sensors tracking customers’ in-store behavior 
to better understand their preferences.15 
Groupe Danone, a French multinational food-product corporation, found itself making accurate predictions 
only 30 percent of the time for responses to promotional offers, which was resulting in significant losses to 
the company.11 When they implemented machine learning in their planning architecture, they saw 
significant improvement in both sales and forecasting. Similarly, Granarolo, an Italian dairy company, used 
machine learning to increase its forecasting accuracy by 5 percent, decreased delivery times by up to 50 
percent of the original time, which resulted in better service levels.11 Morrisons, one of UK’s largest food 
10 
retailers, was able to dramatically improve same store sales and achieve a 30% reduction in shelf gap and 
from Blue Yonder, which uses AI technology to “improve demand planning and reinvigorate replenishment 
based on customer behavior in every store.”16 Blue Yonder’s data-intensive forecasting methods deployed 
as cloud-based services is making such advanced capabilities accessible to other retailer’s as well.12 
Applications and Use Cases in Supply Chains and Warehouses 
In supply chain operations, planning and forecasting are among the most data-driven operations, which use 
an array of supply chain planning tools supported by ERP systems. With the use of supply chain analytics, 
it is now possible to re-envision the planning processes by using external and internal data sources to make 
real-time decisions based on market trends, uncertainty, seasonality, and other fluctuations.  
IBM understood the value of big data analytics early and employed it in optimizing their supply chain 
operations. They have used various analytical tools to solve a range of problems, and a few of them are 
discussed here.17 IBM’s Quality Early-Warning System (QEWS) was typically deployed upstream at 
suppliers, IBM’s operations, and in the field.  QEWS detects and prioritizes quality related issues much 
sooner than the traditional quality control processes. Analyzing big data coming from across their supply 
chain, IBM was able to reduce rework, increase productivity, ensure higher quality standards, and improve 
customer satisfaction, leading to significant cost savings. For a company like IBM, ensuring correct 
inventory levels with so many business partners was challenging. They made use of IBM Buying Analysis 
Tool, which not only provided demand and supply visibility, but ensured better distribution channel 
management, delivery of the right product at the right time to meet customer demand, while maintaining 
proper inventory levels. IBM also used a tool named Accounts Receivable, which uses advanced analytics 
to optimize the resources needed to collect revenues. They also make use of supply chain social listening, 
disrupt the supply chain.17 It also helps them obtain timely information and feedback on their products. As 
an early adopter, IBM has been using predictive and prescriptive analytics in its supply chain over the last 
several years. 
Warehousing is another area where big data analytics is creating new opportunities.  Logivations, a German 
supply chain solutions provider, offers a cloud-based 3D warehouse layout planning and optimization tool, 
camera-guided AGVs and tracking, and various other supply chain analytics solutions.18 Such technologies 
existing warehouse by simulating new configurations. Another example is the analysis of images and videos 
captured by AGVs, and sensor inputs including shelf weight and weight on the forklift, to monitor picking 
accuracy, inventory turns, and warehouse productivity in real-time.12 Also, forklift drive picking 
productivity and route optimization can be achieved by analyzing the route choices and driving behaviors.12  
A leading forklift provider is looking into all these opportunities, and figuring out how a forklift truck can 
be used as a big data hub - collecting  real-time data to identify additional sources of waste in the warehouse 
operations, using a hybrid of analytics and ERP and WMS data. Amazon is another warehouse automation 
pioneer, deploying Kiva robots that bring the items (racks) to the picking and packing area in their 
fulfillment centers. With increasing pressure to reduce order-to-delivery times, warehouses are turning to a 
flexible automation strategy by using autonomous technologies such as Amazon’s Kivac robots and 
GreyOrange’s Butlerd system to increase their picking efficiency.  Amazon has also tried to deliver goods 
c https://www.amazonrobotics.com/#/  
d https://www.greyorange.com/butler-goods-to-person-system  
11 
to people living less than 30 minutes away from an Amazon warehouse or distribution center via a drone. 
Amazon has also patented an “Anticipatory Shipping” technology to identify which orders should be 
packed and pushed into the logistics network before the actual customer orders are placed.12 
Merchandise Warehouse Co. (MW), a logistics provider of multi-temperature warehouse services in the US 
mid-west, provides services such as tempering, inspection, blast freezing, temperature monitoring, labeling, 
import/export, and packaging.19 With such operations there is little room for error, since clients’ food 
products could get spoiled if they are not maintained at correct temperatures.19 MW needed real-time 
enable quality assurance with comprehensive traceability.  They wanted this for all operations including 
inspections and holds.19 Technologies such as CCTV, WMS, electronic data interchange, mobile 
computers, and scanners were employed to help track and analyze data to get real-time information in the 
warehouse and manage inventory. It helped MW’s customers gain visibility by having on-line access to 
temperatures, activity reports, and information about inventory levels. MW’s solution also includes tools 
for pallet management for tracing every pallet from the time it arrives in the warehouse to until it leaves. 
inventory to marked temperature zones were provided by the new system. It also ensures greater labor 
productivity and accuracy using workflow-based warehouse management and could automate processes 
designed for specific customer needs. MW reaped various other benefits from this initiative like accurately 
capturing billing events in real-time resulting in reduced labor used for billing and paperwork. The system 
helped the company deal with the issue of “catch weight”, where the actual weight of the product,  especially 
meat, varies when it hits the retail shelves, a common problem in cold storage warehouses and food 
industry.19  Increased customer satisfaction levels were also achieved, since clients had real-time access to 
information and reports when needed. The solution helped MW achieve an inventory accuracy of 99.9 
percent from a previous 98.6 percent.19 
Applications and Use Cases in Logistics 
Logistics companies need to keep the goods moving at all times, even in the face of disruptions such as 
storms, cargos getting stranded due to ship crashes, and geopolitical events in order to keep the businesses 
running.  A Netherlands based logistics management company uses big data analytics on Microsoft’s Azure 
cloud to keep its customers informed about the number of goods in each container, their location at a given 
time, and expected delivery times.20 Purchase orders are tracked using mobile applications to identify 
challenges which could delay the delivery of an order. Tariff calculations and fees related to the movement 
of shipping containers are calculated by another application which can be accessed by the client, giving 
them a greater insight into financial risks.20 These mobile applications make use of big data analytics in 
internal supply chain operations to provide actionable business insights. Previously the time it took to 
identify a challenge and develop a solution to address it could be anywhere from 3 to 9 months. With the 
use of big data technologies, this time has been brought down to a couple of weeks depending upon how 
complex the problem is.20  
Companies managing their own supply chains and those outsourcing to third-party logistics providers 
manage a massive flow of freight, goods, and products daily while at the same time creating vast data sets. 
Millions of shipments are tracked daily from origins to destinations, generating information such as the 
content, weight, size, location, and route of each individual shipment, across a large number of networks. 
Companies are exploiting and analyzing these large data sets to improve their operational efficiencies, 
effectiveness, and customer service. A study by the Council of Supply Chain Management Professionals 
shows that 93 percent of shippers and 98 percent of 3PL providers feel that data-driven decision making is 
12 
a crucial supply chain activity.21 Also, 71 percent of these believe big data improves performance and 
quality. Logistics companies can utilize big data analytics to consolidate, interpret, and store the data 
coming from various sources for immediate or future use based on their requirements.  
Courier and delivery companies like UPS use real-time routing of deliveries using the trucks’ geo-location 
and traffic information data. UPS spent almost 10 years developing its On-Road Integrated Optimization 
and Navigation system (ORION) to optimize close to 55,000 routes in North America in its delivery 
network.22 This system saves the company $300 million to $400 million annually by saving about 100 
million miles per year, which is a reduction of 10 million gallons of fuel consumed and reducing CO2 
emissions by almost 100,000 metric tons.22  Data mining techniques also help logistics companies deliver 
services with fewer delivery attempts, by using predictive analytics to predict when a customer is more 
likely to be available at home.12 Costs and carbon emissions can also be reduced by selecting the right mode 
of transportation for deliveries. An example is the use of supply chain analytics to understand the priority 
and can be delivered by rail.12  
Better transportation planning can be achieved with the use of Transportation Management System (TMS) 
which can help identify future shipping patterns, optimize routes, carrier selection, or loads, and secure 
necessary capacity. This is achieved by tracking shipment frequency and identifying the endpoints of supply 
chains by studying precise inbound and outbound statistics. Direct application of predictive analytics is 
helping logistics providers make real-time decisions which result in reduced costs, greater reliability, and 
improved customer satisfaction. For example, data streams produced by sensors on delivery trucks, beacons 
which broadcast their presence to nearby devices such as computers and smartphones, radar devices, and 
employing simulation models.23 When a shipment is going to be late, a carrier can make real-time 
adjustments to prevent bottlenecks further down the supply chain.23 
DHL, a global logistics provider, has extensively explored big data analytics in their supply chain activities 
and is currently employing several smart systems around their services. Increasing the last mile efficiencies 
is often the most expensive step in the supply chain.24 Last mile optimization is an extensively studied area 
and researchers have found promising applications of big data analytics here. Data analytics is applied to 
achieve real-time optimization of delivery routes, where streams of data are processed to maximize the 
performance of the delivery fleet. Rapid processing of real-time information supports the goal of route 
optimization on the last mile, saving time in the delivery process. When the vehicles are loaded and 
unloaded, manual sequencing of shipments is eliminated by the use of sensors, and dynamic calculations 
are used to find the optimal delivery sequence. Based on real-time traffic conditions on the road, telematic 
databases are used to change the delivery route automatically. DHL’s SmartTruck uses data mining, 
machine learning, and other data analytics techniques to optimize the initial tour planning based on 
incoming shipment on a daily basis.24 Dynamic routing system recalculates the routes depending on the 
traffic situations and delivery times. This also results in cost reduction and improved CO2 efficiency by 
reducing the miles travelled. 
It is vital for robust supply chains to be able to cope with unforeseen events in today’s rapidly changing 
world. Apart from being flexible and resilient, businesses need accurate risk detection systems to keep 
running smoothly. Big data analytics and complex event processing algorithms are used to alert businesses 
when a pattern falls in the set of critical conditions such as tornadoes or floods in an area, or breakdown of 
fleet. These alert systems send a report on the probability and impact of the risk and provide suitable 
actionable insight to alleviate potential interruption. With this information on hand, customers can re-route 
their shipments or manage supplies from other distribution locations. DHL’s Resilience 360 risk 
13 
management solutions aims to provide such functionalities.24 It is equipped with two components, a risk 
assessment portion and supply chain monitoring instruments, both operating in real time. This improves the 
resilience of the supply chain and prevents production inefficiencies and revenue losses. Resilience 360 is 
designed to maintain prescribed service levels, protect sales and operations, and reduce emergency costs, 
creating a competitive advantage for the company.24 
Future economic development is often modeled on global transportation of goods and services. The type of 
goods shipped indicate the local demand and supply preferences. Logistics providers make use of big data 
shipments by their distribution networks. These shipment records are a valuable resource for market 
intelligence research, and logistics providers refine this data to substantiate existing market research. 
Regression analysis techniques are used to produce demand and supply forecasts with the use of the 
shipment records and market research outcomes. The primary target group for these advanced data analytics 
services are small and medium-sized enterprises, which lack capacity to conduct their own market research. 
The results from regression-based analytics have high predictive value, which can help these enterprises 
serve a larger customer base, and generate accurate forecasts based on industry, geography, and product 
category. DHL Geovista is one such online geo-marketing tool available for small and medium-sized 
enterprises to analyze potential business opportunities.24  
shipments more accurately.24 Customer’s delivery address verification is a fundamental requirement for 
any logistics provider. This can be troublesome in developing countries and other remote areas, where the 
area. Address Management uses daily freight and parcel delivery data and matches this data with reference 
data and returns the incorrect incoming data with validated data from the database, in order to verify the 
address in real-time and optimize route planning for retailers and public sector entities.  
Other Applications 
There are several other applications of big data analytics which a company can encounter on a regular basis. 
Locating a new store is a strategic decision for a company, and big data analytics could play an important 
role here. Extensive data analysis is performed by the analysts in exploring customer data, demographic 
factors, retailer network, location of other competitors in the area, and market potential. A recent example 
of this is the location for Amazon’s HQ2. Visualizing the growth of a company has become easier with the 
use of data analytics, since it is now possible to quickly compare the performance matrix of different sites 
and identify the reasons behind such results. Predictive analytics comes in handy in analyzing the market 
and gaining insight on questions related to global growth strategy, site relocation, new product introduction, 
and supplier selection. 
business profitable.25 Data analytics tools simplify the process of price formation, which not only accounts 
for the cost of production of an item, but also the spending capacity of the customers and presence of 
competitors in the market. Price flexibility, buying patterns of the customers, competitors’ prices, and 
seasonality are analyzed using the data coming from various sources. Machine learning algorithms help 
to changes in prices. Furthermore, using real-time price optimization techniques, retailers can attract new 
customers and retain existing customers by adjusting the price as per market trends. Recommendation 
engines is another great way of predicting customers’ behavior, since they give a retailer insight into 
customers’ reviews and opinions. It also helps the retailers to increase sales and stay abreast with trends. 
14 
Based on machine learning algorithms, recommendation engines make adjustments depending on customer 
preferences, previous shopping and browsing experience, demographic data, need, and usefulness. 
Collaborative or content-based data filtering is used in this process to gain useful insight which gives 
leverage to retailers on customers’ opinions.  
Companies often fail to understand what big data is, its benefits, and more importantly the computing and 
the human infrastructure required to realize its true potential. Without a clear understanding of the concept 
of big data, adopting and implementing a project using big data tools can seriously challenge its success. 
Having discussed various applications and use cases of implementing big data technologies in 
manufacturing, retail, supply chain, and logistics, it is important to understand the associated challenges. 
they decide to invest in technologies using big data.  
reducing their dependence on legacy systems. Even though the industry is shifting its focus to the digital 
age with adoption of IoT and artificial intelligence, it is still a long way before the full potential of big data 
is realized. Industry has to develop an awareness of the various elements of the big data landscape, which 
include sensors to social media that collect data, in-memory to cloud for data storage, data mining to deep 
learning to convert data into useful business insights or actions.  Any new business solution will involve a 
significantly. Most people are resistant to change, and it shows in companies when workers stick to to an 
old way of thinking and doing work. An example is the use of Excel, which to the present day remains one 
of the popular tools in many companies, despite having  many limitations when compared to newer tools.27 
While there is a need to educate industry to change this legacy mentality, there is no need for an abrupt or 
complete shift to newer tools. A viable option is to slowly augment existing systems with big data analytics 
tools and capabilities.   
With the phenomenal increase in the size of data, the problem of storage space for big data has become a 
real problem for many companies. Cloud storage is soon becoming the only viable alternative with the ever-
increasing need for storage space.  With the maturity of the cloud computing infrastructure, which includes 
storage, applications, and computing platforms, companies are beginning to consider shifting to the cloud 
infrastructure for most of their computing needs. But transitioning from the traditional in-house computing 
infrastructure to the cloud infrastructure has its own challenges.  According to McAfee, “Most organizations 
that have been around awhile have a hodgepodge of hardware, operating systems, and applications, often 
described as ‘legacy spaghetti’.”28 First, companies have to address legacy system issues and simplify their 
system before moving to the cloud. For the most part, cloud is cost-effective compared to building and 
running an IT infrastructure.  However, a company needs to carefully evaluate the cost factor based on their 
specific needs, for example, in-house applications requiring continuous transfer of large data sets. 
analytics with specialized MS degrees in Data Science.  These degree programs are housed mostly in 
business schools or computer science departments.  Engineering schools to a large extent are still lagging 
in providing adequate training in data science to their graduates.   Data science professionals can manage 
and analyze large volumes of real-time data coming from multiple sources and in different formats. With 
several new technologies such as the NoSQL data management framework, Hadoop, cloud computing, and 
in-memory analytics, their skills are vital for the rapidly changing computing landscape. Given that 
engineering schools are still looking for the right curriculum mix (e.g., minors, degree options, and 
15 
certificates) to train engineers in data science, training employees at entry level is a challenging and 
expensive proposition for companies dealing with these newer technologies. When industry hires data 
science professionals, akin to software developers and programmers, they need guidance from subject 
matter experts (SMEs) to build the right tools and techniques that can help industry harness the power of 
big data in the long-run.  Industry needs to quickly educate SMEs to understand the big data analytics 
professionals.   
As seen in recent times, data privacy has become one of the major concerns of organizations. With recent 
threats like hacking of personal data, individuals and companies have become apprehensive about linking 
data from multiple sources as it may compromise an individual’s privacy. Also, with an increase in the 
number of connected devices within the industry, data security has also become a big concern and presently 
this risk is greater than ever. Big data analysis uses huge amounts of data for analysis and mining purposes 
to reach some meaningful conclusion, and security of this big data can be enhanced by using techniques 
such as authentication, authorization, and encryption.   
Effective flow and sharing of information among supply chain partners is critical to the success of today’s 
digital supply chains.  Unauthorized disclosure and data leakage of information shared among supply chain 
partners have been identified as two main threats in today’s digital supply chains.29 Visibility needed within 
a supply chain and consumers’ demand for transparency seem to be at odds with security requirements.  
With newer, secure technologies such as blockchain and data cleanroom, it is possible to achieve both 
visibility and transparency.30  Data cleanroom is a shared environment between two or more supply chain 
partners that is completely secure from external access and where each partner can decide the level of 
visibility to their data. Blockchain, a decentralized, distributed database is one of the most secure options 
available for supply chain partners for real-time information tracking.  Another important, but often 
overlooked challenge is the ethical use of data.  The legal infrastructure has not kept up with the rapid 
development in technology, which is able to collect and store vast amounts of consumer data with or without 
their knowledge.  While it may be legal, certain use of the data may be considered unethical.  Such actions 
may have a negative impact on a company as today’s consumers are more educated and have experienced 
negative consequences of such unethical usage. 
In a recent survey of supply chain professionals conducted by APQC, “lack of people with the needed 
skills” was identified as the biggest barrier to advanced analytics applications in industry.31  In addition, 
these employees need “a good understanding of the business to provide solid advice.”29  Resistance to 
change and lack of access to data across disparate systems were the second and third biggest barriers, 
respectively.  In addition to lack of access to data, issues such as inconsistent and unorganized data are also 
issues in some cases as different companies record their data in different formats, platforms, and systems.27 
useful insights.  
As companies make a push for big data analytics applications, they should first establish a clear business 
need such as “solving a problem or seizing an opportunity.”7  According to Watson, “big data initiatives 
should start with a specific or narrowly defined set of objectives rather than a ‘build it and they will come’ 
approach.”7 Pilot schemes are a good way to demonstrate the value of big data analytics.32  It is common to 
focus the initial business case for big data analytics on customer-centric objectives.7 The various 
applications and uses cases discussed earlier cover many different areas that have benefited from big data 
analytics. Whatever be the area, it is desirable that the pilot project address a problem tied to a specific 
16 
business outcome.  The pilot project should not only help solve a business problem, but also demonstrate 
the effectiveness of big data analytics for the organization and its stakeholders. Finally, for successful big 
data initiatives it is essential to have strong, committed sponsorship and alignment between the business 
and analytics strategies.7 In the early stages of adoption, the sponsor could be the CIO and then shifting to 
function-specific executives as business opportunities are identified.   
To benefit from big data analytics companies must also establish a data-driven decision-making culture, 
which calls for acting on insights from data rather than on pure managerial intuition.32 Promotion of data-
sharing practices, increased availability of training in data analytics, and communication of the benefits of 
data-driven decision making are some of the strategies for promoting a data-drive culture.7 While workforce 
training needs to focus on improving technological and digital proficiency, the future work environment 
also demands training in certain soft skills.  The work environment is changing with the rapid introduction 
of AI, automation, and analytics-driven solutions.  Workers need to be open to new ways of working and 
have openness to agility, adaptability, and working in teams to cope with a constantly changing external 
environment.  In the long-run, big data needs to become an integral part of the organization’s operating 
model. There also needs to be clear ownership for big data in the organization with leadership positions 
such as a chief analytics officer.32 Data science should become another established skill in the organization.   
during the development of this white paper.  We would like convey our appreciation to Scott Wahl for his 
guidance and feedback during the formative stages of this effort.  We would also like to thank John 
Ashodian, John Hill, Ying Tat Leung, Juan Ma, Hari Padmanabhan, and John Paxton for carefully reading 
an earlier version of this white paper and providing several constructive suggestions and feedback, which 
have helped us greatly improve the quality of the white paper. 
17 
1. Morten Brinch, Jan Stentoft, and Jesper K. Jensen, “Big Data and its Applications in Supply Chain 
Management: Findings from a Delphi Study,” Proceedings of the 50th Hawaii International Conference 
on System Sciences, 2017: 1351-1360.  
2. IBM Corporation, “The Path to Data Veracity,” IBM Big Data and Analytics Hub, May 2018, 
https://www.ibmbigdatahub.com/whitepaper/path-data-veracity 
3. DataStax Corporation, “Big Data: Beyond the Hype,” October 2013, 
https://www.datastax.com/resources/whitepapers/bigdata 
4. Phillip Russom, “Big Data Analytics,” TDWI Research, 2011, 
https://tdwi.org/research/2011/09/best-practices-report-q4-big-data-
analytics.aspx?tc=page0&m=1  
5. DXC Technology Company, “Five Industries Where Big Data is Making a Difference,” November 
2015, https://assets1.dxc.technology/analytics/downloads/DXC-Analytics-
Five_Industries_Where_Big_Data_is_Making_a_Difference-4AA5-6292ENW.pdf 
6. Nada Elgendy and Ahmed Elragal, “Big Data Analytics: A Literature Review Paper,” In: Perner P. 
(eds) Advances in Data Mining. Applications and Theoretical Aspects. ICDM 2014. Lecture Notes in 
Computer Science, vol 8557, Springer, Cham., 2014, https://link.springer.com/chapter/10.1007/978-
3-319-08976-8_16  
7. Hugh J. Watson, "Tutorial: Big Data Analytics: Concepts, Technologies, and Applications," 
Communications of the Association for Information Systems, 34 (2014), Article 65. 
http://aisel.aisnet.org/cais/vol34/iss1/65  
8. Richard L. Villars, Carl W. Olofson, and Matthew Eastwood, “Big Data: What It Is and Why You 
Should Care,” International Data Corporation, 2011. 
http://www.tracemyflows.com/uploads/big_data/idc_amd_big_data_whitepaper.pdf 
9. Sunil Tiwari, H.M. Wee, and Yosef Daryanto, “Big Data Analytics in Supply Chain Management 
Between 2010 and 2016: Insights to Industries,” Computers and Industrial Engineering, 115 (2017): 319-
330.  
10. Bob Trebilcock, “Supply Chain, Data Analytics, and Big Data,” Logistics Management, August 2015.  
https://www.logisticsmgmt.com/article/supply_chain_data_analytics_and_big_data  
11. Kaushik Pal, “How Machine Learning Can Improve Supply Chain Efficiency,” Techopedia, February-
2018.  https://www.techopedia.com/2/31846/trends/big-data/how-machine-learning-can-improve-
supply-chain-efficiency  
12.  McKinsey & Company, “Big Data and the Supply Chain: The Big-Supply-Chain Analytics 
Landscape: Part 1,” February 2016,  https://www.mckinsey.com/business-functions/operations/our-
insights/big-data-and-the-supply-chain-the-big-supply-chain-analytics-landscape-part-1#  
13. Lorenzo Romano, “Big Data Analytics: A Key Ingredient for Agility in Manufacturing,” May 2019, 
https://www.orange-business.com/en/blogs/big-data-analytics-key-ingredient-agility-manufacturing  
18 
14. Joe McKendrick, “Walmart’s Gigantic Private Cloud for Real-Time Inventory Control,” RT 
Insights.com, January 2017. https://www.rtinsights.com/walmart-cloud-inventory-management-
real-time-data/  
15. RT Insights team, “Levi’s Real-Time Tracking of Jeans: RFID in Retail,” RT Insights.com, April 
2016. https://www.rtinsights.com/rfid-in-retail-customer-experience-levis/ 
16.  JDA, “Store Replenishment at Morrisons,” 2017, https://jda.com/knowledge-center/collateral/by-
morrisons-case-study  
17.  Hans W. Ittmann, “The Impact of Big Data and Business Analytics on Supply Chain Management,” 
Journal of Transport and Supply Chain Management, 9, no. 1 (2015). 
https://jtscm.co.za/index.php/jtscm/article/view/165/331  
18. Logivation, https://www.logivations.com/en/solutions/plan/design_efficiency.php  
19. RT Insights team, “Using Mobile Device for a Real-Time Warehouse,” 2016, 
https://www.rtinsights.com/zebra-omnii-xt15-datek-real-time-warehouse/  
 20. Motifworks, “How Big Data Analytics Can Benefit Supply Chain and Logistics Industry,” 2017. 
https://motifworks.com/2017/02/23/how-big-data-analytics-can-benefit-supply-chain-logistics-
industry/  
 21. “2017 Third-Party Logistics Study,” https://jda.com/-/media/jda/knowledge-center/thought-
leadership/2017stateoflogisticsreport_new.ashx  
 22. UPS, “ORION Backgrounder,” 2019, 
https://www.pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=Factsheet
s&id=1426321616277-282  
23. “Data-Driven Logistics: The Growing Use of Predictive Analytics,” July 2018, https://www.smith-
howard.com/data-driven-logistics-the-growing-use-of-predictive-analytics/ 
 24. Martin Jeske, Moritz Grüner, and Frank Weiẞ, “Big Data in Logistics – A DHL Perspective on How 
to Move Beyond the Hype,” December 2013. 
http://www.dhl.com/content/dam/downloads/g0/about_us/innovation/CSI_Studie_BIG_DATA.pdf  
25.  McKinsey & Company, “Big Data, Analytics, and the Future of Marketing and Sales,” March 2015, 
https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/Marketing%20and%20Sales/Our
%20Insights/EBook%20Big%20data%20analytics%20and%20the%20future%20of%20marketing%20sal
es/Big-Data-eBook.ashx 
26. Gurobi Optimization, “The Power of Analytics,” accessed September 8, 2019. 
http://www.gurobi.com/resources/prescriptive-analytics,  
27.  Transmetrics, “ Big Data and Big Roadblocks:  How the Logistics Industry can Overcome its Big 
Data Challenges,” March 2018, https://www.youredi.com/blog/logistics-industry-can-overcome-big-data-
28. Andrew McAfee, “What Every CEO Needs to Know About the Cloud,” Harvard Business Review, 
Nov. 2011: 124-132. 
19 
29. Bharat Bhargava, Rohit Ranchal, and Lotfi Ben Othmane, “Secure Information Sharing in Digital 
Supply Chains,” 3rd IEEE International Advanced Computing Conference, May 2013, 
https://www.cs.purdue.edu/homes/bb/Bhargava-Supply_Chain-Feb2013-india.pdf  
30. Megan Ray Nicholas, “How to Share Data Safely Across your Supply Chain,” 
https://www.smartdatacollective.com/share-data-safely-across-supply-chain/  
31. APQC, “APQC Quick Poll:  The Current State of Big Data & Advanced Analytics in Supply Chain,” 
May 2019, 
https://www.scmr.com/article/apqc_quick_poll_the_current_state_of_big_data_advanced_analytics_in_su
pply  
32. David Meer, “A Call to Action on Big Data,” Forbes, October 2014, 
https://www.forbes.com/sites/strategyand/2014/10/28/a-call-to-action-on-big-data/#6a4b6c22314  

Removed lines from KimAnh-HTKhoa.pdf:
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381804857
Tích Hợp Big Data Và Điện Toán Đám Mây: Động Lực Thúc Đẩy Thay Đổi Cho
Doanh Nghiệp.
Conference Paper · June 2024
CITATIONS
0
READS
376
1 author:
Vo Thi Kim Anh
Ton Duc Thang University
28 PUBLICATIONS   2 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Vo Thi Kim Anh on 29 June 2024.
The user has requested enhancement of the downloaded file.
KỶ YẾU
HỘI THẢO KHOA HỌC
KHOA CÔNG NGHỆ THÔNG TIN
LẦN 6
2024
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
i 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
KHOA CÔNG NGHỆ THÔNG TIN 
-------------- 
KỶ YẾU HỘI THẢO 
KHOA HỌC CÔNG NGHỆ LẦN 6 
Thành Phố Hồ Chí Minh, tháng 06 năm 2024 
(Lưu hành nội bộ) 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
ii 
BAN BIÊN TẬP 
1. TS. Nguyễn Hà Giang - Trưởng Khoa CNTT 
2. TS. Văn Thị Thiên Trang - Phó Trưởng Khoa CNTT 
3. ThS. Nguyễn Minh Tuấn - Phó Trưởng Khoa CNTT 
4. ThS. Trần Thành Công - Trợ lý Trưởng Khoa, Trưởng Ngành TMĐT 
5. ThS. Hoàng Văn Hiếu - Trưởng Ngành CNTT 
6. ThS. Võ Đình Ngà - Trưởng Ngành TKĐH 
7. ThS. Nguyễn Thị Hoài Linh - Trưởng Ngành KHDL 
8. ThS. Ngô Văn Công Bằng - Trưởng Bộ môn THUD 
9. ThS. Trương Nhã Bình - Trưởng Bộ môn Toán 
THƯ KÝ 
1. KS. Phạm Hữu Kỳ – Giảng viên Khoa CNTT 
2. Trần Thị Phương Anh – Thư ký Khoa CNTT 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iii 
LỜI GIỚI THIỆU 
Công nghệ thông tin đã và đang là yếu tố cốt lõi thúc đẩy nền kinh tế - xã hội phát triển 
mạnh mẽ, đặc biệt trong thời đại kỹ thuật số ngày nay. Sự bùng nổ của các công nghệ mới 
và ứng dụng tiên tiến đã thay đổi cách chúng ta sống, làm việc và tương tác. Với mục đích 
tạo ra một diễn đàn để các nhà nghiên cứu, học giả, giảng viên, cũng như các chuyên gia, 
trao đổi kết quả nghiên cứu, chia sẻ kiến thức, thảo luận quan điểm, ý tưởng về các xu 
hướng mới nhất trong lĩnh vực công nghệ thông tin và ứng dụng, Khoa Công nghệ thông 
tin, Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh (UEF) tổ chức hội thảo 
với chủ đề “Hội thảo khoa học công nghệ Khoa CNTT lần 6 năm 2024”.  
Hội thảo không chỉ nhằm mục đích nâng cao năng lực nghiên cứu mà còn thúc đẩy các 
phát minh, đổi mới và chuyển giao công nghệ trong lĩnh vực công nghệ thông tin. Đây là 
cơ hội để các chuyên gia đầu ngành, nhà nghiên cứu, giảng viên và sinh viên gặp gỡ, học 
hỏi và hợp tác, cùng nhau phát triển và ứng dụng các thành tựu khoa học kỹ thuật vào thực 
tiễn. Qua đó, hội thảo mong muốn góp phần nâng cao chất lượng giáo dục, nghiên cứu và 
thực hành trong lĩnh vực công nghệ thông tin. 
Do thời gian chuẩn bị có hạn, việc biên tập Kỷ yếu này không tránh khỏi những thiếu 
sót. Ban biên tập rất mong ý kiến đóng góp cũng như sự lượng thứ từ quý độc giả để các 
kỳ hội thảo sau được tổ chức ngày một tốt hơn, hiệu quả hơn 
Trân trọng! 
Tp. Hồ Chí Minh, tháng 6 năm 2024 
BAN BIÊN TẬP 
KHOA CÔNG NGHỆ THÔNG TIN 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
141-145 ĐIỆN BIÊN PHỦ, P.15, Q.BÌNH THẠNH, TP.HCM 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iv 
TỐI ƯU HÓA TRUY VẤN TRONG SQL SERVER: PHƯƠNG PHÁP VÀ ỨNG 
DỤNG..........................................................................................Trang 1 
Nguyễn Minh Tuấn - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KÊ CỦA CÔNG CỤ CHATGPT.......................................................Trang 14 
Nguyễn Văn Vinh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
RANSOMWARE: MỐI ĐE DỌA TRONG THỜI ĐẠI SỐ........................Trang 24 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
CHỮ KÝ.....................................................................................Trang 29 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC THÚC ĐẨY 
THAY ĐỔI CHO DOANH NGHIỆP.................................................Trang 35 
Võ Thị Kim Anh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
NHÂN CHO SINH VIÊN NGÀNH THIẾT KẾ ĐỒ HỌA.........................Trang 44 
Võ Đình Ngà - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
ỨNG DỤNG BÀI TOÁN VẬN TẢI: TỐI ƯU CHI PHÍ THU GOM RÁC SINH 
HOẠT CỦA CÁC BỆNH VIỆN..........................................................Trang 59 
Trương Nhã Bình - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
Ngô Thuận Dủ - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
DOANH CỦA DOANH NGHIỆP.......................................................Trang 70 
Hoàng Văn Hiếu - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
SÁNG TẠO NỘI DUNG AI: CÁCH MẠNG HÓA TƯƠNG LAI CỦA TIẾP THỊ NỘI 
DUNG .......................................................................................Trang 85 
Trần Thành Công - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 35 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC 
THÚC ĐẨY THAY ĐỔI CHO DOANH NGHIỆP. 
INTEGRATION OF BIG DATA AND CLOUD COMPUTING: A 
1Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh, anhvtk@uef.edu.vn  
Tóm tắt: Kỷ nguyên số mang đến sự bùng nổ dữ liệu, tạo ra cả thách thức và cơ hội cho doanh nghiệp. 
Sự hội tụ của Big Data và điện toán đám mây nổi lên như giải pháp mạnh mẽ, cách mạng hóa cách 
thức xử lý và khai thác dữ liệu. Bài viết này khám phá tác động biến đổi của sự kết hợp này, đồng thời 
đề xuất những cân nhắc thực tế cho doanh nghiệp bắt đầu áp dụng Big Data trên nền tảng đám mây. 
Từ khóa: Kỷ nguyên số, Big Data, điện toán đám mây, biến đổi, doanh nghiệp. 
Abstract: The digital era has ushered in an unprecedented surge of data, presenting both challenges 
and opportunities for businesses. The convergence of big data and cloud computing has emerged as a 
powerful solution, revolutionizing the way data is processed and harnessed. This paper delves into the 
embarking on their big data on cloud journey. 
Key words: Digital Era, Big Data, Cloud Computing, Transformation, Business 
1. Sự kết hợp mạnh mẽ giữa Big Data và 
liệu, mang đến cả thách thức và cơ hội cho 
doanh nghiệp. Khái niệm Big Data, với đặc 
trưng khối lượng, tốc độ và sự đa dạng, lần 
đầu tiên được giới thiệu bởi Laney (2001) [1] 
và khai thác thông tin. Tuy nhiên, việc quản 
minh là rất phức tạp. 
Sự xuất hiện của điện toán đám mây [2] 
Data, cung cấp giải pháp mạnh mẽ để giải 
quyết thách thức này. Điện toán đám mây 
internet, giúp doanh nghiệp tận dụng tối đa 
linh hoạt. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 36 
Dikaiakos et al. (2009) [3] nhấn mạnh về khả 
năng mở rộng, hiệu quả chi phí và khả năng 
truy cập. 
Đối với khả năng mở rộng: cơ sở hạ tầng 
trên nhu cầu xử lý, loại bỏ nhu cầu đầu tư ban 
đầu tốn kém vào phần cứng. Doanh nghiệp chỉ 
cần trả tiền cho các tài nguyên họ sử dụng, tối 
tức đầu tư [3]. 
Về hiệu quả về chi phí: doanh nghiệp chỉ 
trả tiền cho các tài nguyên họ sử dụng, tối ưu 
đầu tư [3]. 
Còn đối với khả năng truy cập: các giải 
năng truy cập mọi lúc, mọi nơi, thúc đẩy cộng 
tác và sự linh hoạt. 
động, phát triển sản phẩm mới, gia tăng lợi thế 
doanh đầy biến động (xem thống kê Bảng 1).
Bảng 1: Lợi ích của Big Data và Điện toán đám mây  
Tự động mở rộng/thu hẹp tài nguyên, tối ưu hóa chi phí. 
Chỉ trả tiền cho tài nguyên sử dụng. 
Truy cập mọi lúc, mọi nơi, thúc đẩy cộng tác. 
Tự động hóa quy trình, cải thiện ra quyết định, tối ưu hóa 
chuỗi cung ứng. 
Xác định xu hướng thị trường và nhu cầu khách hàng. 
Đưa ra quyết định sáng suốt và nhanh chóng dựa trên dữ liệu. 
Phân tích dữ liệu để dự đoán rủi ro và nắm bắt cơ hội mới. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 37 
2. Ứng dụng thực tiễn của Big Data  
giới thực. Các doanh nghiệp đang tận dụng 
động, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Dưới đây là một số ví dụ cụ 
thể:  
Trước tiên, đó là ở ngành bán lẻ: Các gã 
nền tảng đám mây để quản lý hàng tồn kho, 
thông tin chi tiết về khách hàng [4]. Ví dụ, câu 
chuyện về Amazon retail (Amazon.com). 
Ngày 10 tháng 11 năm 2010 là ngày 
Amazon.com tắt máy chủ web vật lý cuối 
cùng 
trong 
trung 
tâm 
dữ 
liệu 
của 
Amazon.com ([17]). Amazon.com là nhà bán 
lẻ trực tuyến lớn nhất thế giới. Do 
Amazon.com tạo ra rất nhiều dữ liệu, trước 
trữ dữ liệu đó. Nhưng khi Amazon.com phát 
triển lớn hơn, kích thước cơ sở dữ liệu Oracle 
cùng khó khăn. Điều này khiến họ phải cân 
AWS. Bằng cách chuyển sang AWS, họ đã 
trải nghiệm cải thiện hiệu suất gấp 12 lần và 
giảm thời gian khôi phục từ khoảng 15 giờ 
xuống 2,5 giờ ([18]). Amazon.com đã vượt 
qua chi phí cao, hiệu suất chậm và quản lý tốn 
AWS. Họ tận dụng Amazon S3 vì tính tiết 
kiệm chi phí, khả năng mở rộng, bảo mật và 
lưu trữ bền vững, giúp sao lưu và khôi phục 
nhanh hơn đáng kể. Ngoài ra, việc di chuyển 
mạch. Nhìn chung, việc chuyển sang AWS 
giúp giảm chi phí, cải thiện hiệu quả và cung 
phát triển của Amazon (Bảng 3, 4). 
Trong ngành chăm sóc sức khỏe: Ngành 
Data. Nghiên cứu của [5] cho thấy các tổ chức 
mây để phân tích dữ liệu bệnh nhân, từ đó cải 
sáng kiến nghiên cứu. Ví dụ, Mayo Clinic sử 
điều trị mới, chẩn đoán bệnh chính xác hơn và 
cải thiện hiệu quả chăm sóc.  
Và trong ngành dịch vụ tài chính: Phân 
chính. Các nghiên cứu điển hình của [6] cho 
để xác định các giao dịch gian lận, đánh giá 
rủi ro tín dụng và quản lý danh mục đầu tư. Ví 
dụ, JPMorgan Chase sử dụng Big Data để 
phát hiện các trường hợp rửa tiền, ngăn chặn 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 38 
đầu tư.  
vực. Các giải pháp Big Data dựa trên nền tảng 
doanh đầy biến động. Bảng 2 sau đây thống 
kê các ứng dụng:
Bảng 2: Ứng dụng thực tiễn của Big Data 
Quản lý hàng tồn kho, cá nhân hóa 
chiến dịch tiếp thị, thu thập thông tin 
khách hàng, đề xuất sản 
[4, 18] 
Phân tích dữ liệu bệnh nhân, cải thiện 
chất lượng chăm sóc, thúc đẩy nghiên 
cứu 
phương pháp điều trị mới, 
[5] 
Quản lý rủi ro, phát hiện gian lận, 
đánh giá rủi ro tín dụng, quản lý danh 
rửa tiền, ngăn chặn gian lận 
thẻ tín dụng, tối ưu hóa danh 
[6] 
Bảng 3: Bảng so sánh Lưu trữ truyền thống vs Lưu trữ đám mây Amazon S3 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Chi phí trả trước cao cho phần cứng băng, dung 
lượng trung tâm dữ liệu và giấy phép phần mềm. 
Mô hình trả tiền theo nhu cầu, loại 
bỏ chi phí trả trước. 
liệu ngày càng tăng. 
của Amazon. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 39 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Sao lưu và phục hồi chậm do thời gian đọc băng. 
kể so với băng. 
liệu, dễ bị lỗi phần cứng. 
11 số chín (99.999999999%). 
tầng băng. 
cầu quản lý tối thiểu. 
Bảng 4: Bảng so sánh Máy chủ cục bộ vs AWS EC2 đám mây 
Máy chủ On-premises 
trung tâm dữ liệu cục bộ. 
chuyên dụng để giao tiếp liền mạch. 
máy chủ web, cơ sở dữ liệu và 
các công cụ. 
AWS. 
3. Giải quyết thách thức và triển khai hiệu 
toán đám mây mang lại nhiều lợi ích, nó cũng 
thận. Bảo mật dữ liệu là một trong những 
mối quan tâm hàng đầu. Pearson (2013) [7] 
vệ dữ liệu nhạy cảm trên đám mây. Các biện 
pháp này bao gồm: mã hóa dữ liệu, kiểm soát 
quyền truy cập, và tuân thủ các quy định. 
truy cập trái phép. Kiểm soát quyền truy cập 
vào dữ liệu và mức độ truy cập của dữ liệu đó. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 40 
Đối với việc tuân thủ các quy định, như là, 
bảo mật dữ liệu, chẳng hạn như quy định về 
bảo vệ dữ liệu (GDPR) của Liên minh châu 
Âu. 
pháp Big Data trên nền tảng đám mây. 
Achanta (2023) [8] và Setiyawan & Patel 
(2019) [9] đã nêu bật tầm quan trọng của 
việc: chất lượng dữ liệu, và tuân thủ quy định. 
việc xem xét dữ liệu tính chính xác, đầy đủ 
và nhất quán để có thể phân tích hiệu quả. 
quản lý dữ liệu, chẳng hạn như Sarbanes-
Oxley Act (SOX) của Hoa Kỳ.  
Ngoài ra, còn có một số thách thức khác 
Big Data trên nền tảng đám mây, bao gồm: sự 
tương tác, kỹ năng nhân sự, và chi phí triển 
khai – vận hành. Về khả năng tương tác, thì 
doanh nghiệp. Đối với các kỹ năng, thì doanh 
đám mây. Còn lại, đối với quản lý chi phí, thì 
toán đám mây. 
thức và cân nhắc này, các doanh nghiệp có thể 
tranh (Bảng 3). 
Bảng 5: Giải quyết thách thức và triển khai hiệu quả Big Data dựa trên điện toán đám mây 
Mã hóa mạnh mẽ, kiểm soát quyền truy 
cập, tuân thủ quy định 
[7] 
Đảm bảo chất lượng dữ liệu, tuân thủ 
[8, 9] 
[10] 
[11, 12] 
[13, 14] 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 41 
4. Tương lai của việc ra quyết định dựa 
mây. Các xu hướng mới nổi như điện toán 
lý và sử dụng dữ liệu. 
Theo Banjanovic & Husaković (2023) 
[15], điện toán biên tích hợp phân tích Big 
liệu thời gian thực tại ranh giới của mạng. 
nguồn khác nhanh chóng và hiệu quả hơn. 
McGrath & Brenner (2017) [16] cho rằng 
liệu. Nhờ vậy, doanh nghiệp có thể thúc đẩy 
đổi mới và tăng trưởng nhanh hơn. 
Sự kết hợp của Big Data, điện toán đám 
tâm trong việc ra quyết định, đổi mới và tăng 
trưởng (Bảng 4). Doanh nghiệp cần nắm bắt 
đại dữ liệu. 
Bảng 6: Tương lai của việc ra quyết định dựa trên dữ liệu 
biên 
mạng 
liệu nhanh chóng, hiệu quả 
[15] 
chủ 
[16] 
năng lưu trữ, xử lý và phân tích dữ liệu mạnh 
mẽ. Nhờ đó, doanh nghiệp có thể nâng cao 
hiệu quả hoạt động, hiểu rõ hơn về khách 
hàng, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Việc nắm bắt sức mạnh của Big 
số. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 42 
mây hiệu quả, doanh nghiệp cần xác định rõ 
mục tiêu, lựa chọn giải pháp phù hợp, xây 
dựng chiến lược quản trị dữ liệu, đầu tư đào 
án nhỏ đến mở rộng dần. Việc áp dụng thành 
doanh nghiệp thành công trong kỷ nguyên số. 
hiệu quả, doanh nghiệp có thể khai thác sức 
đạt được những lợi ích to lớn. 
[1] Laney, D. (2001) 3D Data Management: 
Controlling Data Volume, Velocity and Variety. 
META Group Research Note, 6. 
[2] Armbrust, M., Griffith, R., Joseph, A. D., Katz, 
R., Konwinski, A., Lee, G., Patterson, D., et al. 
(2010). 
A 
view 
of 
computing. 
Communications of the ACM, 53(4), 50-58. 
ACM. 
[3] Dikaiakos, M., Katsaros, D., Mehra, P., Pallis, G., 
& Vakali, A. (2009). “Cloud computing: 
scientific research”. IEEE Internet Computing, 
13(5), 10-13. 
[4] Chen, W., Li, J., & Jin, X. J. (2016). The 
replenishment policy of agri-products with 
stochastic demand in integrated agricultural 
supply chains. Expert Systems with Applications, 
48, 55-66. 
[5] Halamka, J. (2014). The Argonaut Project 
Charter. Life as a Healthcare CIO. 
[6] Rizvi, S. (2021). Role of big data in financial 
institutions for financial fraud. SSRN Electronic 
Journal, 4, 35. 
[7] Pearson, S. (2013). Privacy, Security and Trust in 
Cloud Computing. In: Pearson, S., Yee, G. (eds) 
Privacy and Security for Cloud Computing. 
and 
Networks. 
Springer, London. https://doi.org/10.1007/978-1-
4471-4189-1_1  
[8]  Achanta, M. (2023). Data governance in the age of 
cloud computing: Strategies and considerations. 
(IJSR), 12, 1338-1343. 
[9]  Setiyawan, D., & Patel, C. (2019). A proposed 
and data management in higher education. SSRN 
Electronic Journal, 6, 19-25. 
[10] Agrawal, D. & Das, S. & Abbadi, A. (2011). Big 
Data and Cloud Computing: Current State and 
Opportunities. 
ACM 
Series. 
530-533. 
10.1145/1951365.1951432. 
[11] Ghaleb, E.A.A.; Dominic, P.D.D.; Fati, S.M.; 
Muneer, A.; Ali, R.F. 2021. The Assessment of Big 
Data Adoption Readiness with a Technology–
Organization–Environment 
Framework: 
A 
Employees. 
2021, 
13, 
8379. 
https://doi.org/10.3390/su13158379 
[12] Shamim, S., Zeng, J., Choksy, U.S. & Shariq, S. 
M. 2020. Connecting big data management 
employee level, International Business Review, 
Volume 29, Issue 6, 101604, ISSN 0969-5931, 
https://doi.org/10.1016/j.ibusrev.2019.101604. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 43 
[13] Muniswamaiah, M., Agerwala, T. & Tappert, C. 
(2019). Big data in cloud computing review and 
opportunities.  International Journal of Computer 
Science & Information Technology (IJCSIT) Vol 
11, 
No 
4. 
https://arxiv.org/ftp/arxiv/papers/1912/1912.108
21.pdf 
[14] El-Seoud, S. A., El-Sofany, H. F., Abdelfattah, 
M. A. F., & Mohamed, R. (2017). Big Data and 
Cloud Computing: Trends and Challenges. 
(iJIM), 
11(2), 
pp. 
34–52. 
https://doi.org/10.3991/ijim.v11i2.6561 
[15] Banjanovic, M. L., & Husaković, A. (2023). Edge 
AI: Reshaping the Future of Edge Computing 
with 
Intelligence. 
10.5644/PI2023.209.07. 
[16] McGrath, G., & Brenner, P. R. (2017). 
"Serverless 
Computing: 
Design, 
Implementation, and Performance," 2017 IEEE 
Computing Systems Workshops (ICDCSW), 
Atlanta, GA, USA, 2017, pp. 405-410, doi: 
10.1109/ICDCSW.2017.36. 
[17] [Amazon Web Services]. (2012, December 10). 
AWS re: Invent ENT 205- Drinking Our Own 
[Video]. 
Https://www.Youtube.com/User/AmazonWebSe
rvices/Cloud 
. 
https://www.youtube.com/watch?v=f45Uo5rw6
YY  
[18] Chavan, A. (2020, September 21). How Amazon 
retail (Amazon.Com) uses the AWS cloud. 
Medium. 
June 
7, 
2024, 
from 
https://ankush-chavan.medium.com/how-
amazon-retail-amazon-com-uses-the-aws-cloud-
View publication stats

Removed lines from sybca-bigdata-ppt.pdf:
Introduction to Big Data
What is Data?
The quantities, characters, or symbols on which operations are performed by a computer, 
magnetic, optical, or mechanical recording media.
What is Big Data?
Big Data is also data but with a huge size. Big Data is a term used to describe a 
collection of data that is huge in volume and yet growing exponentially with time. In 
tools are able to store it or process it efficiently.
“Extremely large data sets that may be analyzed computationally to reveal patterns , 
trends and association, especially relating to human behavior and interaction are 
known as Big Data.”

Following are some the examples of Big Data-
The New York Stock Exchange generates about one terabyte of new trade data per day.
Social Media
The statistic shows that 500+terabytes of new data get ingested into the databases of social 
media site Facebook, every day. This data is mainly generated in terms of photo and video 
uploads, message exchanges, putting comments etc.
A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many 
thousand flights per day, generation of data reaches up to many Petabytes.
Name
Size(In Bytes)
Bit
1/8
1/2 (rare)
Byte
1
1024 bytes
1024
1, 024kilobytes
1, 048, 576
1, 024 megabytes
1, 073, 741, 824
1, 024 gigabytes
1, 099, 511, 627, 776
1, 024 terrabytes
1, 125, 899, 906, 842, 624
1, 024 petabytes
1, 152, 921, 504, 606, 846, 976
1, 024 exabytes
1, 180, 591, 620, 717, 411, 303, 424
1, 024 zettabytes
1, 208, 925, 819, 614, 629, 174, 706, 176
Characteristics Of Big Data
•
The following are known as “Big Data Characteristics”.
1. Volume
2. Velocity
3. Variety
4. Veracity
1. Volume:
Volume means “How much Data is generated”. Now-a-days, 
very vast amount of Data say TB(Tera Bytes) to PB(Peta Bytes) to Exa
Byte(EB) and more.
2. Velocity:
Velocity means “How fast produce Data”. Now-a-days, Organizations or 
fast rate.
3. Variety:
Variety means “Different forms of Data”. Now-a-days, Organizations or 
rate in different formats. We will discuss in details about different formats of 
Data soon.
4. Veracity
Veracity means “The Quality or Correctness or Accuracy of Captured Data”. 
Out of 4Vs, it is most important V for any Big Data Solutions. Because without 
Correct Information or Data, there is no use of storing large amount of data at 
fast rate and different formats. That data should give correct business value.
Types of Digital Data
1. Structured
2. Unstructured
3. Semi-structured
Structured

Any data that can be stored, accessed and processed in the form of fixed format is 
termed as a 'structured' data. 

Over the period of time, talent in computer science has achieved greater success in 
developing techniques for working with such kind of data (where the format is well 
known in advance) and also deriving value out of it.

However, nowadays, we are foreseeing issues when a size of such data grows to a huge 
extent, typical sizes are being in the range of multiple zettabytes.
Do you know? 1021 bytes equal to 1 zettabyte or one billion terabytes forms a zettabyte.
given and imagine the challenges involved in its storage and processing.
Do you know? Data stored in a relational database management system is one 
example of a 'structured' data.
• Examples Of Structured Data
An 'Employee' table in a database is an example of Structured Data
2365
Male
650000
3398
650000
7465
Male
500000
7500
Male
500000
7699
550000
Unstructured

Any data with unknown form or the structure is classified as unstructured data.

In addition to the size being huge, un-structured data poses multiple challenges in terms 
of its processing for deriving value out of it.

combination of simple text files, images, videos etc. 

Now day organizations have wealth of data available with them but unfortunately, they 
don't know how to derive value out of it since this data is in its raw form or unstructured 
format.
• Examples Of Un-structured Data
The output returned by 'Google Search'
Semi-structured

Semi-structured data can contain both the forms of data. 

We can see semi-structured data as a structured in form but it is actually not defined 
with e.g. a table definition in relational DBMS.

Example of semi-structured data is a data represented in an XML file.
Examples Of Semi-structured Data
Personal data stored in an XML file-
<rec><name>Prashant Rao</name><sex>Male</sex><age>35</age></rec>
<rec><name>Seema R.</name><sex>Female</sex><age>41</age></rec>
<rec><name>Satish Mane</name><sex>Male</sex><age>29</age></rec>
<rec><name>Subrato Roy</name><sex>Male</sex><age>26</age></rec>
<rec><name>Jeremiah J.</name><sex>Male</sex><age>35</age></rec>
Big Data Analytics
Big Data Analytics: 

Big Data analytics is the process of collecting, organizing and analyzing 
large sets of data (called Big Data) to discover patterns and other useful 
information.

that is most important to the business and future business decisions. 
from analyzing the data.
High-Performance Analytics Required: 

To analyze such a large volume of data, Big Data analytics is typically 
performed using specialized software tools and applications for predictive 
analytics, data mining, text mining, forecasting and data optimization. 

high-performance analytics.

large volumes of data that a business has collected to determine which data is 
relevant and can be analyzed to drive better business decisions in the future.
The Challenges:

For most organizations, Big Data analysis is a challenge. Consider the sheer 
volume of data and the different formats of the  
data(both structured and unstructured data) that is collected across the entire 
combined, contrasted and analyzed to find patterns and other useful business 
information.

organization stores in different places and often in different systems. 

easily as structured data. 

This massive volume of data is typically so large that it's difficult to process 
using traditional database and software methods.
How Big Data Analytics is Used Today:

data improves, business can be transformed in all sorts of ways. 

Today's advances in analyzing big data allow researchers to decode human DNA in 
minutes, predict where terrorists plan to attack, determine which gene is mostly likely 
to be responsible for certain diseases and, of course, which ads you are most likely to 
respond to on Facebook.

Another example comes from one of the biggest mobile carriers in the world.

France's Orange launched its Data for Development project by releasing subscriber 
data for customers in the Ivory Coast.

The 2.5 billion records, which were made anonymous, included details on calls and 
text messages exchanged between 5 million users.

as the foundation for development projects to improve public health and safety.

cell phone data to map where people went after emergencies; another showed how to 
use cellular data for disease containment. (source)
The Benefits of Big Data Analytics:

data. Many big data projects originate from the need to answer specific 
business questions. With the right big data analytics platforms in place, an 
enterprise can boost sales, increase efficiency, and improve operations, 
customer service and risk management.

Webopedia parent company, QuinStreet, surveyed 540 enterprise decision-
companies plan to use Big Data analytics to improve operations. About half 
of all respondents said they were applying big data analytics to improve 
customer retention, help with product development and gain a competitive 
advantage.

Notably, the business area getting the most attention relates to increasing 
efficiency and optimizing operations. Specifically, 62 percent of respondents 
said that they use big data analytics to improve speed and reduce complexity.
Application of Big Data 
Here is the list of top Big Data applications in today’s world:
•
•
•
Big Data in E-commerce
•
•
•
•
•
Let’s discuss the applications of Big Data in detail.
1. Big Data in Retail

The retail industry is the one that faces the most fierce competition of all. Retailers 
constantly hunt for ways that will give them a competitive edge over others. 
Customers are the real king sounds legit for the retail industry in particular.

For retailers to thrive in this competitive world, they need to understand their 
customers in a better way. If they are aware of their customers’ needs and how to 
fulfill those needs in the best possible way, then they know everything.

– Big Data in Retail.

Through advanced analysis of their customer’s data, retailers are now able to 
understand them from every angle possible. They gather this data from various 
sources such as social media, loyalty programs, etc.

Even a minute detail about any customer has now become significant for them. They are 
now closer to their customers than they have ever been. This empowers them to provide 
customers with more personalized services and predict their demands in advance.

This helps them in building a loyal customer base. Some of the biggest names in the retail 
world like Walmart, Sears and Holdings, Costco, Walgreens, and many more now have Big 
Data as an integral part of their organizations.

are responsible for as much as 30% of retail annual sales.
2. Big Data in Healthcare

Big Data and healthcare are an ideal match. It complements the healthcare industry better 
than anything ever will. The amount of data the healthcare industry has to deal with is 
unimaginable.
Gone are the days when healthcare practitioners were incapable of harnessing this data. 
From finding a cure to cancer to detecting Ebola and much more, Big Data has got it all 
under its belt and researchers have seen some life-saving outcomes through it.

medications. Data analysts are harnessing this data to develop more and more effective 
treatments. Identifying unusual patterns of certain medicines to discover ways for 
developing more economical solutions is a common practice these days.

Explore how Big Data helps to speed up the treatment process – Big Data in 
Healthcare.

people of all age groups. This generates massive amounts of real-time data in the 
form of alerts which helps in saving the lives of the people.
3. Big Data in Education

When you ask people about the use of the data that an educational institute gathers, the 
need it for future references.

Even you had the same perception about this data, didn’t you? But the fact is, this data 
holds enormous importance. Big Data is the key to shaping the future of the people and 
has the power to transform the education system for better.

curriculum. Additionally, universities can even track the dropout rates of the students 
and are taking the required measures to reduce this rate as much as possible.
4. Big Data in E-commerce

One of the greatest revolutions this generation has seen is that of E-commerce. It is now part 
and parcel of our routine life. Whenever we need to buy something, the first thought that 
provokes our mind is E-commerce. And not your surprise, Big Data has been the face of it.

Some of the biggest E-commerce companies of the world like Amazon, Flipkart, Alibaba, and 
popularity Big Data has gained in recent times.

Big Data is now as important as anyone else in these organizations. Amazon, the biggest E-
commerce firm in the world and one of the pioneers of Big Data and analytics, has Big Data as 
the backbone of its system. Flipkart, the biggest E-commerce firm in India, has one of the most 
robust data platforms in the country.

See how Flipkart used Big Data to have one of the most robust data platforms.

Big Data’s recommendation engine is one of the most amazing applications the Big Data world 
has ever witnessed. It furnishes the companies with a 360-degree view of its customers.

Companies then suggest customers accordingly. Customers now experience more personalized 
services than they have ever had. Big Data has completely redefined people’s online shopping 
experiences.
5. Big Data in Media and Entertainment

sheer piece of art. Art and science are often considered to be the two completely 
contrasting domains but when employed together, they do make a deadly duo and Big 
Data’s endeavors in the media industry are a perfect example of it.

Viewers these days need content according to their choices only. Content that is 
relatively new to what they saw the previous time. Earlier the companies 
broadcasted the Ads randomly without any kind of analysis.

But after the advent of Big Data analytics in the industry, companies now are 
aware of the kind of Ads that attracts a customer and the most appropriate time to 
broadcast it for seeking maximum attention.

Customers are now the real heroes of the Media and entertainment industry -
courtesy to Big Data and Analytics.
6. Big Data in Finance

data is one of the toughest challenges any financial firm faces. Data has been the second most 
important commodity for them after money.

Even before Big Data gained popularity, the finance industry was already conquering the 
technical field. In addition to it, financial firms were among the earliest adopters of Big Data 
and Analytics.

has been at the heart of it. Big Data is bossing the key areas of financial firms such as fraud 
detection, risk analysis, algorithmic trading, and customer contentment.

This has brought much-needed fluency in their systems. They are now empowered to focus 
more on providing better services to their customers rather than focussing on security issues. 
Big Data has now enhanced the financial system with answers to its hardest of the challenges.
7. Big Data in Travel Industry

with it, the travel industry was a bit late to realize its worth. Better late than never though. 
Having a stress-free traveling experience is still like a daydream for many.

And now Big Data’s arrival is like a ray of hope, that will mark the departure of all the 
hindrances in our smooth traveling experience.
See how Big Data is revolutionizing the travel & tourism sector.

Through Big Data and analytics, travel companies are now able to offer more 
customized traveling experience. They are now able to understand their customer’s 
requirements in a much-enhanced way.

From providing them with the best offers to be able to make suggestions in real-time, 
Big Data is certainly a perfect guide for any traveler. Big Data is gradually taking the 
window seat in the travel industry.
8. Big Data in Telecom

The telecom industry is the soul of every digital revolution that takes place around the world. 
With the ever-increasing popularity of smartphones, it has flooded the telecom industry with 
massive amounts of data.

And this data is like a goldmine, telecom companies just need to know how to dig it properly. 
Through Big Data and analytics, companies are able to provide the customers with smooth 
connectivity, thus eradicating all the network barriers that the customers have to deal with.
Companies now with the help of Big Data and analytics can track the areas with the lowest as 
well as the highest network traffics and thus doing the needful to ensure hassle-free network 
connectivity.
Big Data alike other industries have helped the telecom industry to understand its customers 
pretty well. 
Telecom industries now provide customers with offers as customized as possible.

Big Data has been behind the data revolution we are currently experiencing.
9. Big Data in Automobile

“A business like an automobile, has to be driven, in order to get results.” B.C. Forbes

smoothly. Big Data is driving the automobile industry towards some unbelievable and never 
before results.

wings to it. Big Data has helped the automobile industry achieve things that were beyond our 

From analyzing the trends to understanding the supply chain management, from taking care 
of its customers to turning our wildest dream of connected cars a reality, Big Data is well 
and truly driving the automobile industry crazy.

Removed lines from TNG_QUAN_V_D_LIU_LN_BIGDATA.pdf:
1 
TỔNG QUAN Vӄ DӲ LIӊU LӞN (BIGDATA) 
Ks. Nguyễn Công Hoan 
Trung Tâm Thông tin Khoa học thống kê (Viện KHTK) 
Trước đây, chúng ta mới chỉ biết đến dữ liệu có cấu trúc (structure data), ngày 
nay, với sự kết hợp của dữ liệu và internet, đã xuất hiện một dạng khác của dữ liệu -  Big 
data (dịch là “dữ liệu lớn”). Dữ liệu này có thể từ các nguồn như: hồ sơ hành chính,giao 
dịch điện tử, dòng trạng thái (status), chia sẻ hình ảnh, bình luận, nhắn tin...của chính 
chúng ta, nói cách khác chúng là dữ liệu được sản sinh qua quá trình chia sẻ thông tin 
trực tuyến liên tục của người sử dụng. Để cung cấp cái nhìn tổng quan, chúng tôi xin giới 
liệu lớn mang lại.    
1.  Khái niӋm, đặc trưng của dӳ liӋu lӟn và sự khác biӋt vӟi dӳ liӋu truyӅn thống 
1.1. Khái niệm về dữ liệu lớn 
- Theo wikipedia: Big data là một thuật ngữ chỉ bộ dữ liệu lớn hoặc phức tạp mà các 
phương pháp truyền thống không đӫ các ứng dөng để xử lỦ dữ liệu này. 
-  Theo Gartner: Dữ liệu lớn là những nguồn thông tin có đặc điểm chung khối lượng lớn,  
tốc độ nhanh và dữ liệu định dạng dưới nhiều hình thức khác nhau, do đó muốn khai thác 
được  đòi hỏi phải có hình thức xử lỦ mới để đưa ra quyết định, khám phá và tối ưu hóa 
quy trình.  
1.2. Nguồn hình thành dữ liệu và phương pháp khai thác và quản lý dữ liệu lớn. 
Qua thống kê và tổng hợp, nguồn dữ liệu lớn được hình thành chӫ yếu từ 6 nguồn: 
(1) Dữ liệu hành chính (phát sinh từ chương trình cӫa một tổ chức, có thể là chính phӫ 
hay phi chính phӫ). Ví dө, hồ sơ y tế điện tử ở bệnh viện, hồ sơ bảo hiểm, hồ sơ ngân 
hàng...; (2) Dữ liệu từ hoạt động thương mại (phát sinh từ các giao dịch giữa hai thực 
thể). Ví dө, các giao dịch thẻ tín dөng, giao dịch trên mạng, bao gồm cả từ các thiết bị di 
động; (3) Dữ liệu từ các thiết bị cảm biến như thiết bị chөp hình ảnh vệ tinh, cảm biến 
đường, cảm biến khí hậu; (4) Dữ liệu từ các thiết bị theo dõi, ví dө theo dõi dữ liệu từ 
điện thoại di động, GPS; (5) Dữ liệu từ các hành vi, ví dө như tìm kiếm trực tuyến về 
(một sản phẩm, một dịch vө hay thông tin khác), đọc các trang mạng trực tuyến...; (6) Dữ 
liệu từ các thông tin về  Ủ kiến, quan điểm cӫa các cá nhân, tổ chức, trên các phương tiện 
thông tin xã hội. 
theo các nguồn hình thành dữ liệu lớn. Mỗi nguồn dữ liệu lớn khác nhau sẽ có phương 
pháp khai thác và quản lỦ dữ liệu lớn khác nhau. Tuy nhiên, hiện nay phần lớn các tổ 
dữ liệu lớn. 
1.3. Đặc trưng 5V cͯa dữ liệu lớn 
Dữ liệu lớn có 5 đặc trưng cơ bản như sau (mô hình 5V):   
(1) Khối lượng dữ liệu (Volume)  
2 
Đây là đặc điểm tiêu biểu nhất cӫa dữ liệu lớn, khối lượng dữ liệu rất lớn. Kích cỡ 
cӫa Big Data đang từng ngày tăng lên, và tính đến năm 2012 thì nó có thể nằm trong 
khoảng vài chөc terabyte cho đến nhiều petabyte (1 petabyte = 1024 terabyte) chỉ cho 
một tập hợp dữ liệu. Dữ liệu truyền thống có thể lưu trữ trên các thiết bị đĩa mềm, đĩa 
cứng. Nhưng với dữ liệu lớn chúng ta sẽ sử dөng công nghệ “đám mây” mới đáp ứng khả 
năng lưu trữ được dữ liệu lớn. 
 (2) Tốc độ (Velocity) 
Tốc độ có thể hiểu theo 2 khía cạnh: (a) Khối lượng dữ liệu gia tăng rất nhanh (mỗi 
giây có tới 72.9 triệu các yêu cầu truy cập tìm kiếm trên web bán hàng cӫa Amazon); (b) 
Xử lỦ dữ liệu nhanh ở mức thời gian thực (real-time), có nghĩa dữ liệu được xử lỦ ngay 
tức thời ngay sau khi chúng phát sinh (tính đến bằng mili giây). Các ứng dөng phổ biến 
trên lĩnh vực Internet, Tài chính, Ngân hàng, Hàng không, Quân sự, Y tế – Sức khỏe như 
hiện nay phần lớn dữ liệu lớn được xử lỦ real-time. Công nghệ xử lỦ dữ liệu lớn ngày nay 
đã cho phép chúng ta xử lỦ tức thì trước khi chúng được lưu trữ vào cơ sở dữ liệu. 
(3) Đa dạng (Variety) 
Đối với dữ liệu truyền thống chúng ta hay nói đến dữ liệu có cấu trúc, thì ngày nay 
hơn 80% dữ liệu được sinh ra là phi cấu trúc (tài liệu, blog, hình ảnh, vi deo, bài hát, dữ 
liệu từ thiết bị cảm biến vật lỦ, thiết bị chăm sóc sức khỏe...). Big Data cho phép liên kết 
và phân tích nhiều dạng dữ liệu khác nhau. Ví dө, với các bình luận cӫa một nhóm người 
dùng nào đó trên Facebook với thông tin video được chia sẻ từ Youtube và Twitter. 
(4) Độ tin cậy/chính xác (Veracity) 
Một trong những tính chất phức tạp nhất cӫa Dữ liệu lớn là độ tin cậy/chính xác cӫa 
dữ liệu. Với xu hướng phương tiện truyền thông xã hội (Social Media) và mạng xã hội 
(Social Network) ngày nay và sự gia tăng mạnh mẽ tính tương tác và chia sẻ cӫa người 
dùng Mobile làm cho bức tranh xác định về độ tin cậy & chính xác cӫa dữ liệu ngày một 
khó khăn hơn. Bài toán phân tích và loại bỏ dữ liệu thiếu chính xác và nhiễu đang là tính 
chất quan trọng cӫa BigData. 
 (5) Giá trị (Value) 
Giá trị là đặc điểm quan trọng nhất cӫa dữ liệu lớn, vì khi bắt đầu triển khai xây 
thông tin mang lại như thế nào, khi đó chúng ta mới có quyết định có nên triển khai dữ 
liệu lớn hay không. Nếu chúng ta có dữ liệu lớn mà chỉ nhận được 1% lợi ích từ nó, thì 
không nên đầu tư phát triển dữ liệu lớn. Kết quả dự báo chính xác thể hiện rõ nét nhất về 
giá trị cӫa dữ liệu lớn mang lại. Ví dө, từ khối dữ liệu phát sinh trong quá trình khám, 
chữa bệnh sẽ giúp dự báo về sức khỏe được chính xác hơn, sẽ giảm được chi phí điều trị 
và các chi phí liên quan đến y tế.  
1.4. Sự khác biệt giữa dữ liệu lớn với dữ liệu truyền thống  
3 
Dữ liệu lớn khác với dữ liệu truyền thống (ví dө, kho dữ liệu - Data Warehouse) ở 4 
điểm cơ bản: Dữ liệu đa dạng hơn; lưu trữ dữ liệu lớn hơn; truy vấn nhanh hơn; độ chính 
xác cao hơn. 
(1) Dữ liệu đa dạng hơn: Khi khai thác dữ liệu truyền thống (Dữ liệu có cấu trúc), 
chúng ta thường phải trả lời các câu hỏi: Dữ liệu lấy ra kiểu gì? định dạng dữ liệu như thế 
nào? Đối với dữ liệu lớn, không phải trả lời các câu hỏi trên. Hay nói khác, khi khai thác, 
chúng; điều quan tâm là giá trị mà dữ liệu mang lại có đáp ứng được cho công việc hiện 
tại và tương lai hay không. 
(2) Lưu trữ dữ liệu lớn hơn: Lưu trữ dữ liệu truyền thống vô cùng phức tạp và luôn 
đặt ra câu hỏi lưu như thế nào? dung lượng kho lưu trữ bao nhiêu là đӫ? gắn kèm với câu 
hỏi đó là chi phí đầu tư tương ứng. Công nghệ lưu trữ  dữ liệu lớn hiện nay đã phần nào 
có thể giải quyết được vấn đề trên nhờ những công nghệ lưu trữ đám mây, phân phối lưu 
xác và xử lỦ nhanh trong thời gian thực. 
(3) Truy vấn dữ liệu nhanh hơn: Dữ liệu lớn được cập nhật liên tөc, trong khi đó 
tin đáp ứng theo yêu cầu. 
(4) Độ chính xác cao hơn: Dữ liệu lớn khi đưa vào sử dөng thường được kiểm định 
lại dữ liệu với những điều kiện chặt chẽ, số lượng thông tin được kiểm tra thông thường 
rất lớn, và đảm bảo về nguồn lấy dữ liệu không có sự tác động cӫa con người vào thay 
đổi số liệu thu thập. 
2. Bͱc tranh tổng thể ͱng dụng dữ liệu lớn  
 Dữ liệu lớn đã được ứng dөng trong nhiều lĩnh vực như: hoạt động chính trị; giao 
thông; y tế; thể thao; tài chính; thương mại; thống kê... dưới đây là một số ví dө về ứng 
dөng dữ liệu lớn. 
2.1. Ͱng dụng dữ liệu lớn trong hoạt động chính trị 
cӫa mình. Ông xây dựng một đội ngũ nhân viên chuyên đi 
triển khai về dữ liệu lớn. Đội ngũ nhân viên này thu thập tất 
cả thông tin về người dân ở các khu vực, sau đó phân tích và 
chỉ ra một số thông tin quan trọng về người dân Mỹ như: 
Thích đọc sách gì, thích mua loại thuốc gì, thích sử dөng phương tiện gì... Thậm chí còn 
biết được cả thông tin về mẹ cӫa cử tri đó đã bỏ phiếu tín nhiệm ai ở lần bầu cử trước. 
Trên cơ sở những thông tin này, Tổng thống Obama đã đưa ra kế hoạch vận động phù 
hợp, giúp ông tái đắc cử Tổng thống nước Mỹ lần thứ 2. 
4 
dөng như: Hệ thống chính phӫ điện tử; phân tích quy định và việc tuân thӫ quy định; 
phân tích, giám sát, theo dõi và phát hiện gian lận, mối đe dọa, an ninh mạng. 
2.2. Ͱng dụng dữ liệu lớn trong giao thông 
dòng giao thông trong thành phố vào các giờ cao điểm, từ đó có 
những kế hoạch phân luồng giao thông chi tiết, hợp lỦ giúp giảm 
thiểu kẹt xe. Ngoài ra còn đưa ra thông tin cho người tham gia 
đi vào giờ nào để tránh kẹt xe, hoặc đi đường nào là ngắn nhất.v.v. Ngoài ra dữ liệu lớn 
còn giúp phân tích định vị người dùng thiết bị di động, ghi nhận chi tiết cuộc gọi trong 
thời gian thực; và giảm thiểu tình trạng ùn tắc giao thông. 
2.3. Ͱng dụng dữ liệu lớn trong y tế 
để đưa ra dự đoán về nguy cơ mắc bệnh. Đồng thời cũng đưa ra 
được xu hướng lây lan cӫa bệnh. Ví dө, ứng dөng Google Flu 
dөng này dựa trên từ khóa tìm kiếm ở một khu vực nào đó, sau đó 
kiếm đó, sau cùng là đưa ra dự báo về xu hướng dịch cúm tại khu 
vực đó. Qua đó cho biết tình hình cúm tại khu vực đó sẽ diễn ra như thế nào để đưa ra các 
giải pháp phòng tránh. Những kết quả mà Google Flu Trend đưa ra, hoàn toàn phù hợp 
với báo cáo cӫa Tổ chức y tế thế giới WHO về tình hình bệnh cúm tại các khu vực đó. 
2.4. Ͱng dụng dữ liệu lớn trong thể thao 
cӫa đội tuyển Đức (hình bên) đã đưa ra những điểm bất hợp lỦ 
trong cấu trúc cӫa đội tuyển Đức, từ đó giúp cho đội tuyển Đức 
khắc phөc được điểm yếu và đã dành được World cup 2014. 
2.5. Ͱng dụng dữ liệu lớn trong tài chính 
Từ những dữ liệu chính xác, kịp thời thu thập được thông qua các giao dịch cӫa 
khách hàng, tiến hành phân tích, xếp hạng và quản lỦ các rӫi ro trong đầu tư tài chính, tín 
dөng. 
2.6. Ͱng dụng dữ liệu lớn trong thương mại 
sau: Phân khúc thị trường và khách hàng; phân tích hành vi khách hàng tại cửa hàng; tiếp 
thị trên nền tảng định vị; phân tích tiếp thị chéo kênh, tiếp thị đa kênh; quản lỦ các chiến 
dịch tiếp thị và khách hàng thân thiết; So sánh giá; Phân tích và quản lỦ chuỗi cung ứng; 
Phân tích hành vi, thói quen người tiêu dùng. 
2.7. Ͱng dụng dữ liệu lớn trong thống kê 
5 
thức, Ӫy ban Thống kê Liên hợp quốc cũng như các tổ chức thống kê khu vực và cơ quan 
thống kê quốc gia cӫa nhiều nước đã triển khai hàng loạt các hoạt động về Bigdata như: 
Hàn Quốc sử dөng ảnh vệ tinh để thống kê nông nghiệp và một số lĩnhvực khác;Australia 
sử dөng ảnh vệ tinh để thống kê diện tích đất nông nghiệp và năng suất; Italia sử dөng dữ 
liệu điện thoại di động để thống kê di cư; Bhutan dùng thiết bị di động để tính toán chỉ số 
giá tiêu dùng; Estonia dùng điện thoại di động định vị vệ tinh để thống kê du lịch; 
3. Nhӳng cơ hội và thách thͱc khi ͱng dụng Big data trong thống kê chính thͱc 
3.1 Cơ hội  
(1) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp cho chúng ta có thêm phương án 
giải quyết, xử lỦ và đối phó với những thách thức đối sản xuất số liệu thống kê chính thức 
trong hiện tại và tương lai. Những nghiên cứu thực nghiệm cần phải được tiến hành để 
khám phá những ứng dөng tiềm năng cӫa dữ liệu lớn trong số liệu thống kê chính thức, 
và nghiên cứu thực nghiệm đó phải là một phần trong quy trình sản xuất số liệu thống kê. 
(2) Nghiên cứu về dữ liệu lớn cần phải có cơ sở hạ tầng công nghệ thông tin hiện 
đại, đáp ứng các yêu cầu xử lỦ khối lượng lớn dữ liệu và nhanh, đồng thời có thể tập hợp 
dữ liệu từ nhiều nguồn khác nhau. Thực hiện được điều này chúng ta có được đội ngũ 
qua kinh nghiệm thực tế. 
(3) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp chúng ta có được những văn bản 
được khai thác dữ liệu thông qua hồ sơ hành chính, ngoài ra dữ liệu cũng được bảo đảm 
và giữ bí mật nhờ những văn bản pháp lỦ bổ sung này. 
(4) Sử dөng dữ liệu lớn đem lại niềm tin cӫa cộng đồng với thống kê chính thức do 
tác động chӫ Ủ cӫa con người. 
3.2 Thách thͱc  
(1)Tài chính 
Nhiều đơn vị, tổ chức không đo lường được vấn đề sẽ phát sinh trong quá trình triển 
khai thực hiện, dự toán kinh phí chưa chính xác, do vậy dự án không thực hiện được. Để 
triển khai được thành công, yếu tố tài chính có Ủ nghĩa rất quan trọng, một số tập đoàn 
Big data như IBM, website bán hàng thương mại điện tử Amazon ... 
(2) Chính sách, quy định Luật pháp về truy cập và sử dụng dữ liệu 
Việc sử dөng và khai thác dữ liệu lớn phө thuộc vào luật quy định cӫa mỗi quốc gia. 
1 Xem Báo cáo “Thống kê chính thức với Big data: Kinh nghiệm quốc tế và định hướng của Thống kê Việt Nam. 
6 
  Ví dө: ở Canada người dùng có thể được tiếp cận dữ liệu từ cả hai tổ chức chính 
phӫ và phi chính phӫ, nhưng ở những nước khác như Ireland thì phải được sự cho phép 
từ các cơ quan chính phӫ. Điều này có thể dẫn đến những hạn chế để truy cập vào một số 
loại dữ liệu lớn. 
 (3) Trình độ khai thác và quản lý dữ liệu  
quản lỦ là cũng khác nhau tuy nhiên, Một vấn đề liên quan đến quản lỦ thông tin hiện nay 
là nguồn nhân lực. Khoa học dữ liệu lớn đang phát triển mạnh trong những tổ chức tư 
nhân, trong khi đó bộ phận này chưa được liên kết với những tổ chức cӫa chính phӫ một 
cách chặt chẽ dẫn đến việc quản lỦ vẫn còn nhiều vướng mắc.. 
(4) Hạ tầng Công nghệ thông tin 
sử dөng giao diện ứng dөng cӫa Chương trình chuyên sâu tiêu chuẩn (API) để truy cập 
dữ liệu. Bằng cách này, nó có thể kết nối các ứng dөng cho dữ liệu thu về và xử lỦ dữ liệu 
trực tiếp với dữ liệu hành chính. Ngoài ra hệ thống khai thác dữ liệu lớn cũng cần phải 
được tính toán để có thể kết nối vào được kho cơ sở dữ liệu truyền thống, đó cũng là một 
trong những thách thức lớn cần được giải quyết.  
data, những lợi ích mà Big data mang lại cho chúng ta. Bên cạnh đó cũng chỉ ra những 
thách thức khi triển khai áp dөng khai thác Big data. 
cung cấpthông tin để chung ta xử lỦ được tình huống nhanh nhất, chính xác nhất và giá trị 
cӫa Big data mang lại luôn có tính định hướng đến tương  lai ? giải đáp những câu hỏi tại 
sao việc ấy lại xảy ra?;  Sau chuyện đó thì điều gì sẽ sảy ra? và chúng ta nên ứng phó như 
thế nào trong hoàn cảnh đó? 
1. Tài liệu cơ hội và thách thức với bigdata –E cӫa Liên Hợp Quốc: 
http://unstats.un.org/unsd/statcom/doc14/2014-11-BigData-E.pdf 
2. Báo cáo Hội thảo về tương lai cӫa Thống kê học London: 
https://statistics.stanford.edu/statistics-and-science-london-workshop-report 
3. Tài liệu về các khái niệm và đặc trưng cӫa Big data: 
https://viblo.asia/dovv/posts/3OEqGjWwv9bL 

Removed lines from what-is-big-data-ebook-4421383.pdf:
What is Big Data? 
04 
  08 
Big Data Use Cases                                                                      10 
                                                             13 
15 
18 
4 
5 
What exactly is big data? 
To put it simply: big data is larger, more 
complex data sets, especially from new data 
sources. These data sets are so voluminous that 
traditional data processing software just can’t 
manage them. But these massive volumes of 
you wouldn’t have been able to tackle before. 
To really understand big data, it’s helpful to have 
some historical background. Here’s Gartner’s 
defnition, circa 2001(which is still the go-to 
defnition): 
“Big data is data that contains greater variety 
arriving in increasing volumes and with ever 
higher velocity. This is known as the three Vs.” 
• Volume.The amount of data matters. With 
big data, you’ll have to process high volumes 
of low-density, unstructured data. This can be 
data of unknown value, such as Twitter data 
feeds, clickstreams on a webpage or a mobile 
app, or sensor-enabled equipment. For some 
organizations, this might be tens of 
terabytes of data. For others, it may be 
hundreds of petabytes. 
Velocity. Velocity is the fast rate at which data 
is received and (perhaps) acted on. Normally, 
memory versus being written to disk.  Some 
internet-enabled smart products operate in real 
real-time evaluation and action. 
• Variety. In today’s big data world, data 
comes in new unstructured data types. 
Unstructured and semi-structured data types, 
such as text, audio, and video require addition 
support metadata. 
6 
Volume
1 
2 
3 
THE VALUE—AND TRUTH—OF 
Since 2001, two more Vs have become apparent: 
value and veracity. Data has intrinsic value. But 
it’s of no use until that value is discovered. 
Equally important: How truthful is your data—and 
how much can you rely on it? 
Today, big data has become capital. Think of 
some of the world’s biggest tech companies. A 
their data, which they’re constantly analyzing to 
new products. 
and compute, making it easier and less expensive 
to store more data than ever before. With an 
increased volume of big data now cheaper and 
more accessible, you can make more accurate 
and precise business decisions. 
Finding value in big data isn’t only about 
analyzing it (which is a whole other beneft). 
It’s an entire discovery process that requires 
insightful analysts, business users, and 
executives who ask the right questions, recognize 
patterns, make informed assumptions, and 
predict behavior. 
But how did we get here? 
7 
8 
Around 2005, people began to realize just how 
much data users generated through Facebook, 
YouTube, and other online services. Hadoop (an 
open-source framework created specifcally to 
store and analyze big data sets) was developed 
that same year. NoSQL also began to gain 
popularity during this time. 
The development of open-source frameworks, 
such as Hadoop (and more recently, Spark) was 
store. In the years since then, the volume of big 
data has skyrocketed. Users are still generating 
huge amounts of data—but it’s not just humans. 
With the advent of Internet of Things (IoT) , more 
objects and devices are connected to the internet, 
product performance. The emergence of machine 
learning has produced still more data. 
While big data has come far, its popularity is only 
just beginning. Cloud computing has expanded 
big data possibilities even further. 
The cloud offers a truly elastic scalability, where 
test around a subset of data. It’s an exciting time 
to see what’s going to happen next. 
THE VALUE OF BIG DATA COMES IS TWOFOLD: 
1. Big data makes it possible for you to gain 
2. More complete answers means more 
confdence in the data–which means 
a completely different approach to 
9 
10 
cases that you haven’t been able to fully delve 
into before. Here are just a few.  (More use cases 
are on our solutions page): 
Companies like Netfix and Procter & Gamble 
use big data to anticipate customer demand. 
products or services, and then modeling the 
commercial success of the offerings, they build 
predictive models for new products and services. 
In addition, P&G uses data and analytics from 
focus groups, social media, test markets, and 
early store rollouts to plan, produce, and launch 
new products. 
be deeply buried in structured data, such as the 
equipment year, make, and model, as well as 
entries, senor data, error messages, and engine 
temperature., By analyzing these indications of 
potential issues before the problems happen, 
equipment uptime. 
The race for customers is on. A clearer view of 
ever before. Big data enables you to gather data 
from social media, web visits, call logs, and 
experience and maximize the value delivered. 
Start delivering personalized offers, reduce 
customer churn, and handle issues proactively. 
When it comes to security, it’s not just a few 
rogue hackers; you’re up against entire expert 
teams. Security landscapes and compliance 
requirements are constantly evolving. Big 
indicate fraud and aggregate large volumes of 
much faster. 
now. And data—specifcally big data—is one of 
the reasons why. It’s only recently that we’ve 
them. And the availability of big data to train 
machine-learning models makes that happen. 
11 
news, but it’s an area in which big data is having 
the most impact. With big data, you can analyze 
and assess production, customer feedback and 
returns, and other factors to reduce outages and 
anticipate future demands. Big data can also be 
used to improve decision-making in line with 
current market demand. 
interdependencies between humans, institutions, 
entities, and process and then determining new 
ways to use those insights. Use data insights to 
considerations. Examine trends and what 
services. Implement dynamic pricing. There are 
endless possibilities. 
While big data holds a lot of promise, it is not 
without its challenges.  
First, big data is... big. Although new 
technologies have been developed to store data, 
data volumes are doubling in size around every 
two years. Organizations still struggle to keep 
store it. 
But it’s not enough to just store the data. Data 
must be used to be valuable, and that depends 
on curation. Clean data, or data that’s relevant 
meaningful analysis requires a lot of work. 
Data scientists spend 50 to 80 percent of their 
actually be used. 
Finally, big data technology is changing at a fast 
pace. A few years ago, Apache Hadoop was the 
popular technology used to handle big data. That 
is, until Apache Spark was introduced in 2014. 
Today, a combination of the two frameworks 
appears to be the best approach. Keeping up with 
big data technology is an ongoing challenge. 
12 
13 
Oracle Cloud for  Big  Data  Analytics 
Data 
Enterprise Apps 
Data 
new opportunities and business models. Getting 
started involves three key actions:   
disparate sources and applications. Traditional 
data integration mechanisms, such as ETL 
(extract, transform, and load) generally aren’t 
up to the task. It requires new strategies and 
technologies to analyze big data sets at terabyte, 
or even petabyte, scale. At the same time, big 
data has the same requirements for quality, 
governance, and confdence as traditional data 
sources. During integration, you need to bring in 
the data, process it, and make sure it’s formatted 
analysts can get started with. 
Big data requires storage. Your storage solution 
can be in the cloud, on-premises, or both. 
on an on-demand basis. Many people choose 
data is currently residing. The cloud is gradually 
gaining popularity because it supports your 
to spin up resources as needed. 
analyze and act on your data. Get new clarity with 
a visual analysis of your varied data sets. Explore 
the data further to make new discoveries. Share 
your fndings with others. Build data models with 
machine learning and artifcial intelligence. Put 
your data to work. 
To help you on your big data journey, we’ve put 
in mind. Here are our guidelines for building a 
successful big data foundation. 
14 
15 
#1: ALIGN BIG DATA WITH 
new discoveries. To that end, it is important to 
base new investments in skills, organization, 
or infrastructure with a strong business-
investments and funding. To determine if 
you are on the right track, ask how big data 
supports and enables your top business and IT 
priorities. Examples include understanding how 
behavior, deriving sentiment from social 
media and customer support interactions, and 
and their relevance for customer, product, 
manufacturing, and engineering data. 
#2: EASE SKILLS SHORTAGE 
shortage. You can mitigate this risk by ensuring 
that big data technologies, considerations, 
governance program. 
Standardizing your approach will allow you 
to manage costs and leverage resources. 
proactively identify any potential skill gaps. These 
can be addressed by training/cross-training 
existing resources, hiring new resources, and 
leveraging consulting frms. 
#3: OPTIMIZE KNOWLEDGE 
Use a Center of Excellence approach to share 
knowledge, control oversight, and manage 
project communications. Whether big data is a 
new or an expanding investment, the soft and 
hard costs can be shared across the enterprise. 
Leveraging this approach can help increase 
systematic way. 
#4:TOP PAYOFF IS ALIGNING 
own.But you can bring even greater business 
already using today. 
16 
Whether you are capturing customer, product, 
equipment, or environmental big data, the goal 
core master and analytical summaries, leading 
to better conclusions. For example, there is 
sentiment from that of only your best customers. 
capabilities, data warehousing platform, and 
information architecture. 
processes and models can be both human- and 
machine-based. Big data analytical capabilities 
include statistics, spatial analysis, semantics, 
interactive discovery, and visualization. Using 
analytical models, you can correlate different 
and meaningful discoveries. 
#5: PLAN YOUR DISCOVERY LAB 
straightforward. Sometimes we don’t even 
know what we’re looking for. That’s expected. 
Management and IT needs to support this “lack 
of direction” or “lack of clear requirement.” 
At the same time, it’s important for analysts and 
requirements. To accommodate the interactive 
statistical algorithms, you need high-performance 
work areas. Be sure that sandbox environments 
have the power they need—and are 
properly governed. 
#6: ALIGN WITH THE CLOUD 
jobs. A big data solution includes all data 
realms including transactions, master data, 
reference data, and summarized data. Analytical 
sandboxes should be created on demand. 
control of the entire data fow including pre- 
and post-processing, integration, in-database 
summarization, and analytical modeling. A well-
supporting these changing requirements. 
17 
18 
Clearly, big data has tremendous potential. 
customers, make more accurate decisions, and 
create new growth opportunities. Contact us to 
learn more. 
See how Oracle can help your big data journey. 
Start your free trial today. 
Contact us URL: 
https://www.oracle.com/marketingcloud/contact­
sales.html 
Free Trial URL: 
https://go.oracle.com/LP=50758/? 
19 
Oracle Corporation 
Copyright © 2019, Oracle and/or its affiliates. All rights reserved. This document is provided for information purposes only, and the contents hereof are subject 
to change without notice. This document is not warranted to be error-free, nor subject to any other warranties or conditions, whether expressed orally or 
implied in law, including implied warranties and conditions of merchantability or fitness for a particular purpose. We specifically disclaim any liability with 
500 Oracle Parkway 
respect to this document, and no contractual obligations are formed either directly or indirectly by this document. This document may not be reproduced or 
transmitted in any form or by any means, electronic or mechanical, for any purpose, without our prior written permission. 
CA 94065 
Oracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners. 
USA 
Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or 
registered trademarks of SPARC International, Inc. AMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks or registered trademarks of 
Advanced Micro Devices. UNIX is a registered trademark of The Open Group. 
Phone: +1.650.506.7000 
+1.800.ORACLE1 
Fax: 
+1.650.506.7200 
oracle.com 

Removed lines from 2_iis_2015_81-90.pdf:
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
81 
BIG DATA ANALYTICS 
Jasmine Zakir, Minot State University,	  jasminezakir@outlook.com 
Tom Seymour, Minot State University,	  tom.seymour@minotstateu.edu 
Kristi Berg, Minot State University,	  kristi.berg@minostatateu.edu 
ABSTRACT 
Today Big Data draws a lot of attention in the IT world. The rapid rise of the Internet and the digital economy has 
fuelled an exponential growth in demand for data storage and analytics, and IT department are facing tremendous 
challenge in protecting and analyzing these increased volumes of information. The reason organizations are 
collecting and storing more data than ever before is because their business depends on it. The type of information 
being created is no more traditional database-driven data referred to as structured data rather it is data that 
include documents, images, audio, video, and social media contents known as unstructured data or Big Data. Big 
Data Analytics is a way of extracting value from these huge volumes of information, and it drives new market 
opportunities and maximizes customer retention. This paper primarily focuses on discussing the various 
technologies that work together as a Big Data Analytics system that can help predict future volumes, gain insights, 
take proactive actions, and give way to better strategic decision-making. Further this paper analyzes the adoption, 
usage and impact of big data analytics to the business value of an enterprise to improve its competitive advantage 
using a set of data algorithms for large data sets such as Hadoop and MapReduce.  
Keywords: Big Data, Analytics, Hadoop, MapReduce 
Big Data is an important concept, which is applied to data, which does not conform to the normal structure of the 
traditional database. Big Data consists of different types of key technologies like Hadoop, HDFS, NoSQL, 
MapReduce, MongoDB, Cassandra, PIG, HIVE, and HBASE that work together to achieve the end goal like 
extracting value from data that would be previously considered dead. According to a recent market report published 
by Transparency Market Research, the total value of big data was estimated at $6.3 billion as of 2012, but by 2018, 
it’s expected to reach the staggering level of $48.3 billion that’s almost a 700 percent increase [29]. Forrester 
Research estimates that organizations effectively utilize less than 5 percent of their available data. This is because 
the rest is simply too expensive to deal with. Big Data is derived from multiple sources. It involves not just 
traditional relational data, but all paradigms of unstructured data sources that are growing at a significant rate. For 
instance, machine-derived data multiplies quickly and contains rich, diverse content that needs to be discovered. 
Another example, human-derived data from social media is more textual, but the valuable insights are often 
overloaded with many possible meanings.  
Big Data Analytics reflect the challenges of data that are too vast, too unstructured, and too fast moving to be 
managed by traditional methods. From businesses and research institutions to governments, organizations now 
routinely generate data of unprecedented scope and complexity. Gleaning meaningful information and competitive 
advantages from massive amounts of data has become increasingly important to organizations globally. Trying to 
efficiently extract the meaningful insights from such data sources quickly and easily is challenging. Thus, analytics 
increase their market share. The tools available to handle the volume, velocity, and variety of big data have 
improved greatly in recent years. In general, these technologies are not prohibitively expensive, and much of the 
software is open source. Hadoop, the most commonly used framework, combines commodity hardware with open-
source software. It takes incoming streams of data and distributes them onto cheap disks; it also provides tools for 
analyzing the data. However, these technologies do require a skill set that is new to most IT departments, which will 
need to work hard to integrate all the relevant internal and external sources of data. Although attention to technology 
isn’t sufficient, it is always a necessary component of a big data strategy. This paper discusses some of the most 
commonly used big data technologies mostly open source that work together as a big data analytics system for 
leveraging large quantities of unstructured data to make more informed decisions.  
https://doi.org/10.48009/2_iis_2015_81-90
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
82 
Big Data is a data analysis methodology enabled by recent advances in technologies that support high-velocity data 
capture, storage and analysis. Data sources extend beyond the traditional corporate database to include emails, 
mobile device outputs, and sensor-generated data where data is no longer restricted to structured database records 
but rather unstructured data having no standard formatting [30]. Since Big Data and Analytics is a relatively new 
and evolving phrase, there is no uniform definition; various stakeholders have provided diverse and sometimes 
contradictory definitions. One of the first widely quoted definitions of Big Data resulted from the Gartner report of 
2001. Gartner proposed that, Big Data is defined by three V’s volume, velocity, and variety. Gartner expanded its 
definition in 2012 to include veracity, representing requirements about trust and uncertainty pertaining to data and 
the outcome of data analysis. In a 2012 report, IDC defined the 4th V as value—highlighting that Big Data 
applications need to bring incremental value to businesses. Big Data Analytics is all about processing unstructured 
information from call logs, mobile-banking transactions, online user generated content such as blog posts and 
tweets, online searches, and images which can be transformed into valuable business information using 
computational techniques to unveil trends and patterns between datasets. 
Another dimension of the Big Data definition involves technology. Big Data is not only large and complex, but it 
requires innovative technology to analyze and process. In 2013, the National Institute of Standard and Technology 
(NIST) Big Data workgroup proposed the following definition of Big Data that emphasizes application of new 
technology; Big Data exceed the capacity or capability of current or conventional methods and systems, and enable 
novel approaches to frontier questions previously inaccessible or impractical using current or conventional methods. 
Business challenges rarely show up in the appearance of a perfect data problem, and even when data are abundant, 
practitioners have difficulties to incorporate it into their complex decision-making that adds business value. In 2012, 
McKinsey & Company conducted a survey of 1,469 executives across various regions, industries and company 
sizes, in which 49 percent of respondents said that their companies are focusing big data efforts on customer 
insights, segmentation and targeting to improve overall performance [10] An even higher number of respondents 60 
percent said their companies should focus efforts on using data and analytics to generate these insights. Yet, just 
one-fifth said that their organizations have fully deployed data and analytics to generate insights in one business unit 
or function, and only 13 percent use data to generate insights across the company. As these survey results show, the 
question is no longer whether big data can help business, but how can business derive maximum results from big 
data. 
Predictive Analytics is the use of historical data to forecast on consumer behavior and trends [18]. It is the use of 
past/historical data to predict future trends. This analysis makes use of the statistical models and machine learning 
algorithms to identify patterns and learn from historical data [25]. Predictive Analysis can also be defined as a 
process that uses machine learning to analyze data and make predictions [22].  
future, and 68% sight competitive advantage as the prime benefit of predictive analysis [17]. Broadly speaking, 
predictive analysis can be applied in ecommerce for product recommendation, price management, and predictive 
search. Typically a large e-commerce site offers thousands of product and services for sale. Navigating and 
searching for a product out of thousands on a website could be a major setback to consumers. However, with the 
invention of recommender system, an E-Commerce site/application can quickly identify/predict products that 
closely suit the consumer’s taste [24].  
Using a technology called Collaborative Filtering a database of historical user preferences is created. When a new 
customer access the ecommerce site, the customer is matched with the database of preferences, in order to discover a 
preference class that closely matches the customer taste. These products are then recommended to the customer [24]. 
Another technology that is used in ecommerce is the clustering algorithm. Clustering algorithm works by identifying 
groups of users that have similar preferences. These users are then clustered into a single group and are given a 
unique identifier.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
83 
New customers cluster are predicted by calculating the average similarities of the individual members in that cluster. 
Hence a user could be a partial member of more than one cluster depending of the weight of the user’s average 
opinion [24]. Advanced analytics is defined as the scientific process of transforming data into insight for making 
better decisions. As a formal discipline, advanced analytics have grown under the Operational Research domain. 
There are some fields that have considerable overlap with analytics, and also different accepted classifications for 
the types of analytics [2].  
Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large 
amounts of log data from many different sources to a centralized data store. Flume deploys as one or more agents, 
each contained within its own instance of the Java Virtual Machine (JVM). Agents consist of three pluggable 
components: sources, sinks, and channels. Flume agents ingest incoming streaming data from one or more sources. 
Data ingested by a Flume agent is passed to a sink, which is most commonly a distributed file system like Hadoop. 
agent to be the sink of another. Flume sources listen and consume events. Events can range from newline-terminated 
strings in stdout to HTTP POSTs and RPC calls — it all depends on what sources the agent is configured to use. 
Flume agents may have more than one source, but at the minimum they require one. Sources require a name and a 
type; the type then dictates additional configuration parameters. 
Channels are the mechanism by which Flume agents transfer events from their sources to their sinks. Events written 
to the channel by a source are not removed from the channel until a sink removes that event in a transaction. This 
allows Flume sinks to retry writes in the event of a failure in the external repository (such as HDFS or an outgoing 
network connection). For example, if the network between a Flume agent and a Hadoop cluster goes down, the 
channel. Sink is an interface implementation that can remove events from a channel and transmit them to the next 
agent in the flow, or to the event’s final destination and also sinks can remove events from the channel in 
transactions and write them to output. Transactions close when the event is successfully written, ensuring that all 
events are committed to their final destination.  
Apache Sqoop is a CLI tool designed to transfer data between Hadoop and relational databases. Sqoop can import 
been transformed using MapReduce. Sqoop also has the ability to import data into HBase and Hive. Sqoop connects 
imported. Both import and export utilize MapReduce, which provides parallel operation as well as fault tolerance. 
During import, Sqoop reads the table, row by row, into HDFS. Because import is performed in parallel, the output in 
HDFS is multiple files.  
Apache’s Pig is a major project, which is lying on top of Hadoop, and provides higher-level language to use 
Hadoop’s MapReduce library. Pig provides the scripting language to describe operations like the reading, filtering 
and transforming, joining, and writing data which are exactly the same operations that MapReduce was originally 
designed for. Instead of expressing these operations in thousands of lines of Java code which uses MapReduce 
directly, Apache Pig lets the users express them in a language that is not unlike a bash or Perl script.  
Pig was initially developed at Yahoo Research around 2006 but moved into the Apache Software Foundation in 
2007. Unlike SQL, Pig does not require that the data must have a schema, so it is well suited to process the 
unstructured data. But, Pig can still leverage the value of a schema if you want to supply one. PigLatin is relationally 
complete like SQL, which means it is at least as powerful as a relational algebra. Turing completeness requires 
conditional constructs, an infinite memory model, and looping constructs.  
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
84 
SQL for querying. Being a SQL dialect, HIVEQL is a declarative language. In PigLatin, you specify the data flow, 
but in Hive we describe the result we want and hive figures out how to build a data flow to achieve that result. 
Unlike Pig, in Hive a schema is required, but you are not limited to only one schema. Like PigLatin and SQL, 
HiveQL itself is a relationally complete language but it is not a Turing complete language.  
Apache Zoo Keeper is an effort to develop and maintain an open-source server, which enables highly reliable 
distributed coordination. It provides a distributed configuration service, a synchronization service and a naming 
registry for distributed systems. Distributed applications use ZooKeeper to store and mediate updates to import 
configuration information. ZooKeeper is especially fast with workloads where reads to the data are more common 
than writes. The ideal read/write ratio is about 10:1. ZooKeeper is replicated over a set of hosts (called an ensemble) 
and the servers are aware of each other and there is no single point of failure.   
Figure 1. Intel Manager for Hadoop [3] 
MongoDB is an open source, document-oriented NoSQL database that has lately attained some space in the data 
industry. It is considered as one of the most popular NoSQL databases, competing today and favors master-slave 
replication. The role of master is to perform reads and writes whereas the slave confines to copy the data received 
from master, to perform the read operation, and backup the data. The slaves do not participate in write operations 
but may select an alternate master in case of the current master failure. MongoDB uses binary format of JSON-like 
documents underneath and believes in dynamic schemas, unlike the traditional relational databases. The query 
system of MongoDB can return particular fields and query set compass search by fields, range queries, regular 
expression search, etc. and may include the user-defined complex JavaScript functions. As hinted already, 
MongoDB practice flexible schema and the document structure in a grouping, called Collection, may vary and 
common fields of various documents in a collection can have disparate types of the data. 
The MongoDB is equipped with the suitable drivers for most of the programming languages, which are used to 
develop the customized systems that use MongoDB as their backend player. There is an increasingly demand of 
using MongoDB as pure in-memory database; in such cases, the application dataset will always be small. Though, it 
is probably are easy for maintenance and can make a database developer happier; this can be a bottle neck for 
complex applications that require tremendous database management capabilities. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
85 
In order to efficiently address the challenges of Big Data, the leading vendor developed the Oracle NoSQL database. 
It was built by Oracle Berkeley DB team and the Berkeley DB Java Edition is the building block of Oracle NoSQL. 
Berkeley DB is a robust and scalable key-value store and used as the underlying storage for several popular data 
model such as Amazon Dynamo, GenieDB, MemcacheDB and Voldemort [28].  
scalability, throughput, and reliability with little tuning efforts. It is an efficient and a resilient transaction model that 
significantly eases the development process of applications, involving Big Data. It is a distributed, scalable yet 
simple key-value pair data model that fully supports the ACID transactions and JSON format and integrated with 
Oracle Database and Hadoop. It offers scalable throughput with bounded latency. The model very well 
accommodates the horizontal scaling with dynamic annexation of new capacity, citing high availability; the design 
architecture of Oracle NoSQL does not support single point of failure, and lucid load balancing. Actually, the goals 
of high availability, rapid failover in the event of a node failure, etc. are achieved by replicating the storage nodes. 
which is able to handle big data requirements. It is a highly scalable and high-performance distributed database 
management system that can handle real-time big data applications that drive key systems for modern and successful 
businesses. It has a built-for-scale architecture that can handle petabytes of information and thousands of concurrent 
users/operations per second as easily as it can manage much smaller amount of data and user traffic. It has a peer to 
peer design that offers no single point of failure for any database process or function, in addition to the location 
independence capabilities that equate to a true network-independent method of storing and accessing data, data can 
be read and written anywhere. Apache Cassandra is also equipped with flexible/dynamic schema design that 
accommodates all formats of big data applications, including structured, semi-structured, and unstructured data. 
online.  
clusters of computers. It is designed to scale up from single servers to thousands of machines, with each offering 
local computation and storage. The basic notion is to allow a single query to find and collect results from all the 
cluster members, and this model is clearly suitable for Google's model of search support. One of the largest 
technological challenges in software systems research today is to provide mechanisms for storage, manipulation, and 
information retrieval on large amount of data. Web services and social media produce together an impressive 
amount of data, reaching the scale of petabytes daily (Facebook, 2012). These data may contain valuable 
information, which sometimes is not properly explored by existing systems. Most of this data is stored in a non-
structured manner, using different languages and format, which, in many cases, are in compatible. 
large datasets. Over the last years, commodity hardware became part of clusters, since the x86 platform cope with 
the need of having an overall better cost/performance ratio, while decreasing maintenance cost. Apache Hadoop is a 
framework developed to take advantage of this approach, using such commodity clusters for storage, processing and 
manipulation of large amount of data. The framework was designed over the MapReduce paradigm and uses the 
HDFS as a storage file system. Hadoop presents key characteristics when performing parallel and distributed 
computing, such as data integrity, availability, scalability, exception handling, and failure recovery.   
Hadoop is a popular choice when you need to filter, sort, or pre-process large amounts of new data in place and 
distill it to generate denser data that theoretically contains more information. Pre-processing involves filtering new 
data sources to make them suitable for additional analysis in a data warehouse.  Hadoop is a top-level open source 
project of the Apache Software Foundation. Several suppliers, including Intel, offer their own commercial Hadoop 
distributions, packaging the basic software stack with other Hadoop software projects such as Apache Hive, Apache 
Pig, and Apache Sqoop. These distributions must integrate with data warehouses, databases, and other data 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
86 
process or query.	  
Figure 2. Data Architecture with Hadoop Integrated with existing data system [12] 
MapReduce is the original massively scalable, parallel processing framework commonly used with Hadoop and 
other components such as the Hadoop Distributed File System (HDFS) and YARN. YARN can be described as a 
large-scale, distributed operating system for big data implementations. As Hadoop has matured, the batch-oriented, 
disk-intensive MapReduce’ s limitations have become more apparent as Big Data analytics moves to more real-time, 
streaming processing and advanced implementations such as the aforementioned machine learning.  
MapReduce is the model of distributed data processing introduced by Google in 2004. The fundamental concept of 
MapReduce is to divide problems into two parts: a map function that processes source data into sufficient statistics 
and a reduce function that merges all sufficient statistics into a final answer. By definition, any number of 
concurrent map functions can be run at the same time without intercommunication. Once all the data has had the 
map function applied to it, the reduce function can be run to combine the results of the map phases.  For large scale 
batch processing and high speed data retrieval, common in Web search scenarios, MapReduce provides the fastest, 
most cost-effective and most scalable mechanism for returning results. Today, most of the leading technologies for 
managing "big data" are developed on MapReduce. With MapReduce there are few scalability limitations, but 
leveraging it directly does require writing and maintaining a lot of code. 
Splunk is a general-purpose search, analysis and reporting engine for time-series text data, typically machine data. 
Splunk software is deployed to address one or more core IT functions: application management, security, 
compliance, IT operations management and providing analytics for the business. The Splunk engine is optimized for 
quickly indexing and persisting unstructured data loaded into the system. Specifically, Splunk uses a minimal 
schema for persisted data – events consist only of the raw event text, implied timestamp, source (typically the 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
87 
filename for file based inputs), source type (an indication of the general type of data) and host (where the data 
originated).  
Once data enters the Splunk system, it quickly proceeds through processing, is persisted in its raw form and is 
indexed by the above fields along with all the keywords in the raw event text. Indexing is an essential element of the 
canonical “super-grep” use case for Splunk, but it also makes most retrieval tasks faster. Any more sophisticated 
processing on these raw events is deferred until search time. This serves four important goals: indexing speed is 
increased as minimal processing is performed, bringing new data into the system is a relatively low effort exercise as 
no schema planning is needed, the original data is persisted for easy inspection and the system is resilient to change 
as data parsing problems do not require reloading or re-indexing the data. 
Apache Spark an open source big data processing framework built around speed, ease of use, and sophisticated 
analytics. It was originally developed in 2009 in UC Berkeley’s AMP Lab, and open sourced in 2010 as an Apache 
project. Hadoop as a big data processing technology has been around for ten years and has proven to be the solution 
of choice for processing large data sets. MapReduce is a great solution for one-pass computations, but not very 
efficient for use cases that require multi-pass computations and algorithms. Each step in the data processing 
workflow has one Map phase and one Reduce phase and you'll need to convert any use case into MapReduce pattern 
to leverage this solution. Spark takes MapReduce to the next level with less expensive shuffles in the data 
processing. With capabilities like in-memory data storage and near real-time processing, the performance can be 
several times faster than other big data technologies.  
Spark also supports lazy evaluation of big data queries, which helps with optimization of the steps in data processing 
workflows. It provides a higher-level API to improve developer productivity and a consistent architect model for big 
data solutions. Spark holds intermediate results in memory rather than writing them to disk, which is very useful 
especially when you need to work on the same dataset multiple times. It’s designed to be an execution engine that 
works both in-memory and on-disk. Spark operators perform external operations when data does not fit in memory. 
Spark can be used for processing datasets that larger than the aggregate memory in a cluster. Spark will attempt to 
store as much as data in memory and then will spill to disk. It can store part of a data set in memory and the 
remaining data on the disk. You have to look at your data and use cases to assess the memory requirements. With 
this in-memory data storage, Spark comes with a great performance advantage. 
Spark is written in Scala Programing Language and runs on the Java Virtual machine. It currently supports 
programming languages like Scala, java, python, Clojure and R. Other than Spark Core API, there are additional 
libraries that are part of the Spark ecosystem and provide additional capabilities in Big Data analytics. Spark 
Streaming is one among the spark library that can be used for processing the real-time streaming data. This is based 
on micro based on micro batch style of computing and processing. Spark SQL provides the capabilities to expose the 
visualization tools. MLlib, GraphX are some other libraries from spark. 
Thomas H. Davenport was perhaps the first to observe in his Harvard Business Review article published in January 
2006 (“Competing on Analytics”) how companies who orientated themselves around fact based management 
approach and compete on their analytical abilities considerably out-performed their peers in the marketplace. The 
reality is that it takes continuous improvement to become an analytics-driven organization. In a presentation given at 
the Strata New York conference in September 2011, McKinsey & Company showed the eye opening; 10-year 
category growth rate differences (see Figure 7, below) between businesses that smartly use their big data and those 
that do not.  
Amazon uses Big Data to monitor, track and secure 1.5 billion items in its inventory that are laying around 200 
fulfillment centers around the world, and then relies on predictive analytics for its ‘anticipatory shipping’ to predict 
when a customer will purchase a product, and pre-ship it to a depot close to the final destination. Wal-Mart handles 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
88 
more than a million customer transactions each hour [23], imports information into databases to contain more than 
2.5 petabytes and asked their suppliers to tag shipments with radio frequency identification (RFID) systems [8] that 
can generate 100 to 1000 times the data of conventional bar code systems. UPS deployment of telematics in their 
freight segment helped in their global redesign of logistical networks [6]. Amazon is a big data giant and the largest 
online retail store. The company pioneered e-commerce in many different ways, but one of its biggest successes was 
the personalized recommendation system, which was built from the big data it gathers from its millions of 
customers’ transactions. 
The U.S. federal government collects more than 370,000 raw and geospatial datasets from 172 agencies and sub 
agencies.  It leverages that data to provide a portal to 230 citizen-developed apps, with the aim of increasing public 
access to information not deemed private or classified. Professional social network LinkedIn uses data from its more 
than 100 million users to build new social products based on users’ own definitions of their skill sets. Silver Spring 
Networks deploys smart, two-way power grids for its utility customers that utilize digital technology to deliver more 
help manage energy use and maximize efficiency. Jeffrey Brenner and the Camden Coalition mapped a city’s crime 
trends to identify problems with its healthcare system, revealing services that were both medically ineffective and 
expensive. 
Today’s technology landscape is changing fast. Organizations of all shapes and sizes are being pressured to be data-
driven and to do more with less. Even though big data technologies are still in a nascent stage, relatively speaking, 
the impact of the 3V’s of big data, which now is 5v’s cannot be ignored. The time is now for organizations to begin 
planning for and building out their Hadoop-based data lake. Organizations with the right infrastructures, talent and 
vision in place are well equipped to take their big data strategies to the next level and transform their businesses. 
They can use big data to unveil new patterns and trends, gain additional insights and begin to find answers to 
pressing business issues. The deeper organizations dig into big data and the more equipped they are to act upon 
what’s learned, the more likely they are to reveal answers that can add value to the top line of the business. This is 
where the returns on big data investments multiply and the transformation begins. Harnessing big data insight 
delivers more than cost cutting or productivity improvement but it definitely reveals new business opportunities. 
Data-driven decisions always tend to be better decisions. 
1. Apache Software Foundation. (2010). Apache ZooKeeper. Retrieved April 5, 2015 from 
https://zookeeper.apache.org 
2. Chae, B., Sheu, C., Yang, C. and Olson, D. (2014). The impact of advanced analytics and data accuracy on 
operational performance: A contingent resource based theory (RBT) perspective, Decision Support Systems, 59, 
119-126. 
3. Chambers, C., Raniwala, A., Adams, S., Henry, R., Bradshaw, R., and Weizenbaum, N. (2010). Flume Java: 
Easy, Efficient Data-Parallel Pipelines. Google, Inc. Retrieved April 1, 2015 from 
http://pages.cs.wisc.edu/~akella/CS838/F12/838-CloudPapers/FlumeJava.pdf 
4. Cisco Systems. Cisco UCS Common Platform Architecture Version 2 (CPA v2) for Big Data with 
Comprehensive Data Protection using Intel Distribution for Apache Hadoop. Retrieved March 15, 2015, from 
http://www.cisco.com/c/en/us/td/docs/unified_computing/ucs/UCS_CVDs/Cisco_UCS_CPA_for_Big_Data_wi
th_Intel.html 
5. DATASTAX Corporation. (2013, October). Big Data: Beyond the Hype - Why Big data Matters to you [White 
paper]. Retrieved March 15, 2015 from https://www.datastax.com/wp-content/uploads/2011/10/WP-DataStax-
BigData.pdf 
6. Davenport, T & Patil, D. (2012). Data Scientist: The Sexiest Job of the 21st Century. Harvard Business Review, 
90, 70-76. 
7. Dhawan, S & Rathee, S. (2013). Big Data Analytics using Hadoop Components like Pig and Hive. American 
International Journal of Research in Science, Technology, Engineering & Mathematics, 88, 13-131. Retrieved 
from http://iasir.net/AIJRSTEMpapers/AIJRSTEM13-131.pdf 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
89 
8. Edwards, P., Peters, M. and Sharman, G. (2001). The Effectiveness of Information Systems in Supporting the 
Extended Supply Chain, Journal of Business Logistics 22 (1), 1-27 
9. EMC Corporation. (2013, January). EMC Accelerates Journey to Big Data with Business Analytics-as-a-
Service [White paper]. Retrieved from http://www.emc.com/collateral/white-papers/h11259-emc-accelerates-
journey-big-data-ba-wp.pdf 
10. EMC Corporation. Big Data, Big Transformations [White paper]. Retrieved from 
http://www.emc.com/collateral/white-papers/idg-bigdata-umbrella-wp.pdf 
11. EMC Solutions Group. (2012, July). Big Data-as-a-Service [White paper]. Retrieved from 
https://www.emc.com/collateral/software/white-papers/h10839-big-data-as-a-service-perspt.pdf 
12. Enterprise Hadoop: The Ecosystem of Projects. Retrieved from http://hortonworks.com/hadoop/ 
13. George, L. (2014, September). Getting Started with Big Data Architecture. Retrieved April 5, 2015, from 
http://blog.cloudera.com/blog/2014/09/getting-started-with-big-data-architecture/ 
14. IBM Corporation. IBM Big Data Platform. Retrieved from http://www-
01.ibm.com/software/in/data/bigdata/enterprise.html 
15. Intel Corporation. Big Data Analytics - Extract, Transform, and Load Big data with Apache Hadoop [White 
paper]. Retrieved April 3, 2015 from https://software.intel.com/sites/default/files/article/402274/etl-big-data-
with-hadoop.pdf 
16. McClary, D. (2013, June). Acquiring Big Data Using Apache Flume. Retrieved March 3, 2015 from 
http://www.drdobbs.com/database/acquiring-big-data-using-apache-flume/240155029 
17. Millard, S. (2013). Big Data Brewing Value in Human Capital Management – Ventana Research. Retrieved 
April 2, 2015 from http://stephanmillard.ventanaresearch.com/2013/08/28/big-data-brewing-value-in-human-
capital-management 
18. Mosavi, A. and Vaezipour, A. (2013). Developing Effective Tools for Predictive Analytics and Informed 
Decisions. Technical Report. University of Tallinn.  
19. Oracle Corporation. (2013, March). Big Data Analytics - Advanced Analytics in Oracle Database [White 
paper]. Retrieved March 5, 2015 from http://www.oracle.com/technetwork/database/options/advanced-
analytics/advanced-analytics-wp-12c-1896138.pdf?ssSourceSiteId=ocomen 
20. Oracle Enterprise Architecture. (2015, April). An Enterprise Architect's Guide to Big Data - Reference 
Architecture Overview [White paper]. Retrieved from 
http://www.oracle.com/technetwork/topics/entarch/articles/oea-big-data-guide-1522052.pdf 
21. Penchikala, S. (2015, January). Big Data Processing with Apache Spark - Part 1: Introduction. Retrieved from 
http://www.infoq.com/articles/apache-spark-introduction 
22. Puri, R. (2013). How Online Retailers Use Predictive Analytics To Improve Your Shopping Experience. 
Retrieved April5, 2015 from http://blogs.sap.com/innovation/analytics/how-online-retailers-use-predictive-
analytics-to-improve-your-shopping-experience-0108060 
23. Sanders, N.R. (2014). Big Data Driven Supply Chain Management: A Framework for Implementing Analytics 
and Tuning Information into Intelligence, 1st Edition, Pearson, NJ 
24. Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. (2002). Recommendation systems for large e-commerce: 
Scalable neighborhood formation using clustering. In Proceedings of the fifth international conference on 
computer and information technology, 1.  
25. Shmueli, G. & Koppius, O. (2011). Predictive Analytics in Information Systems Research. MIS Quarterly, 
35(3), pp. 553-72. 
26. Sorkin, S. (2011). Splunk Technical Paper: Large-Scale, Unstructured Data Retrieval and Analysis Using 
Splunk. Retrieved April 15, 2015 from https://www.splunk.com/content/dam/splunk2/pdfs/technical-
briefs/splunk-and-mapreduce.pdf 
27. The Bloor Group. IBM and the Big Data Information Architecture. Retrieved April 3, 2015 from 
http://insideanalysis.com/wp-content/uploads/2014/08/BDIAVendor-IBMv01.pdf 
28. Tiwari, S. (2011). Using Oracle Berkeley DB as a NoSQL Data Store. Retrieved April 5 2015 from 
http://www.oracle.com/technetwork/articles/cloudcomp/berkeleydb-nosql-323570.htm 
29. Transparency Market Report. (May, 2015).Big Data Applications in Healthcare likely to Propel Market to 
US$48.3 Bn by 2018. Retrieved June 26, 2015, from 
http://www.transparencymarketresearch.com/pressrelease/big-data-market.htm 
30. Villars, R. L., Olofson, C. W., & Eastwood, M. (2011, June). Big data: What it is and why you should care. IDC 
White Paper. Framingham, MA: IDC. 
Issues in Information Systems 
Volume 16, Issue II, pp. 81-90, 2015 
90 
31. Wolpe, T. (2015, March). How Facebook is speeding up the Pesto SQL query engine. Retrieved April 3, 2015, 
from http://www.zdnet.com/article/how-facebook-is-speeding-up-the-presto-sql-query-engine 
32. Zahari et al. (2010). Spark: Cluster Computing with Working Sets. Retrieved April 7, 2015, from 
http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf 

Removed lines from 48077-157-151840-1-10-20200520.pdf:
THÔNG TIN VÀ TƯ LIỆU - 2/2020 23
BIG DATA VÀ XU HƯỚNG ỨNG DỤNG TRONG HOẠT ĐỘNG THÔNG TIN - THƯ VIỆN
ThS Nguyễn Lê Phương Hoài
Viện Thông tin Khoa học xã hội 
● Tóm tắt: Big Data là một thuật ngữ được sử dụng để chỉ những bộ dữ liệu khổng lồ, chủ yếu không 
có cấu trúc, được thu thập từ nhiều nguồn khác nhau. Big Data có nhiều tác động, ứng dụng và được 
xem như một yếu tố quyết định đến việc phát triển, mang lại lợi thế cạnh tranh cho tổ chức. Bài viết 
tổng quan lược sử các quan điểm về Big Data, đồng thời nhấn mạnh vào các xu hướng ứng dụng 
trong hoạt động thông tin - thư viện.
● Từ khóa: Big Data; dữ liệu lớn; hoạt động thư viện.
BIG DATA APPLICATION IN LIBRARY AND INFORMATION ACTIVITIES
● Abstract: Big Data is a term used to refer to huge, mostly unstructured datasets, collected from 
a variety of sources. Big Data has many impacts, applications and is considered as a decisive 
factor in the development, bringing competitive advantages to the organization. The overview paper 
summarizes the views on Big Data and emphasizes application trends in library and information 
activities.
● Keywords: Big Data; library activities.
1. LƯỢC SỬ CÁC QUAN ĐIỂM VỀ BIG DATA
Hiện nay, chưa có một định nghĩa chính 
xác cho thuật ngữ Big Data. Big được ghi 
nhận lần đầu tiên trong báo cáo “Application-
controlled demand paging for out-of-core 
visualization” của Michael Cox và David 
thứ 8 (vào tháng 10 năm 1997). Ý tưởng đưa 
xử lý không còn đủ số lượng cần khảo sát, do 
thể phân tích được tất cả các thông tin [11].
Tháng 8 năm 1999, Steve Bryson, David 
Kenwright, Michael Cox, David Ellsworth, và 
Robert Haimes đăng bài “Visually exploring 
gigabyte data sets in real time” trên Tạp chí 
Communications of the ACM. Đây là bài viết 
đầu tiên sử dụng thuật ngữ “Big Data”. Các 
tác giả nhận định: “Những chiếc máy tính 
vực, cũng có thể là bất lợi; tính toán nhanh 
chóng tạo ra một lượng lớn dữ liệu. Nếu trước 
là lớn, thì bây giờ chúng ta có thể tìm thấy 
300 GB” [15]. 
Tháng 11 năm 2000, Francis X. Diebold 
Hiệp hội Kinh tế lượng bài viết “Big Data 
Measurement and Forecasting”. Trong bài 
viết này, tác giả khẳng định: “Gần đây, nhiều 
ngành khoa học như vật lý, sinh học, khoa 
học xã hội, vốn đang buộc phải đương đầu với 
khó khăn - đã thu được lợi từ hiện tượng Big 
Data và đã gặt hái được nhiều thành công. Big 
Data chỉ sự bùng nổ về số lượng (và đôi khi, 
chất lượng), khả năng liên kết cũng như độ 
sẵn sàng của dữ liệu, chủ yếu là kết quả của 
việc ghi lại dữ liệu và công nghệ lưu trữ” [4].
Tháng 2 năm 2001, Doug Laney - nhà 
phân tích của Tập đoàn Meta, công bố nghiên 
cứu “3D Data Managment: Controlling Data 
Volume, Velocity, and Variety”. Laney cho 
rằng, những thách thức và cơ hội nằm trong 
bằng mô hình “3Vs”: tăng về số lượng lưu trữ 
(Volume), tăng về tốc độ xử lý (Velocity) và 
tăng về chủng loại (Variety) [3]. Một thập kỷ 
sau, mô hình “3Vs” đã trở thành thuật ngữ 
dữ liệu lớn ba chiều. Nhiều công ty và tổ chức 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
24 THÔNG TIN VÀ TƯ LIỆU - 1/2020
24
dụng mô hình “3Vs” này để định nghĩa Big 
Data.
Tháng 12 năm 2008, Randal E. Bryant, 
Randy H. Katz, và Edward D. Lazowska 
công bố bài viết “Big-Data Computing: 
Commerce, Science and Society”, trong đó 
miêu tả: “Cũng như công cụ tìm kiếm đã làm 
thay đổi cách chúng ta tiếp cận thông tin, các 
ty, các nhà nghiên cứu khoa học, các học 
viên y tế, quốc phòng và tình báo,... Sử dụng 
nghệ máy tính suốt một thập kỷ qua. Chúng 
nó trong việc thu thập, sắp xếp và xử lý dữ 
liệu của tất cả các tầng lớp xã hội. Một khoản 
sẽ thúc đẩy phát triển và mở rộng nó” [13].
Tháng 2 năm 2010, Kenneth Cukier đăng 
“Data, data everywhere”. Cukier viết: “... thế 
mức không tưởng, và càng ngày càng được 
nhân rộng với tốc độ nhanh hơn bao giờ hết... 
Hiệu quả đã được thể hiện ở khắp mọi nơi, từ 
kinh doanh đến khoa học, từ chính phủ đến 
nghệ thuật. Các nhà khoa học và kỹ sư máy 
tượng này: Big Data” [8].
Tháng 5 năm 2012, Danah Boyd và Kate 
bài “Critical Question for Big Data” trên tờ 
Information, Communications and Society. 
Các tác giả định nghĩa Big Data như là “một 
hiện tượng văn hóa, công nghệ và học thuật 
dựa trên sự tương tác của: 1) Công nghệ tối 
thuật toán để thu thập, phân tích, liên kết, và 
so sánh các tập dữ liệu lớn; 2) Phân tích: tạo 
tuyên bố kinh tế, xã hội, kỹ thuật và pháp lý; 3) 
Thần thoại: niềm tin phổ biến rằng dữ liệu lớn 
biết mà trước đây không thể, với hào quang 
của sự thật, khách quan, chính xác” [2].
Sau đó, Gartner - công ty nghiên cứu và 
tư vấn công nghệ thông tin - bổ sung thêm 
rằng “Big Data ngoài 3 tính chất (số lượng, 
tốc độ xử lý và chủng loại) thì còn phải cần 
khám phá sâu vào sự vật/sự việc và tối ưu 
hóa các quy trình làm việc” [5]. Cùng quan 
điểm đó, Tan Jee Toon cho rằng Big Data 
mọi thứ xung quanh chúng ta, từ các thiết bị 
kỹ thuật số như di động, video, hình ảnh, tin 
nhắn tới các thiết bị cảm biến, các máy móc 
hội. Big Data có đặc điểm là được sinh ra với 
khối lượng (volume), tốc độ (velocity), độ đa 
dạng (variety) và tính xác thực (veracity) rất 
lớn [16].
Năm 2014, Gartner đưa ra khái niệm 
mới về Big Data bằng mô hình “5Vs”, gồm: 
Volume (khối lượng), Velocity (tốc độ), 
Variety (tính đa dạng), Veracity (tính xác 
thực) và Value (giá trị). Trong đó: Volume là 
khối lượng Big Data được tạo ra mỗi ngày. 
phân tán, nơi mà dữ liệu chỉ được lưu trữ một 
bởi phần mềm. Velocity là tốc độ dữ liệu mới 
được tạo ra và tốc độ dữ liệu chuyển động. 
giữ chúng trong các cơ sở dữ liệu. Variety là 
các kiểu khác nhau của dữ liệu. Công nghệ 
có cấu trúc truyền thống (được lưu trữ trong 
các bảng hoặc các cơ sở dữ liệu quan hệ) và 
phi cấu trúc (bao gồm các thông điệp, trao 
đổi của mạng xã hội, các hình ảnh, dữ liệu 
cảm biến, video, tiếng nói...). Veracity là tính 
hỗn độn hoặc tính tin cậy của dữ liệu. Công 
kiểm soát những loại dữ liệu này. Value là giá 
trị của dữ liệu. Việc tiếp cận Big Data sẽ chỉ 
thành những thứ có giá trị. Đây là khái niệm 
đầy đủ về 5 tính chất của Big Data [5].
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 25
THÔNG TIN VÀ TƯ LIỆU - 1/2020 25
2. XU HƯỚNG ỨNG DỤNG BIG DATA TRONG HOẠT 
ĐỘNG THÔNG TIN - THƯ VIỆN
Ngày nay, một lượng lớn dữ liệu số có thể 
các mạng xã hội. Theo Howe D. (2008): “Chỉ 
riêng trong các lĩnh vực nghiên cứu khoa học, 
trưởng theo cấp số nhân” [7]. Số lượng dữ 
nhiều lĩnh vực khác nhau và dữ liệu lớn (Big 
Data) được sử dụng rộng rãi trong các lĩnh 
vực, tổ chức với nhiều mục đích khác nhau. 
hành vi tiêu dùng của khách hàng, để đề xuất 
trên thông tin thu thập được (Ebay, Facebook, 
Google...). Các cơ sở nghiên cứu khoa học sử 
khoa học mới, ví dụ như xây dựng bản đồ 
gene của con người,... Việc sử dụng Big Data 
trong hoạt động thông tin - thư viện đã bắt 
đầu được quan tâm nghiên cứu. Campbell 
D. Grant, Cowan Scott R. (2016) phân tích 
và dữ liệu liên kết [1]. Kim Young Seok (2017) 
khuôn mặt Chernoff [9]. Gerrard D., Mooney 
J. và Thompson D. (2017) xem xét kiến trúc 
phân tích dữ liệu, các bộ tài nguyên được bảo 
thời gian tới [6]. Waqar Ahmed và Kanwal 
Ameen (2017) tổng quan các khái niệm về 
thư viện [17]. Ye Chunlei (2017) nghiên cứu 
trong thư viện đại học [18]. Zhan Ming, Widén 
Gunilla (2018) nghiên cứu vai trò của thư 
viện công cộng trong thời đại Big Data [20]. 
Li Shuqing; Jiao Fusen; Zhang Yong; Xu Xia 
(2019) nghiên cứu các vấn đề và thay đổi của 
dịch vụ người dùng tin [10],... Các nghiên cứu 
trong thời đại Big Data. Bàn về xu hướng ứng 
tiếp nào, nhưng có thể tổng hợp thành các xu 
hướng chính như sau:
Một là, tổ chức lưu trữ, bảo quản dữ liệu
Marydee Ojala nhận định: “Các thư viện 
ngày nay. Bộ sưu tập các tài nguyên số được 
các thư viện. Khối lượng và tính đa dạng dữ 
thư viện phải có phương pháp tổ chức lưu trữ, 
bảo quản dữ liệu hợp lý” [12]. Nguồn dữ liệu 
thư viện bao gồm: nguồn dữ liệu mô tả tài liệu 
thư viện, nguồn tài nguyên số hóa tài liệu thư 
viện, nguồn tài liệu số thư viện bổ sung qua 
việc mua hay sử dụng chung, nguồn dữ liệu 
khảo sát thư viện, dữ liệu định tính, dữ liệu 
tương tác xã hội,... Trước đây, các thư viện 
băng, đặt trong các cơ sở lưu trữ. Trước tác 
mạng công nghiệp lần thứ tư, các thư viện 
chi phí hiệu quả. Dữ liệu được lưu trữ theo 
hai cách, cả trên các thiết bị ngoại tuyến (thẻ 
nhớ SD, ổ cứng ngoài, ổ đĩa flash) và lưu trữ 
trực tuyến trên đám mây. Với phương thức 
kết hợp sử dụng băng từ để bảo quản lưu trữ, 
được yêu cầu, và sử dụng lưu trữ đám mây 
cho các Big Data. Các thư viện hướng đến 
thư viện (bao gồm cả tài nguyên vật chất và 
dữ liệu), xác định nhu cầu của người dùng 
thư viện. Trong thời gian tới, khi các yêu cầu 
mới thúc đẩy việc sử dụng Big Data, các thư 
viện hướng tới việc thu nhận, tổ chức lưu trữ 
dữ liệu (lưu trữ vật lý trong các máy chủ hoặc 
trong các cơ sở dữ liệu), bảo tồn dữ liệu và 
phổ biến dữ liệu, làm cho dữ liệu có sẵn trong 
qua các sản phẩm trực quan. Các thư viện 
tiến tới xây dựng, tạo lập hệ thống bảo quản 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
26
kỹ thuật số (bảo tồn cả tài nguyên số và siêu 
dữ liệu mô tả) có thể phát triển trong nhiều 
năm tới để đáp ứng với các yêu cầu mới. 
Hai là, cung cấp sản phẩm, dịch vụ thư 
viện mang tính cá nhân hóa, tùy chỉnh 
 Hiện nay, các thư viện đang có xu 
vụ trực tuyến. Nhiều thư viện đang sử dụng 
facebook, instagram để quảng bá các sản 
phẩm và dịch vụ thư viện. Các phương tiện 
thư viện. Cùng với các dữ liệu khảo sát thư 
viện, dữ liệu định tính (thông qua phỏng vấn, 
bảng trò chuyện...), dữ liệu tương tác xã hội 
(từ các trang truyền thông xã hội)... các thư 
thư viện. Từ đó, thư viện cung cấp các sản 
cầu của người dùng. Tuy nhiên, trong thời 
gian tới, “sự gia tăng của Big Data làm cho 
nhanh hơn, cho phép các thư viện vượt ra 
học tập và phân tích hiệu suất nghiên cứu” 
[19]. “Trong kỷ nguyên Big Data, không chỉ 
Big Data để đổi mới.” [10]. “Big Data có thể 
cũng sẽ thay đổi cho phù hợp” [17]. Các nhà 
có thể tác động đến hoạt động thông tin - thư 
viện, chuyển đổi phương thức cung cấp dịch 
vụ và tích hợp các hệ thống xử lý. Các hỗ trợ 
cạnh tranh để thư viện thu hút người dùng tin. 
Theo Li Shuqing, Jiao Fusen, Zhang Yong, 
Xu Xia: “Các vấn đề và tiềm năng của các thư 
dữ liệu, công nghệ, dịch vụ và người dùng 
tin. Sử dụng Big Data hiện có và xem xét các 
tại theo quan điểm của người dùng tin, thư 
viện có thể đưa ra các ý tưởng, phương pháp 
có trong các thư viện số” [10]. Đồng thời, nhu 
dùng tin. Kim Young Seok cho rằng: “Bằng 
thực, các thư viện có thể thiết kế các dịch 
tin. Big Data cung cấp thông tin chuyên sâu 
dùng tin, từ đó tạo ra trải nghiệm cá nhân 
hóa” [9]. Ví dụ, người dùng tin tìm kiếm trên 
gì người dùng tin gõ ở mục tìm kiếm, tần suất 
tìm kiếm, số lần tham khảo danh mục tài liệu, 
số lần xem mô tả tài liệu,... được thu thập và 
phân tích để tối ưu trải nghiệm, tạo cơ hội lớn 
hóa. Đặc biệt, với các công cụ phân tích dự 
báo của Big Data, thư viện sẽ nắm được thị 
hiếu, nhu cầu chính xác để cung cấp các sản 
phẩm, dịch vụ phù hợp với người dùng tin 
trong thời gian thực.
Ba là, ứng dụng dịch vụ phân tích dự báo
Giống như hầu hết các ngành khác, phân 
tích dự báo sẽ là một sự thay đổi lớn, quan 
trọng trong các cơ quan thông tin - thư viện. 
hoạt động hiệu quả hơn, đồng thời làm thay 
người dùng tin. Theo cách truyền thống, mối 
khá đơn giản. Người dùng thư viện nộp tiền, 
làm thẻ thư viện và đổi lại, họ được phục vụ 
trong các dịch vụ khác nhau của thư viện. Tuy 
nhiên, mối quan hệ này đang dần thay đổi 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020 27
viện. Người dùng thư viện cung cấp dữ liệu 
hành vi người dùng. Thông qua các dữ liệu cá 
nhân như lịch sử sử dụng tài liệu thư viện, lịch 
sử tìm kiếm, cách thức, thói quen tìm kiếm,... 
các công cụ Big Data phân tích dữ liệu, đưa 
ra thông tin chi tiết, xác định khuynh hướng, 
nhu cầu sử dụng thư viện, nhu cầu tài liệu 
người dùng thư viện trong tương lai; các công 
cầu,... Simovic Aleksandar (2018) nhận định: 
“Các công cụ Big Data kết hợp với các thuật 
toán khoa học cho phép các thư viện dự đoán 
lai, giúp dự báo tốt hơn các vấn đề phát sinh 
tin tốt nhất cho người dùng tin” [14]. Về phía 
thư viện, việc sử dụng tài nguyên Big Data 
của người dùng tin, có thể đưa ra các ý tưởng 
các dịch vụ hiện có trong thư viện kỹ thuật số. 
Đồng thời, căn cứ vào các kết quả phân tích, 
dự báo, thư viện có thể xác định thời gian, 
và dịch vụ thư viện đến người dùng thư viện. 
Về phía người dùng thư viện, dựa vào các 
kết quả dự báo về hành vi tìm kiếm, tra cứu, 
sử dụng thư viện, các hệ thống khuyến nghị 
(Recommendation Engine) sẽ gửi đến người 
dùng tin các tài liệu có thể họ quan tâm.
Bốn là, mở rộng dịch vụ chăm sóc 
thư viện, đặc biệt trong môi trường thư viện 
điện tử, thư viện số. Các thư viện đang cố 
gắng để hiểu được người dùng thư viện, giúp 
các thắc mắc, nhu cầu của mình. Big Data 
hoạt, tạo ra giá trị từ quá trình xây dựng mối 
quan hệ thân thiết với người dùng thư viện. 
Cùng với Big Data, hệ thống trả lời tự động 
(như Chatbot) không cần sự trợ giúp của 
con người phát triển tương ứng, giúp tăng 
hiệu quả phân tích dữ liệu Big Data. Hiện 
nay, nhiều thư viện sử dụng Chatbot để giao 
tiếp, trao đổi với người dùng thư viện, tiếp 
các nhu cầu của người dùng. Khi người dùng 
viện, họ có rất nhiều thắc mắc và muốn được 
giải đáp. Chatbot sẽ đưa ra các gợi ý, hỗ trợ 
từng bước một, cung cấp thông tin về các 
sản phẩm, dịch vụ của thư viện cho người 
dùng. Chatbot được thiết kế và phát triển để 
đối thoại. Qua những dữ liệu người dùng thu 
thập được, công cụ phân tích dữ liệu Big Data 
tiến hành phân tích, xác định những nhu cầu, 
dùng thư viện. Bên cạnh đó, Chatbot nhắc 
viện như thời hạn trả tài liệu, thời hạn đổi thẻ 
sử dụng,... Đặc biệt, Chatbot giúp thư viện 
chủ động hỗ trợ 24/7, tăng trải nghiệm tối 
đa cho người dùng thư viện mọi lúc. Chatbot 
lưu lại lịch sử đối thoại, thông tin người dùng 
trong chính thư viện. Chatbot hỗ trợ các thư 
viện khai thác Big Data phục vụ người dùng. 
Trong tương lai, số thư viện sử dụng Chatbot 
tính năng và lợi ích mà Chatbot mang lại. 
Cùng với đó, thông qua dữ liệu người dùng, 
NGHIÊN CỨU - TRAO ĐỔI
THÔNG TIN VÀ TƯ LIỆU - 2/2020
28
các thư viện có thể phân tích, dự đoán các 
các sản phẩm, dịch vụ thông qua phân tích 
hiện các giải pháp kịp thời. 
Có thể thấy, Big Data giúp tối ưu hóa hoạt 
động thư viện bằng việc thu thập, phân tích 
thông tin, tăng trải nghiệm của người dùng 
tin bằng cách cá nhân hóa thư viện số. Cùng 
với đó, Big Data có thể giúp các thư viện tiến 
hành phân tích dự báo, tìm ra các đặc điểm 
chung dự báo thị hiếu đọc, tình trạng sử dụng 
các cơ sở dữ liệu. Không chỉ vậy, Big Data tạo 
dùng tin trong quá trình sử dụng thư viện.
1. Campbell D. Grant, Cowan Scott R. (2016). “The 
Paradox of privacy: revisiting a core library value in 
an age of big data and linked data”, Library trends, 
Vol. 64, No. 3, P. 492-811.
2. 
Boyd, 
Kate 
(2012). 
Critical Question for Big Data, Information, 
Communications and Society.
3. Doug Laney (2001). “3D Data Managment: 
controlling Data Volume, Velocity, and Variety”, 
Application Delivery Strategies, Meta Group. 
File:949.
4. Francis X. Diebold (2000). “Big Data Dynamic 
and Forecasting”, Discussion of Reichlin and 
Watson paper, in Economics and Econometrics, 
Eighth World Congress of the Econometric Society. 
5. Gartner (2013). Survey Analysis: Big Data Adoption 
in 2013 shows substance behind the hype. 
6. Gerrard, D., Mooney, J. , Thompson, D. (2017). 
“Digital Preservation at Big data scale: proposing a 
step - change in preservation system architectures”, 
Library Hi Tech, http://doi.org/10.1108/LHT-06-
2017-0122, truy cập ngày 17/10/2019.
7. Howe D. (2008). “The future of biocuration”, 
Nature 455, P. 47-50.
8. Kenneth Cukier (2010). “Data, data everywhere”, A 
special report on managing information, Economist 
Newspaper, Volume 394.
9. Kim Young Seok (2017). “Big data analysis of 
chernoff face method”, Journal of Documentation, 
Vol. 73, No. 3, P. 466-480.
10. Li Shuqing; Jiao Fúen; Zhang Yong; Xu Xia 
(2019). “Problems and Changes in Digital Libraries 
services”, Journal of Academic Librarianship, Vol, 
45.
11. Michael Cox, David Ellsworth (1997). “Application 
- Controlled Demand Paging for Out - of - Core 
Visualization”, Report NAS-97-010, NASA Ames 
Research Center.
12. Marydee Ojala (2018). “Big Data and AI: 
technology, transparency, and trust”, http://www.
infotoday.com, truy cập ngày 14/11/2019.
13. Randal E. Bryant, Randy H. Katz, và Edward D. 
Lazowska (2008). “Big - Data Computing: Creating 
in 
Commerce, 
Science and Society”, Computing Community 
Consortium, http://www.cra.org/ccc/initiatives, truy 
cập ngày 14/11/2019.
14. Simovic Aleksandar (2018). “A Big Data smart 
institution”, Library Hi Tech, Bradford, Vol. 36, Iss. 
3, tr.498-523
15. Steve Bryson, David Kenwright, Michael 
Cox, David Ellsworth, Robert Haimes (1999). 
“Visually exploring gigabyte data sets in real”, 
Communications of the ACM, Vol. 42, No.8, tr.83-
90.
16. Tan Jee Toon (2014). “Dữ liệu lớn, nhân 
tố thay đổi “cuộc chơi” của doanh nghiệp”, 
http://vneconomy.vn/cuoc-song-so/du-lieu-
lon-nhan-to-thay-doi-cuoc-choi-cua-doanh-
nghiep-20140422025542917.htm, truy cập ngày 
14/11/2019.
17. Waqar Ahmed, Kanwal Ameen (2017). “Defining 
the field of information and library management”, 
Library Hi Tech News, p. 21-24.
18. Ye Chunlei (2017). “Research on the key 
technology of big data service in university library”, 
the Institude of Electrical and Electronics Engineers, 
Inc. Conference Proceedings, Piscataway.
19. Yu Jen Chien (2016). “Library Data, Big Data or 
Better Data: Challenges from the Field”, ASIST 
Meeting, Proceeding of ASIST annual meeting, 
Vol 53, No. 1.
20. Zhan Ming, Widén Gunilla (2018). “Public 
libraries: roles in big data”, The Electronic library, 
Vol. 36, No.1, P. 133-145.
(Ngày Tòa soạn nhận được bài: 26-12-2019; 
Ngày phản biện đánh giá: 10-03-2020; Ngày chấp 
nhận đăng: 15-03-2020).
NGHIÊN CỨU - TRAO ĐỔI

Removed lines from big-data.pdf:
Adding Value to Manufacturing, Retail, Supply Chain, and 
Oklahoma State University, Stillwater, OK 74078 
The concept of big data has been around for many years.  Only in the last few years have organizations 
started to understand how they can use big data to gain insightful knowledge about their business operations, 
which is enabling them to make better business decisions. While there is no single definition, big data 
usually works on the principles of four Vs - Volume, Velocity, Variety, and Veracity. As the name suggests, 
big data is really big, meaning a huge amount of data is being generated daily, reaching the scale of 
petabytes. This data comes in all forms -  structured, semi-structured, and unstructured and is pouring in 
from all directions and generated by many systems and devices, such as transactional systems, log files, 
GPS devices, smartphones, RFID readers, surveillance cameras, sensor networks, Internet of Things (IoT), 
and social media. Finally, as big data becomes an important asset for enterprises, the focus is also on the 
trustworthiness of data and its sources. 
According to Gartner, Inc., “Big data is high-volume, high-velocity and high-variety information assets that 
demand cost-effective, innovative forms of information processing for enhanced insight and decision 
making.”a In this article, we first elaborate on the big data concept and present the storage and processing 
technologies that have been developed to deal with big data.  We then briefly discuss the evolution of 
traditional analytical processing to today’s big data analytics.   Through several applications and use cases, 
we illustrate how big data analytics is adding value to manufacturing, retail, supply chain, and logistics 
operations.  Finally, we conclude by discussing key challenges that businesses have to face as the use of 
big data analytics becomes more widespread. 
Regardless of the decision to be made - optimized production/work schedules, accurate forecasts, customer 
preferences - data nowadays has the potential to help businesses succeed more than ever before.  From an 
organizational perspective, big data is a holistic approach of obtaining actionable insight to create a 
competitive advantage over others.1 There are two distinct approaches to applying big data - improve the 
propositions. A challenge that organizations increasingly face is finding and working with trusted data. 
Working with inaccurate and untrusted data can be worse than having no data at all. As data requirements 
and regulations become more complex, organizations must be aware of where all their data is coming from, 
where it is getting stored, and who is interacting with this data as conclusions are drawn.2 
a https://www.gartner.com/it-glossary/big-data/ 
2 
What is Big Data? 
of data that needs to be handled and tracked, the speed at 
which the information is flowing into online systems, and 
regular basis. Because of the changes happening in the Web 
environment, new definitions for big data have been 
proposed, with a focus on technologies that handle this data. 
O’Reilly defines big data as “Big data is data that exceeds 
the processing capacity of conventional database systems. 
The data is too big, moves too fast, and doesn’t fit the 
structures of traditional database architectures. To gain 
value from this data, organizations must choose an 
alternative way to process it.”3 
To understand how big data is transforming businesses, we 
focus on the size of data in storage.4 Size is important but 
there are other aspects to big data namely variety, volume, 
and more recently, veracity.2 Together they are called the 4 
Vs of big data: Volume, Velocity, Variety, and Veracity. 
databases, data warehouses, and data marts.6 Here, the data 
is uploaded to operational data stores using Extract, 
Transform, and Load (ETL) tools which extract data from 
internal and external sources, transform the data to fit the 
operational needs, and finally load the data into the data 
warehouse. The key point is that the data is getting cleaned, 
transformed, and cataloged before being made available for 
data mining and online analytical functions. This traditional 
data warehouse approach discourages the incorporation of 
new data sources until they are cleansed and integrated.  
Since data is ubiquitous these days, big data storage 
environments need to be “magnetic” in nature, attracting 
data from all sources. Hence, big data calls for Magnetic, 
Agile, and Deep (MAD) analysis skills, which differs from 
the traditional data warehousing approach. Given the growing number of data sources and the sophisticated 
tools for data analysis, big data storage should allow analysts to easily process and use data rapidly. 
Solutions like distributed file systems and Massive Parallel Processing (MPP) databases are available 
nowadays for providing high query performance and platform scalability. Non-relational databases such as 
Not Only SQL (NoSQL) were developed for storing and managing unstructured data.7 These newer 
technologies aim for scalability, data model flexibility, and simplified application development and 
deployment. They separate data management and data storage and focus on high performance scalable data 
Volume. The ability to process a large 
social media, from Internet of Things to 
system logs, etc. 
Velocity. The rate at which data is 
getting created every second of the day. 
contributor, 
more 
data 
is 
generated and logged than ever before.5 
Also, the rapid adoption of social 
created a deluge of data. Advances in 
useful now. 
Variety.  It is the diversity of data 
which organizations are witnessing. 
processing a limited set of data, such as 
and 
logs. 
includes images, voice recordings, 
videos, and texts generated from 
media to deliver new insight. 
Veracity.  It is not just the quality of 
data, but also the trustworthiness of 
data sources.  Basic issues are the 
accuracy and applicability of data.  
uncertainty due to inconsistencies, 
incompleteness, ambiguities, etc. 
3 
storage, allowing management tasks to be written in the application layer instead of having it written in 
database specific languages.   
Why Big Data? 
When organizations adopt big data as a part of their business model, the first tangible question is usually 
what value this big data will provide to the company.7 Data must be used to make better decisions, to 
optimize resource consumption, and improve process quality and performance. It should also aim to 
perform precise customer segmentation, optimize customer satisfaction, and increase customer loyalty. 
from existing products and create additional revenue from new products. 
Newer Data Sources, Newer Opportunities 
The new sources of big data include industries which are taking a big step step towards digitization, and as 
a result, data growth in the past few years has been phenomenal. Some of the areas where data is coming 
from include social media, internet browsing pattern data, advertising response data, financial forecasts, 
location information, driving patterns, vehicle diagnostics, and traffic and weather data from sensors, 
monitors, and forecast systems. Other sources of data include data from healthcare, where the healthcare 
industry is implementing electronic medical records and digital imaging, which is used for short-term public 
health monitoring and long-term research programs.  Similarly, low cost gene sequencing can generate 
effectiveness in life sciences.8 Another area is data from video surveillance which is transitioning from 
patterns for security and service enhancement. Transportation and logistics industry has been generating 
and storing enormous amount of data coming from sensors, GPS transceivers, RFID tag readers, smart 
meters, cell phones, material handling equipment enabled with sensors, etc. This data can be used to 
opportunities. 
information contained therein.9 It involves applying algorithmic processes to derive insights. Analytics is 
used to extract previously unknown, useful, valid, and hidden patterns and information from large data 
sets.6 While the focus of analytics has been on inference, it can also provide prescriptive insights as 
explained later in this section.  Hence, analytics has a significant impact on research and technology, as 
businesses recognize its great potential in helping them gain competitive advantage. 
“Big data analytics is the use of advanced analytic techniques against very large and diverse data sets that 
include structured, semi-structured, and unstructured data from different sources, and in different sizes from 
terabytes to zettabytes.”b It helps in uncovering hidden patterns, unknown correlations, market trends, 
customer preferences and other useful information. Advanced analytics can help organizations discover 
what has changed and how they should react.  Analytics is the best way to discover new customer segments, 
identify the best suppliers, associate products of affinity, understand sales seasonality and so on.4 
Organizations are implementing specific forms of analytics tools and techniques which include data mining, 
statistical analysis, data visualization, artificial intelligence, machine learning, and other data capabilities 
b https://www.ibm.com/analytics/hadoop/big-data-analytics 
4 
which support analytics4. Though these techniques have been around for many years, organizations are 
using them now as most of these techniques adapt well to very large, multi-petabyte data sets. 
Big data’s worth is only realized when businesses can indulge in decision making using this data. To enable 
such data-driven decision making, organizations must use efficient processes to turn the high volume of 
fast moving and diverse data into meaningful insights. Analyzing big data allows researchers and businesses 
harness their data and use it to identify new opportunities which in turn leads to better and smarter business 
moves, more efficient operations, higher profits and satisfied customers and an overall competitive 
advantage.6 Big data analytics could be viewed as a sub-process in the complete process of knowledge 
extraction from big data.  
As organization began to adopt data analytics in the late 1990s and early 2000s, they faced many hurdles.  
professionals. Analysts used to spend more time collecting and preparing data than analyzing it.  They 
focused on finding more accurate and reliable solutions to business problems, while keeping the solutions 
simple at the same time so that business users could understand it.  Some examples of tools used during 
this time period are SAS, a tool for building backend data inference and modeling; Oracle and Teradata, 
detailed solution suites for easy development of solutions; IBM CPLEX, a tool for solving large 
optimization problems; and Cognos and MicroStrategy, tools for visualization, mostly in the form of 
reports. 
In late 2000s, social media giants like Google and Facebook and other internet-based companies in general 
started uncovering, collecting, and analyzing newer types of data which later evolved into big data. In 
addition to the data generated by companies in their internal operations and transactions, newer data was 
brought in from external sources including public data sources, social media, and mobile devices. Analysts 
realized this new data was qualitatively different (e.g., unstructured text, pictures, audio, and video) along 
with the much larger volumes as compared to internal company data.  This led to the development of newer 
tools and technologies, examples of which are Hadoop, a pioneer in distributed data storage and processing 
with low cost, flexibility, and scalability; Python and R, open source programming languages with vast and 
ever-evolving libraries for statistical data analysis; Tableau, Looker, and Microsoft Power BI, popular 
visualization products to develop, customize, and build visually appealing and interactive web dashboards. 
Descriptive, Predictive, and Prescriptive Analytics 
Analyzing data is not limited to deriving insights from the past, but it can also help businesses in predicting 
future outcomes and optimizing business performance. Currently organizations use three types of analytics 
at different stages in their decision-making process - Descriptive, Predictive, and Prescriptive analytics as 
shown in Figure 1. The latter two are also referred to collectively as advanced analytics. 
Descriptive analytics does exactly as the name suggests, ‘describe’ or summarize the data and convert it 
into something useful. It is the most basic type of analytics and almost 90% of the organizations today use 
this technique. Descriptive analytics is the analysis of historical data using data aggregation or data mining 
and lies at the bottom of the big data analytics value chain. However, it is extremely valuable because it 
organization’s future.  
Descriptive analytics is an important step to make raw data understandable to its users, and it helps in 
answering questions like “What is happening?” Consider for example, a metric that companies get from 
5 
web servers using Google Analytics tools, namely page views.  It can be used to determine if a strategy was 
a success or not. The main objective in descriptive analytics is to find the reasons behind the previous 
it can help the organization in strategizing.  
The majority of the statistics we use comes from descriptive analytics – e.g., calculations as simple as 
averages and standard deviations. Descriptive models use basic mathematical and statistical techniques to 
derive key performance indicators that can highlight the historical trends in data. STATA, MS Excel, and 
SPSS represent the older generation of descriptive analytics tools, while R and Python are quickly becoming 
the preferred tools in industry because of vast open-source libraries and the ease of development and 
deployment. Descriptive analytics can yield historical insights into an organization’s production, inventory 
levels, sales, operations, financials, and customer behavior.  
Figure 1. Analytics Framework by Tom Davenport26 
Predictive analytics can be defined as the ability to “predict” what might happen and a better understanding 
of future outcomes. It is one of the more sophisticated types of analytics techniques and employs statistical 
techniques and machine learning. It is used to detect clusters, tendencies, and exceptions, and to predict 
future trends, making it a valuable tool for forecasting. The foundation of predictive analytics is probability. 
It takes the data which the user has and tries to fill in the missing data values with best guesses. It helps in 
finding the answer to ‘What could happen?’ With properly tuned models, predictive analytics can support 
complex forecasting in marketing and sales. This helps an organization to set realistic goals for business, 
restrain expectations, and do effective planning. 
Tools used to apply predictive modeling vary by the nature of model’s complexity, but some commonly 
used tools are SAS, MATLAB, R, Python, among others. The common functionality of these tools is that 
they combine historical data found in POS, ERP, CRM, and HR systems to identify patterns in the data and 
apply algorithms such as random forest and Generalized Linear Model (GLM) for prediction, and K-means 
clustering for identifying clusters. Finally, simulation can be employed to statistically predict the outcomes 
of specific decision scenarios.   
6 
An application of predictive analytics is to produce credit scores, which are used by financial services to 
determine the probability of customer making timely payments. Other business uses include, how sales 
might close at the end of a year, inventory level forecasts, predicting what items a customer might purchase 
together and other customer purchasing patterns. Despite all the advantages that predictive analytics brings 
to the table, it is important to understand that forecasting is just an estimation, and its accuracy depends on 
the quality and stability of data.  
Prescriptive analytics is the most sophisticated analytics approach which makes use of optimization 
techniques to explore a given set of options and prescribe the best possible solution for a given scenario. 
As the name suggests, it “prescribes” a solution to a specific problem. One approach is machine learning 
neurons using training data sets.  Once trained, the neural network model can suggest the optimal course of 
action supporting the business objective for a given set of business inputs. Simulation, a predictive analytics 
tool at its core, can also be part of a powerful prescriptive analytics approach when combined with 
appropriate search or optimization techniques.  Prescriptive analytics not only predicts ‘What will happen?’, 
but also determines “What the company should do?” It provides recommendations for the actions to be 
taken to achieve optimal business performance. Because it has power to suggest optimal solutions, 
prescriptive analytics is the ultimate frontier for advanced analytics. 
Prescriptive analytical models are complex in nature. However, when implemented efficiently, prescriptive 
analytics can have a significant impact on the decision-making effectiveness of the organization. Technical 
advancements such as cloud computing have made deployment of these complex models much easier. 
prescriptive analytics to optimize production and inventory decisions in supply chains, optimize customer 
experience, and to make sure that the right product is being delivered at the right time.  Airline systems use 
travel factors, demand levels, purchasing patterns, timings, etc., in order to maximize the revenue generated.  
hence, they are ensuring to choose the right kind of analytics solutions to reduce operational cost, enhance 
service quality, and increase ROI. 
Big Data Analytics Applications and Use Cases 
Supply chain activities produce a huge amount of data, which is being continuously generated by systems 
and devices such as POS, ERP, SCM, RFID, GPS, blogs, and wiki entries, not to mention the unlimited 
data generated from sources like CCTVs, digital clickstreams, imagery, social media posts, and discussions 
on various forum platforms. Advanced connected devices and technologies which support today’s supply 
chain such as sensors, smart devices, and tags are continuously gathering real-time data and providing an 
end-to-end visibility in the supply chain. It becomes the task of supply chain managers to tap and process 
this data to make insightful decisions which could help boost productivity and reduce costs. 
7 
Application 
Area
Technique/Technology/
Supplier Insight Program Greater insight into suppliers' financial stability, 
performance, and ability to provide services.
Achieved product excellence, reduction in time-to-
market through co-development and co-production. 
Better alignment between engineers, suppliers, and 
customers.
Better service level; accurate prediction of customer 
needs and demand; automated planning and 
forecasting operations.
Inventory management with streaming analytics, real-
time data delivery and updates every few hours, and 
accurate performance analysis of each store.
Machine learning-based 
accuracy and greater profit margins.
application
Accurate forecasts, reduction in delivery time by 
upto 50%, and better service levels.
& Co.
Analytics application 
using Intel's Trusted 
Better tracking of in-store items using RFID tags; 
updating item location and inventory; helping 
salesperson track misplaced item to avoid lost sales.
Data-intensive 
Increase in forcasting accuracy; reduced inventory, 
stockouts, and obsolescence; better access to 
company's logistics needs.
Anticipatory shipping
before actual customer orders.
and packing area.
Drone-based delivery
Goods delivered to locations less than 30 minutes 
Cloud-based 3D 
Optimize picking accuracy, inventory turns, and 
warehouse productivity in real-time using inputs 
from sensors, such as  shelf weight and weight on 
forklift.
Quality early-warning 
Reduced rework, increased productivity and cost 
savings, higher quality standards, and improved 
service levels, by detecting and prioritizing quality 
related issues much sooner in the supply chain.
Greater demand and supply visibility, better 
distribution channel management, better service level, 
and improved inventory management.
collection.
Co.
Real-time monitoring and 
Greater visibility for customers, better pallet 
management, optimized space utilization, greater 
labor productivity, inventory accuracy of 99.9%, and 
improved customer satisfaction.
UPS
Optimized 55,000 delivery routes in North America, 
saving close to $400 million annually. Reducton in 
transportation.
shipment information, reduction in mileage and cost, 
and improved CO2 efficiency.
Resilience 360
Accuracy in risk detection, prevent production 
inefficiencies and revenue losses, maintain service 
levels, and reduce emergency cost by efficiently re-
routing shipments in case of unforeseen events.
analyze potential business opportunities. Real-time 
in a given location.
quality of address information is poor. Real-time 
ddress verification to optimize route planning.
DHL
Applications of Big Data Analytics
IBM
Supply Chain 
and 
8 
Applications and Use Cases in Manufacturing 
Raytheon, a major U.S. based defense contractor and industrial corporation, made use of data analytics to 
reduce costs within their supply chain and production operations. They developed a Supplier Insight 
program, which integrated structured and unstructured data from internal and external sources.10 With more 
than 10,000 suppliers, they needed a platform that could provide rapid, data-driven decision making 
capability. With this new system, they could track suppliers’ financial stability, performance, and their 
ability to provide services in the face of disruptive events. Raytheon was able to immediately identify if a 
supplier could provide what they needed, and quickly made decisions that reduced any adverse impact on 
their customers. Supplier Insight has allowed them to negotiate the cost better, by engaging in long-term 
contracts with suppliers for multiple programs.10 They now have an ability to look across all their suppliers 
and programs to achieve cost reductions. Raytheon has also developed smart factories which have the 
capacity to handle big data coming from different sources like sensors, instruments, CAD models, internet 
transactions, simulations, and digital records in the company, which equips them with real-time control of 
various elements of the production processes. For example, their Immersive Design Center (IDC) makes 
use of a 3-D immersive environment to achieve product excellence and decrease time-to-market through 
co-development and co-production of products by immersive data visualization and interaction.10 This also 
resulted in better alignment between their engineers, suppliers, and customers. They work together to refine 
the design and detect potential problems without the work and rework associated with expensive prototypes, 
resulting in reduced costs.10  
Lennox International, a U.S. based cooling and heating devices manufacturing company, integrated 
their expansion throughout North America.11 With the help of machine learning algorithms, they accurately 
predicted customer needs, while understanding customer demand better. It also helped the company to 
automate its planning and forecasting operations. 
Many companies gather data on supplier information and purchasing volumes for annual supplier 
performance review, spend analysis, and cost savings analysis functions to support strategic decisions.  For 
example, a pharmaceutical company created a database of all the bids submitted for packaging.12 This data 
was then evaluated to understand the cost structure of suppliers and to create detailed cost models for 
different packaging options. Such models can help in the selection of the most cost-effective supplier for 
new packaging.12 Another example is how IoT with its network of sensors embedded in millions of devices 
can enable new opportunities in manufacturing. For example, real-time information on a machine’s 
condition can initiate a production order for a spare part, which then can be shipped using a drone to the 
plant engineer for replacing the faulty or near faulty part.12 It also helps in determining when and how 
critical maintenance is required by a specific machine, thereby avoiding costly equipment breakdowns and 
improving the overall production efficiency.  
Daily production needs to be monitored to maintain the efficiency and output of a company. Big data 
analytics uses the data collected from operational machines, employee records, and data logs of the number 
of units produced, to provide insights to the operations manager, helping him/her to make changes that are 
profitable for the company. Manufacturers are also exploring predictive analytics to realize significant 
savings in product testing and improving product quality. Since different products and parts require 
different tests, instead of performing numerous quality tests on each part, data mining and pattern 
recognition can be used to determine the type and number of tests truly needed for each part or product.13  
9 
Applications and Use Cases in Retail 
Walmart, the number one fortune 500 company, has the world’s largest private cloud, which helps support 
real-time data feeds to its decision makers. Walmart’s Data Café based at their Bentonville, Arkansas 
headquarters takes care of most of this cloud architecture.14 Their original data infrastructure only enabled 
managers to get weekly reports, which prevented them from making decisions based on real-time market 
conditions. Also, the reports were standardized with little room for customization. Data café, which was 
built on SAP’s HANA in-memory analytics engine, enabled inventory management with streaming 
analytics, and provided an enterprise view of timely information flow for a large cross-sectional staff 
looking to resolve every-day business issues.14 The data delivered through this system is almost real-time 
and updated every few hours.  Furthermore, the system was designed to be responsive to providing reports 
and queries required by managers in the given time frame, which helped them gain timely insight and make 
better decisions. These insights are derived from “200 streams of internal and external data which includes 
40 petabytes of recent transactional data, and can be manipulated, modeled, and visualized.”14 The 
importance of near real-time insights is crucial since it helps managers respond to challenges in real-time 
as they arise. For example, on Black Friday, Walmart’s Data Café provides near real-time insights on the 
performance of east-coast stores, which enables Walmart to make pricing adjustments for west-coast stores 
before they open.14 During a recent Halloween, sales analysts were able to see that two stores were not 
selling a novelty cookie that was very popular in most stores. Using near real-time data from Data Café, it 
was discovered that simple stocking oversight led to the cookies not being put on shelves in these stores.14 
The company was able to react in real-time to avoid additional lost sales. Data Café also provides automated 
alerts to managers when a metric falls below a threshold in a department. This tool has reduced the problem-
solving time from weeks to minutes using reliable internal and external sources of data. 
Levi Strauss & Co, a leading American clothing company, provides better in-store shopping experience for 
items using IoT technology coupled with advanced analytics. Levi’s in collaboration with Intel® 
implemented a solution using Intel’s Trusted Analytics Platform (TAP), which helped salespersons to 
quickly find misplaced items in the store.15 This application made use of RFID tags woven into clothing 
items, in-store antenna sensors installed in the ceiling of the store to continuously track the RFID tags, and 
cloud-based analytical tool built on TAP for detailed analysis. This technology helped determine when 
items are no longer in their correct place or no longer available at that time. TAP algorithms use data 
collected overnight to determine the exact location of various groups of items, and during store hours 
sensors track the location of items and an algorithm determines if an item is in its assigned location. If an 
item is placed in its assigned group location, no action is generated by the algorithm.  Suppose a pair of 
jeans is lying in the T-shirt section or left in the fitting room, the TAP algorithm will generate an alert on 
the mobile application instructing the salesperson to put the item back in its assigned location.15 This helps 
the salesperson to keep the item where it belongs and avoid lost sales. Levi’s also aims to generate customer 
insight using big data analytics with the data collected from sensors tracking customers’ in-store behavior 
to better understand their preferences.15 
Groupe Danone, a French multinational food-product corporation, found itself making accurate predictions 
only 30 percent of the time for responses to promotional offers, which was resulting in significant losses to 
the company.11 When they implemented machine learning in their planning architecture, they saw 
significant improvement in both sales and forecasting. Similarly, Granarolo, an Italian dairy company, used 
machine learning to increase its forecasting accuracy by 5 percent, decreased delivery times by up to 50 
percent of the original time, which resulted in better service levels.11 Morrisons, one of UK’s largest food 
10 
retailers, was able to dramatically improve same store sales and achieve a 30% reduction in shelf gap and 
from Blue Yonder, which uses AI technology to “improve demand planning and reinvigorate replenishment 
based on customer behavior in every store.”16 Blue Yonder’s data-intensive forecasting methods deployed 
as cloud-based services is making such advanced capabilities accessible to other retailer’s as well.12 
Applications and Use Cases in Supply Chains and Warehouses 
In supply chain operations, planning and forecasting are among the most data-driven operations, which use 
an array of supply chain planning tools supported by ERP systems. With the use of supply chain analytics, 
it is now possible to re-envision the planning processes by using external and internal data sources to make 
real-time decisions based on market trends, uncertainty, seasonality, and other fluctuations.  
IBM understood the value of big data analytics early and employed it in optimizing their supply chain 
operations. They have used various analytical tools to solve a range of problems, and a few of them are 
discussed here.17 IBM’s Quality Early-Warning System (QEWS) was typically deployed upstream at 
suppliers, IBM’s operations, and in the field.  QEWS detects and prioritizes quality related issues much 
sooner than the traditional quality control processes. Analyzing big data coming from across their supply 
chain, IBM was able to reduce rework, increase productivity, ensure higher quality standards, and improve 
customer satisfaction, leading to significant cost savings. For a company like IBM, ensuring correct 
inventory levels with so many business partners was challenging. They made use of IBM Buying Analysis 
Tool, which not only provided demand and supply visibility, but ensured better distribution channel 
management, delivery of the right product at the right time to meet customer demand, while maintaining 
proper inventory levels. IBM also used a tool named Accounts Receivable, which uses advanced analytics 
to optimize the resources needed to collect revenues. They also make use of supply chain social listening, 
disrupt the supply chain.17 It also helps them obtain timely information and feedback on their products. As 
an early adopter, IBM has been using predictive and prescriptive analytics in its supply chain over the last 
several years. 
Warehousing is another area where big data analytics is creating new opportunities.  Logivations, a German 
supply chain solutions provider, offers a cloud-based 3D warehouse layout planning and optimization tool, 
camera-guided AGVs and tracking, and various other supply chain analytics solutions.18 Such technologies 
existing warehouse by simulating new configurations. Another example is the analysis of images and videos 
captured by AGVs, and sensor inputs including shelf weight and weight on the forklift, to monitor picking 
accuracy, inventory turns, and warehouse productivity in real-time.12 Also, forklift drive picking 
productivity and route optimization can be achieved by analyzing the route choices and driving behaviors.12  
A leading forklift provider is looking into all these opportunities, and figuring out how a forklift truck can 
be used as a big data hub - collecting  real-time data to identify additional sources of waste in the warehouse 
operations, using a hybrid of analytics and ERP and WMS data. Amazon is another warehouse automation 
pioneer, deploying Kiva robots that bring the items (racks) to the picking and packing area in their 
fulfillment centers. With increasing pressure to reduce order-to-delivery times, warehouses are turning to a 
flexible automation strategy by using autonomous technologies such as Amazon’s Kivac robots and 
GreyOrange’s Butlerd system to increase their picking efficiency.  Amazon has also tried to deliver goods 
c https://www.amazonrobotics.com/#/  
d https://www.greyorange.com/butler-goods-to-person-system  
11 
to people living less than 30 minutes away from an Amazon warehouse or distribution center via a drone. 
Amazon has also patented an “Anticipatory Shipping” technology to identify which orders should be 
packed and pushed into the logistics network before the actual customer orders are placed.12 
Merchandise Warehouse Co. (MW), a logistics provider of multi-temperature warehouse services in the US 
mid-west, provides services such as tempering, inspection, blast freezing, temperature monitoring, labeling, 
import/export, and packaging.19 With such operations there is little room for error, since clients’ food 
products could get spoiled if they are not maintained at correct temperatures.19 MW needed real-time 
enable quality assurance with comprehensive traceability.  They wanted this for all operations including 
inspections and holds.19 Technologies such as CCTV, WMS, electronic data interchange, mobile 
computers, and scanners were employed to help track and analyze data to get real-time information in the 
warehouse and manage inventory. It helped MW’s customers gain visibility by having on-line access to 
temperatures, activity reports, and information about inventory levels. MW’s solution also includes tools 
for pallet management for tracing every pallet from the time it arrives in the warehouse to until it leaves. 
inventory to marked temperature zones were provided by the new system. It also ensures greater labor 
productivity and accuracy using workflow-based warehouse management and could automate processes 
designed for specific customer needs. MW reaped various other benefits from this initiative like accurately 
capturing billing events in real-time resulting in reduced labor used for billing and paperwork. The system 
helped the company deal with the issue of “catch weight”, where the actual weight of the product,  especially 
meat, varies when it hits the retail shelves, a common problem in cold storage warehouses and food 
industry.19  Increased customer satisfaction levels were also achieved, since clients had real-time access to 
information and reports when needed. The solution helped MW achieve an inventory accuracy of 99.9 
percent from a previous 98.6 percent.19 
Applications and Use Cases in Logistics 
Logistics companies need to keep the goods moving at all times, even in the face of disruptions such as 
storms, cargos getting stranded due to ship crashes, and geopolitical events in order to keep the businesses 
running.  A Netherlands based logistics management company uses big data analytics on Microsoft’s Azure 
cloud to keep its customers informed about the number of goods in each container, their location at a given 
time, and expected delivery times.20 Purchase orders are tracked using mobile applications to identify 
challenges which could delay the delivery of an order. Tariff calculations and fees related to the movement 
of shipping containers are calculated by another application which can be accessed by the client, giving 
them a greater insight into financial risks.20 These mobile applications make use of big data analytics in 
internal supply chain operations to provide actionable business insights. Previously the time it took to 
identify a challenge and develop a solution to address it could be anywhere from 3 to 9 months. With the 
use of big data technologies, this time has been brought down to a couple of weeks depending upon how 
complex the problem is.20  
Companies managing their own supply chains and those outsourcing to third-party logistics providers 
manage a massive flow of freight, goods, and products daily while at the same time creating vast data sets. 
Millions of shipments are tracked daily from origins to destinations, generating information such as the 
content, weight, size, location, and route of each individual shipment, across a large number of networks. 
Companies are exploiting and analyzing these large data sets to improve their operational efficiencies, 
effectiveness, and customer service. A study by the Council of Supply Chain Management Professionals 
shows that 93 percent of shippers and 98 percent of 3PL providers feel that data-driven decision making is 
12 
a crucial supply chain activity.21 Also, 71 percent of these believe big data improves performance and 
quality. Logistics companies can utilize big data analytics to consolidate, interpret, and store the data 
coming from various sources for immediate or future use based on their requirements.  
Courier and delivery companies like UPS use real-time routing of deliveries using the trucks’ geo-location 
and traffic information data. UPS spent almost 10 years developing its On-Road Integrated Optimization 
and Navigation system (ORION) to optimize close to 55,000 routes in North America in its delivery 
network.22 This system saves the company $300 million to $400 million annually by saving about 100 
million miles per year, which is a reduction of 10 million gallons of fuel consumed and reducing CO2 
emissions by almost 100,000 metric tons.22  Data mining techniques also help logistics companies deliver 
services with fewer delivery attempts, by using predictive analytics to predict when a customer is more 
likely to be available at home.12 Costs and carbon emissions can also be reduced by selecting the right mode 
of transportation for deliveries. An example is the use of supply chain analytics to understand the priority 
and can be delivered by rail.12  
Better transportation planning can be achieved with the use of Transportation Management System (TMS) 
which can help identify future shipping patterns, optimize routes, carrier selection, or loads, and secure 
necessary capacity. This is achieved by tracking shipment frequency and identifying the endpoints of supply 
chains by studying precise inbound and outbound statistics. Direct application of predictive analytics is 
helping logistics providers make real-time decisions which result in reduced costs, greater reliability, and 
improved customer satisfaction. For example, data streams produced by sensors on delivery trucks, beacons 
which broadcast their presence to nearby devices such as computers and smartphones, radar devices, and 
employing simulation models.23 When a shipment is going to be late, a carrier can make real-time 
adjustments to prevent bottlenecks further down the supply chain.23 
DHL, a global logistics provider, has extensively explored big data analytics in their supply chain activities 
and is currently employing several smart systems around their services. Increasing the last mile efficiencies 
is often the most expensive step in the supply chain.24 Last mile optimization is an extensively studied area 
and researchers have found promising applications of big data analytics here. Data analytics is applied to 
achieve real-time optimization of delivery routes, where streams of data are processed to maximize the 
performance of the delivery fleet. Rapid processing of real-time information supports the goal of route 
optimization on the last mile, saving time in the delivery process. When the vehicles are loaded and 
unloaded, manual sequencing of shipments is eliminated by the use of sensors, and dynamic calculations 
are used to find the optimal delivery sequence. Based on real-time traffic conditions on the road, telematic 
databases are used to change the delivery route automatically. DHL’s SmartTruck uses data mining, 
machine learning, and other data analytics techniques to optimize the initial tour planning based on 
incoming shipment on a daily basis.24 Dynamic routing system recalculates the routes depending on the 
traffic situations and delivery times. This also results in cost reduction and improved CO2 efficiency by 
reducing the miles travelled. 
It is vital for robust supply chains to be able to cope with unforeseen events in today’s rapidly changing 
world. Apart from being flexible and resilient, businesses need accurate risk detection systems to keep 
running smoothly. Big data analytics and complex event processing algorithms are used to alert businesses 
when a pattern falls in the set of critical conditions such as tornadoes or floods in an area, or breakdown of 
fleet. These alert systems send a report on the probability and impact of the risk and provide suitable 
actionable insight to alleviate potential interruption. With this information on hand, customers can re-route 
their shipments or manage supplies from other distribution locations. DHL’s Resilience 360 risk 
13 
management solutions aims to provide such functionalities.24 It is equipped with two components, a risk 
assessment portion and supply chain monitoring instruments, both operating in real time. This improves the 
resilience of the supply chain and prevents production inefficiencies and revenue losses. Resilience 360 is 
designed to maintain prescribed service levels, protect sales and operations, and reduce emergency costs, 
creating a competitive advantage for the company.24 
Future economic development is often modeled on global transportation of goods and services. The type of 
goods shipped indicate the local demand and supply preferences. Logistics providers make use of big data 
shipments by their distribution networks. These shipment records are a valuable resource for market 
intelligence research, and logistics providers refine this data to substantiate existing market research. 
Regression analysis techniques are used to produce demand and supply forecasts with the use of the 
shipment records and market research outcomes. The primary target group for these advanced data analytics 
services are small and medium-sized enterprises, which lack capacity to conduct their own market research. 
The results from regression-based analytics have high predictive value, which can help these enterprises 
serve a larger customer base, and generate accurate forecasts based on industry, geography, and product 
category. DHL Geovista is one such online geo-marketing tool available for small and medium-sized 
enterprises to analyze potential business opportunities.24  
shipments more accurately.24 Customer’s delivery address verification is a fundamental requirement for 
any logistics provider. This can be troublesome in developing countries and other remote areas, where the 
area. Address Management uses daily freight and parcel delivery data and matches this data with reference 
data and returns the incorrect incoming data with validated data from the database, in order to verify the 
address in real-time and optimize route planning for retailers and public sector entities.  
Other Applications 
There are several other applications of big data analytics which a company can encounter on a regular basis. 
Locating a new store is a strategic decision for a company, and big data analytics could play an important 
role here. Extensive data analysis is performed by the analysts in exploring customer data, demographic 
factors, retailer network, location of other competitors in the area, and market potential. A recent example 
of this is the location for Amazon’s HQ2. Visualizing the growth of a company has become easier with the 
use of data analytics, since it is now possible to quickly compare the performance matrix of different sites 
and identify the reasons behind such results. Predictive analytics comes in handy in analyzing the market 
and gaining insight on questions related to global growth strategy, site relocation, new product introduction, 
and supplier selection. 
business profitable.25 Data analytics tools simplify the process of price formation, which not only accounts 
for the cost of production of an item, but also the spending capacity of the customers and presence of 
competitors in the market. Price flexibility, buying patterns of the customers, competitors’ prices, and 
seasonality are analyzed using the data coming from various sources. Machine learning algorithms help 
to changes in prices. Furthermore, using real-time price optimization techniques, retailers can attract new 
customers and retain existing customers by adjusting the price as per market trends. Recommendation 
engines is another great way of predicting customers’ behavior, since they give a retailer insight into 
customers’ reviews and opinions. It also helps the retailers to increase sales and stay abreast with trends. 
14 
Based on machine learning algorithms, recommendation engines make adjustments depending on customer 
preferences, previous shopping and browsing experience, demographic data, need, and usefulness. 
Collaborative or content-based data filtering is used in this process to gain useful insight which gives 
leverage to retailers on customers’ opinions.  
Companies often fail to understand what big data is, its benefits, and more importantly the computing and 
the human infrastructure required to realize its true potential. Without a clear understanding of the concept 
of big data, adopting and implementing a project using big data tools can seriously challenge its success. 
Having discussed various applications and use cases of implementing big data technologies in 
manufacturing, retail, supply chain, and logistics, it is important to understand the associated challenges. 
they decide to invest in technologies using big data.  
reducing their dependence on legacy systems. Even though the industry is shifting its focus to the digital 
age with adoption of IoT and artificial intelligence, it is still a long way before the full potential of big data 
is realized. Industry has to develop an awareness of the various elements of the big data landscape, which 
include sensors to social media that collect data, in-memory to cloud for data storage, data mining to deep 
learning to convert data into useful business insights or actions.  Any new business solution will involve a 
significantly. Most people are resistant to change, and it shows in companies when workers stick to to an 
old way of thinking and doing work. An example is the use of Excel, which to the present day remains one 
of the popular tools in many companies, despite having  many limitations when compared to newer tools.27 
While there is a need to educate industry to change this legacy mentality, there is no need for an abrupt or 
complete shift to newer tools. A viable option is to slowly augment existing systems with big data analytics 
tools and capabilities.   
With the phenomenal increase in the size of data, the problem of storage space for big data has become a 
real problem for many companies. Cloud storage is soon becoming the only viable alternative with the ever-
increasing need for storage space.  With the maturity of the cloud computing infrastructure, which includes 
storage, applications, and computing platforms, companies are beginning to consider shifting to the cloud 
infrastructure for most of their computing needs. But transitioning from the traditional in-house computing 
infrastructure to the cloud infrastructure has its own challenges.  According to McAfee, “Most organizations 
that have been around awhile have a hodgepodge of hardware, operating systems, and applications, often 
described as ‘legacy spaghetti’.”28 First, companies have to address legacy system issues and simplify their 
system before moving to the cloud. For the most part, cloud is cost-effective compared to building and 
running an IT infrastructure.  However, a company needs to carefully evaluate the cost factor based on their 
specific needs, for example, in-house applications requiring continuous transfer of large data sets. 
analytics with specialized MS degrees in Data Science.  These degree programs are housed mostly in 
business schools or computer science departments.  Engineering schools to a large extent are still lagging 
in providing adequate training in data science to their graduates.   Data science professionals can manage 
and analyze large volumes of real-time data coming from multiple sources and in different formats. With 
several new technologies such as the NoSQL data management framework, Hadoop, cloud computing, and 
in-memory analytics, their skills are vital for the rapidly changing computing landscape. Given that 
engineering schools are still looking for the right curriculum mix (e.g., minors, degree options, and 
15 
certificates) to train engineers in data science, training employees at entry level is a challenging and 
expensive proposition for companies dealing with these newer technologies. When industry hires data 
science professionals, akin to software developers and programmers, they need guidance from subject 
matter experts (SMEs) to build the right tools and techniques that can help industry harness the power of 
big data in the long-run.  Industry needs to quickly educate SMEs to understand the big data analytics 
professionals.   
As seen in recent times, data privacy has become one of the major concerns of organizations. With recent 
threats like hacking of personal data, individuals and companies have become apprehensive about linking 
data from multiple sources as it may compromise an individual’s privacy. Also, with an increase in the 
number of connected devices within the industry, data security has also become a big concern and presently 
this risk is greater than ever. Big data analysis uses huge amounts of data for analysis and mining purposes 
to reach some meaningful conclusion, and security of this big data can be enhanced by using techniques 
such as authentication, authorization, and encryption.   
Effective flow and sharing of information among supply chain partners is critical to the success of today’s 
digital supply chains.  Unauthorized disclosure and data leakage of information shared among supply chain 
partners have been identified as two main threats in today’s digital supply chains.29 Visibility needed within 
a supply chain and consumers’ demand for transparency seem to be at odds with security requirements.  
With newer, secure technologies such as blockchain and data cleanroom, it is possible to achieve both 
visibility and transparency.30  Data cleanroom is a shared environment between two or more supply chain 
partners that is completely secure from external access and where each partner can decide the level of 
visibility to their data. Blockchain, a decentralized, distributed database is one of the most secure options 
available for supply chain partners for real-time information tracking.  Another important, but often 
overlooked challenge is the ethical use of data.  The legal infrastructure has not kept up with the rapid 
development in technology, which is able to collect and store vast amounts of consumer data with or without 
their knowledge.  While it may be legal, certain use of the data may be considered unethical.  Such actions 
may have a negative impact on a company as today’s consumers are more educated and have experienced 
negative consequences of such unethical usage. 
In a recent survey of supply chain professionals conducted by APQC, “lack of people with the needed 
skills” was identified as the biggest barrier to advanced analytics applications in industry.31  In addition, 
these employees need “a good understanding of the business to provide solid advice.”29  Resistance to 
change and lack of access to data across disparate systems were the second and third biggest barriers, 
respectively.  In addition to lack of access to data, issues such as inconsistent and unorganized data are also 
issues in some cases as different companies record their data in different formats, platforms, and systems.27 
useful insights.  
As companies make a push for big data analytics applications, they should first establish a clear business 
need such as “solving a problem or seizing an opportunity.”7  According to Watson, “big data initiatives 
should start with a specific or narrowly defined set of objectives rather than a ‘build it and they will come’ 
approach.”7 Pilot schemes are a good way to demonstrate the value of big data analytics.32  It is common to 
focus the initial business case for big data analytics on customer-centric objectives.7 The various 
applications and uses cases discussed earlier cover many different areas that have benefited from big data 
analytics. Whatever be the area, it is desirable that the pilot project address a problem tied to a specific 
16 
business outcome.  The pilot project should not only help solve a business problem, but also demonstrate 
the effectiveness of big data analytics for the organization and its stakeholders. Finally, for successful big 
data initiatives it is essential to have strong, committed sponsorship and alignment between the business 
and analytics strategies.7 In the early stages of adoption, the sponsor could be the CIO and then shifting to 
function-specific executives as business opportunities are identified.   
To benefit from big data analytics companies must also establish a data-driven decision-making culture, 
which calls for acting on insights from data rather than on pure managerial intuition.32 Promotion of data-
sharing practices, increased availability of training in data analytics, and communication of the benefits of 
data-driven decision making are some of the strategies for promoting a data-drive culture.7 While workforce 
training needs to focus on improving technological and digital proficiency, the future work environment 
also demands training in certain soft skills.  The work environment is changing with the rapid introduction 
of AI, automation, and analytics-driven solutions.  Workers need to be open to new ways of working and 
have openness to agility, adaptability, and working in teams to cope with a constantly changing external 
environment.  In the long-run, big data needs to become an integral part of the organization’s operating 
model. There also needs to be clear ownership for big data in the organization with leadership positions 
such as a chief analytics officer.32 Data science should become another established skill in the organization.   
during the development of this white paper.  We would like convey our appreciation to Scott Wahl for his 
guidance and feedback during the formative stages of this effort.  We would also like to thank John 
Ashodian, John Hill, Ying Tat Leung, Juan Ma, Hari Padmanabhan, and John Paxton for carefully reading 
an earlier version of this white paper and providing several constructive suggestions and feedback, which 
have helped us greatly improve the quality of the white paper. 
17 
1. Morten Brinch, Jan Stentoft, and Jesper K. Jensen, “Big Data and its Applications in Supply Chain 
Management: Findings from a Delphi Study,” Proceedings of the 50th Hawaii International Conference 
on System Sciences, 2017: 1351-1360.  
2. IBM Corporation, “The Path to Data Veracity,” IBM Big Data and Analytics Hub, May 2018, 
https://www.ibmbigdatahub.com/whitepaper/path-data-veracity 
3. DataStax Corporation, “Big Data: Beyond the Hype,” October 2013, 
https://www.datastax.com/resources/whitepapers/bigdata 
4. Phillip Russom, “Big Data Analytics,” TDWI Research, 2011, 
https://tdwi.org/research/2011/09/best-practices-report-q4-big-data-
analytics.aspx?tc=page0&m=1  
5. DXC Technology Company, “Five Industries Where Big Data is Making a Difference,” November 
2015, https://assets1.dxc.technology/analytics/downloads/DXC-Analytics-
Five_Industries_Where_Big_Data_is_Making_a_Difference-4AA5-6292ENW.pdf 
6. Nada Elgendy and Ahmed Elragal, “Big Data Analytics: A Literature Review Paper,” In: Perner P. 
(eds) Advances in Data Mining. Applications and Theoretical Aspects. ICDM 2014. Lecture Notes in 
Computer Science, vol 8557, Springer, Cham., 2014, https://link.springer.com/chapter/10.1007/978-
3-319-08976-8_16  
7. Hugh J. Watson, "Tutorial: Big Data Analytics: Concepts, Technologies, and Applications," 
Communications of the Association for Information Systems, 34 (2014), Article 65. 
http://aisel.aisnet.org/cais/vol34/iss1/65  
8. Richard L. Villars, Carl W. Olofson, and Matthew Eastwood, “Big Data: What It Is and Why You 
Should Care,” International Data Corporation, 2011. 
http://www.tracemyflows.com/uploads/big_data/idc_amd_big_data_whitepaper.pdf 
9. Sunil Tiwari, H.M. Wee, and Yosef Daryanto, “Big Data Analytics in Supply Chain Management 
Between 2010 and 2016: Insights to Industries,” Computers and Industrial Engineering, 115 (2017): 319-
330.  
10. Bob Trebilcock, “Supply Chain, Data Analytics, and Big Data,” Logistics Management, August 2015.  
https://www.logisticsmgmt.com/article/supply_chain_data_analytics_and_big_data  
11. Kaushik Pal, “How Machine Learning Can Improve Supply Chain Efficiency,” Techopedia, February-
2018.  https://www.techopedia.com/2/31846/trends/big-data/how-machine-learning-can-improve-
supply-chain-efficiency  
12.  McKinsey & Company, “Big Data and the Supply Chain: The Big-Supply-Chain Analytics 
Landscape: Part 1,” February 2016,  https://www.mckinsey.com/business-functions/operations/our-
insights/big-data-and-the-supply-chain-the-big-supply-chain-analytics-landscape-part-1#  
13. Lorenzo Romano, “Big Data Analytics: A Key Ingredient for Agility in Manufacturing,” May 2019, 
https://www.orange-business.com/en/blogs/big-data-analytics-key-ingredient-agility-manufacturing  
18 
14. Joe McKendrick, “Walmart’s Gigantic Private Cloud for Real-Time Inventory Control,” RT 
Insights.com, January 2017. https://www.rtinsights.com/walmart-cloud-inventory-management-
real-time-data/  
15. RT Insights team, “Levi’s Real-Time Tracking of Jeans: RFID in Retail,” RT Insights.com, April 
2016. https://www.rtinsights.com/rfid-in-retail-customer-experience-levis/ 
16.  JDA, “Store Replenishment at Morrisons,” 2017, https://jda.com/knowledge-center/collateral/by-
morrisons-case-study  
17.  Hans W. Ittmann, “The Impact of Big Data and Business Analytics on Supply Chain Management,” 
Journal of Transport and Supply Chain Management, 9, no. 1 (2015). 
https://jtscm.co.za/index.php/jtscm/article/view/165/331  
18. Logivation, https://www.logivations.com/en/solutions/plan/design_efficiency.php  
19. RT Insights team, “Using Mobile Device for a Real-Time Warehouse,” 2016, 
https://www.rtinsights.com/zebra-omnii-xt15-datek-real-time-warehouse/  
 20. Motifworks, “How Big Data Analytics Can Benefit Supply Chain and Logistics Industry,” 2017. 
https://motifworks.com/2017/02/23/how-big-data-analytics-can-benefit-supply-chain-logistics-
industry/  
 21. “2017 Third-Party Logistics Study,” https://jda.com/-/media/jda/knowledge-center/thought-
leadership/2017stateoflogisticsreport_new.ashx  
 22. UPS, “ORION Backgrounder,” 2019, 
https://www.pressroom.ups.com/pressroom/ContentDetailsViewer.page?ConceptType=Factsheet
s&id=1426321616277-282  
23. “Data-Driven Logistics: The Growing Use of Predictive Analytics,” July 2018, https://www.smith-
howard.com/data-driven-logistics-the-growing-use-of-predictive-analytics/ 
 24. Martin Jeske, Moritz Grüner, and Frank Weiẞ, “Big Data in Logistics – A DHL Perspective on How 
to Move Beyond the Hype,” December 2013. 
http://www.dhl.com/content/dam/downloads/g0/about_us/innovation/CSI_Studie_BIG_DATA.pdf  
25.  McKinsey & Company, “Big Data, Analytics, and the Future of Marketing and Sales,” March 2015, 
https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/Marketing%20and%20Sales/Our
%20Insights/EBook%20Big%20data%20analytics%20and%20the%20future%20of%20marketing%20sal
es/Big-Data-eBook.ashx 
26. Gurobi Optimization, “The Power of Analytics,” accessed September 8, 2019. 
http://www.gurobi.com/resources/prescriptive-analytics,  
27.  Transmetrics, “ Big Data and Big Roadblocks:  How the Logistics Industry can Overcome its Big 
Data Challenges,” March 2018, https://www.youredi.com/blog/logistics-industry-can-overcome-big-data-
28. Andrew McAfee, “What Every CEO Needs to Know About the Cloud,” Harvard Business Review, 
Nov. 2011: 124-132. 
19 
29. Bharat Bhargava, Rohit Ranchal, and Lotfi Ben Othmane, “Secure Information Sharing in Digital 
Supply Chains,” 3rd IEEE International Advanced Computing Conference, May 2013, 
https://www.cs.purdue.edu/homes/bb/Bhargava-Supply_Chain-Feb2013-india.pdf  
30. Megan Ray Nicholas, “How to Share Data Safely Across your Supply Chain,” 
https://www.smartdatacollective.com/share-data-safely-across-supply-chain/  
31. APQC, “APQC Quick Poll:  The Current State of Big Data & Advanced Analytics in Supply Chain,” 
May 2019, 
https://www.scmr.com/article/apqc_quick_poll_the_current_state_of_big_data_advanced_analytics_in_su
pply  
32. David Meer, “A Call to Action on Big Data,” Forbes, October 2014, 
https://www.forbes.com/sites/strategyand/2014/10/28/a-call-to-action-on-big-data/#6a4b6c22314  

Removed lines from KimAnh-HTKhoa.pdf:
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381804857
Tích Hợp Big Data Và Điện Toán Đám Mây: Động Lực Thúc Đẩy Thay Đổi Cho
Doanh Nghiệp.
Conference Paper · June 2024
CITATIONS
0
READS
376
1 author:
Vo Thi Kim Anh
Ton Duc Thang University
28 PUBLICATIONS   2 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Vo Thi Kim Anh on 29 June 2024.
The user has requested enhancement of the downloaded file.
KỶ YẾU
HỘI THẢO KHOA HỌC
KHOA CÔNG NGHỆ THÔNG TIN
LẦN 6
2024
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
i 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
KHOA CÔNG NGHỆ THÔNG TIN 
-------------- 
KỶ YẾU HỘI THẢO 
KHOA HỌC CÔNG NGHỆ LẦN 6 
Thành Phố Hồ Chí Minh, tháng 06 năm 2024 
(Lưu hành nội bộ) 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
ii 
BAN BIÊN TẬP 
1. TS. Nguyễn Hà Giang - Trưởng Khoa CNTT 
2. TS. Văn Thị Thiên Trang - Phó Trưởng Khoa CNTT 
3. ThS. Nguyễn Minh Tuấn - Phó Trưởng Khoa CNTT 
4. ThS. Trần Thành Công - Trợ lý Trưởng Khoa, Trưởng Ngành TMĐT 
5. ThS. Hoàng Văn Hiếu - Trưởng Ngành CNTT 
6. ThS. Võ Đình Ngà - Trưởng Ngành TKĐH 
7. ThS. Nguyễn Thị Hoài Linh - Trưởng Ngành KHDL 
8. ThS. Ngô Văn Công Bằng - Trưởng Bộ môn THUD 
9. ThS. Trương Nhã Bình - Trưởng Bộ môn Toán 
THƯ KÝ 
1. KS. Phạm Hữu Kỳ – Giảng viên Khoa CNTT 
2. Trần Thị Phương Anh – Thư ký Khoa CNTT 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iii 
LỜI GIỚI THIỆU 
Công nghệ thông tin đã và đang là yếu tố cốt lõi thúc đẩy nền kinh tế - xã hội phát triển 
mạnh mẽ, đặc biệt trong thời đại kỹ thuật số ngày nay. Sự bùng nổ của các công nghệ mới 
và ứng dụng tiên tiến đã thay đổi cách chúng ta sống, làm việc và tương tác. Với mục đích 
tạo ra một diễn đàn để các nhà nghiên cứu, học giả, giảng viên, cũng như các chuyên gia, 
trao đổi kết quả nghiên cứu, chia sẻ kiến thức, thảo luận quan điểm, ý tưởng về các xu 
hướng mới nhất trong lĩnh vực công nghệ thông tin và ứng dụng, Khoa Công nghệ thông 
tin, Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh (UEF) tổ chức hội thảo 
với chủ đề “Hội thảo khoa học công nghệ Khoa CNTT lần 6 năm 2024”.  
Hội thảo không chỉ nhằm mục đích nâng cao năng lực nghiên cứu mà còn thúc đẩy các 
phát minh, đổi mới và chuyển giao công nghệ trong lĩnh vực công nghệ thông tin. Đây là 
cơ hội để các chuyên gia đầu ngành, nhà nghiên cứu, giảng viên và sinh viên gặp gỡ, học 
hỏi và hợp tác, cùng nhau phát triển và ứng dụng các thành tựu khoa học kỹ thuật vào thực 
tiễn. Qua đó, hội thảo mong muốn góp phần nâng cao chất lượng giáo dục, nghiên cứu và 
thực hành trong lĩnh vực công nghệ thông tin. 
Do thời gian chuẩn bị có hạn, việc biên tập Kỷ yếu này không tránh khỏi những thiếu 
sót. Ban biên tập rất mong ý kiến đóng góp cũng như sự lượng thứ từ quý độc giả để các 
kỳ hội thảo sau được tổ chức ngày một tốt hơn, hiệu quả hơn 
Trân trọng! 
Tp. Hồ Chí Minh, tháng 6 năm 2024 
BAN BIÊN TẬP 
KHOA CÔNG NGHỆ THÔNG TIN 
TRƯỜNG ĐẠI HỌC KINH TẾ - TÀI CHÍNH THÀNH PHỐ HỒ CHÍ MINH 
141-145 ĐIỆN BIÊN PHỦ, P.15, Q.BÌNH THẠNH, TP.HCM 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
iv 
TỐI ƯU HÓA TRUY VẤN TRONG SQL SERVER: PHƯƠNG PHÁP VÀ ỨNG 
DỤNG..........................................................................................Trang 1 
Nguyễn Minh Tuấn - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KÊ CỦA CÔNG CỤ CHATGPT.......................................................Trang 14 
Nguyễn Văn Vinh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
RANSOMWARE: MỐI ĐE DỌA TRONG THỜI ĐẠI SỐ........................Trang 24 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
CHỮ KÝ.....................................................................................Trang 29 
Nguyễn Minh Thắng - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC THÚC ĐẨY 
THAY ĐỔI CHO DOANH NGHIỆP.................................................Trang 35 
Võ Thị Kim Anh - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
NHÂN CHO SINH VIÊN NGÀNH THIẾT KẾ ĐỒ HỌA.........................Trang 44 
Võ Đình Ngà - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
ỨNG DỤNG BÀI TOÁN VẬN TẢI: TỐI ƯU CHI PHÍ THU GOM RÁC SINH 
HOẠT CỦA CÁC BỆNH VIỆN..........................................................Trang 59 
Trương Nhã Bình - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
Ngô Thuận Dủ - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
DOANH CỦA DOANH NGHIỆP.......................................................Trang 70 
Hoàng Văn Hiếu - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
SÁNG TẠO NỘI DUNG AI: CÁCH MẠNG HÓA TƯƠNG LAI CỦA TIẾP THỊ NỘI 
DUNG .......................................................................................Trang 85 
Trần Thành Công - Trường Đại học Kinh Tế - Tài chính Thành Phố Hồ Chí Minh 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 35 
TÍCH HỢP BIG DATA VÀ ĐIỆN TOÁN ĐÁM MÂY: ĐỘNG LỰC 
THÚC ĐẨY THAY ĐỔI CHO DOANH NGHIỆP. 
INTEGRATION OF BIG DATA AND CLOUD COMPUTING: A 
1Trường Đại học Kinh tế - Tài chính Thành Phố Hồ Chí Minh, anhvtk@uef.edu.vn  
Tóm tắt: Kỷ nguyên số mang đến sự bùng nổ dữ liệu, tạo ra cả thách thức và cơ hội cho doanh nghiệp. 
Sự hội tụ của Big Data và điện toán đám mây nổi lên như giải pháp mạnh mẽ, cách mạng hóa cách 
thức xử lý và khai thác dữ liệu. Bài viết này khám phá tác động biến đổi của sự kết hợp này, đồng thời 
đề xuất những cân nhắc thực tế cho doanh nghiệp bắt đầu áp dụng Big Data trên nền tảng đám mây. 
Từ khóa: Kỷ nguyên số, Big Data, điện toán đám mây, biến đổi, doanh nghiệp. 
Abstract: The digital era has ushered in an unprecedented surge of data, presenting both challenges 
and opportunities for businesses. The convergence of big data and cloud computing has emerged as a 
powerful solution, revolutionizing the way data is processed and harnessed. This paper delves into the 
embarking on their big data on cloud journey. 
Key words: Digital Era, Big Data, Cloud Computing, Transformation, Business 
1. Sự kết hợp mạnh mẽ giữa Big Data và 
liệu, mang đến cả thách thức và cơ hội cho 
doanh nghiệp. Khái niệm Big Data, với đặc 
trưng khối lượng, tốc độ và sự đa dạng, lần 
đầu tiên được giới thiệu bởi Laney (2001) [1] 
và khai thác thông tin. Tuy nhiên, việc quản 
minh là rất phức tạp. 
Sự xuất hiện của điện toán đám mây [2] 
Data, cung cấp giải pháp mạnh mẽ để giải 
quyết thách thức này. Điện toán đám mây 
internet, giúp doanh nghiệp tận dụng tối đa 
linh hoạt. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 36 
Dikaiakos et al. (2009) [3] nhấn mạnh về khả 
năng mở rộng, hiệu quả chi phí và khả năng 
truy cập. 
Đối với khả năng mở rộng: cơ sở hạ tầng 
trên nhu cầu xử lý, loại bỏ nhu cầu đầu tư ban 
đầu tốn kém vào phần cứng. Doanh nghiệp chỉ 
cần trả tiền cho các tài nguyên họ sử dụng, tối 
tức đầu tư [3]. 
Về hiệu quả về chi phí: doanh nghiệp chỉ 
trả tiền cho các tài nguyên họ sử dụng, tối ưu 
đầu tư [3]. 
Còn đối với khả năng truy cập: các giải 
năng truy cập mọi lúc, mọi nơi, thúc đẩy cộng 
tác và sự linh hoạt. 
động, phát triển sản phẩm mới, gia tăng lợi thế 
doanh đầy biến động (xem thống kê Bảng 1).
Bảng 1: Lợi ích của Big Data và Điện toán đám mây  
Tự động mở rộng/thu hẹp tài nguyên, tối ưu hóa chi phí. 
Chỉ trả tiền cho tài nguyên sử dụng. 
Truy cập mọi lúc, mọi nơi, thúc đẩy cộng tác. 
Tự động hóa quy trình, cải thiện ra quyết định, tối ưu hóa 
chuỗi cung ứng. 
Xác định xu hướng thị trường và nhu cầu khách hàng. 
Đưa ra quyết định sáng suốt và nhanh chóng dựa trên dữ liệu. 
Phân tích dữ liệu để dự đoán rủi ro và nắm bắt cơ hội mới. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 37 
2. Ứng dụng thực tiễn của Big Data  
giới thực. Các doanh nghiệp đang tận dụng 
động, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Dưới đây là một số ví dụ cụ 
thể:  
Trước tiên, đó là ở ngành bán lẻ: Các gã 
nền tảng đám mây để quản lý hàng tồn kho, 
thông tin chi tiết về khách hàng [4]. Ví dụ, câu 
chuyện về Amazon retail (Amazon.com). 
Ngày 10 tháng 11 năm 2010 là ngày 
Amazon.com tắt máy chủ web vật lý cuối 
cùng 
trong 
trung 
tâm 
dữ 
liệu 
của 
Amazon.com ([17]). Amazon.com là nhà bán 
lẻ trực tuyến lớn nhất thế giới. Do 
Amazon.com tạo ra rất nhiều dữ liệu, trước 
trữ dữ liệu đó. Nhưng khi Amazon.com phát 
triển lớn hơn, kích thước cơ sở dữ liệu Oracle 
cùng khó khăn. Điều này khiến họ phải cân 
AWS. Bằng cách chuyển sang AWS, họ đã 
trải nghiệm cải thiện hiệu suất gấp 12 lần và 
giảm thời gian khôi phục từ khoảng 15 giờ 
xuống 2,5 giờ ([18]). Amazon.com đã vượt 
qua chi phí cao, hiệu suất chậm và quản lý tốn 
AWS. Họ tận dụng Amazon S3 vì tính tiết 
kiệm chi phí, khả năng mở rộng, bảo mật và 
lưu trữ bền vững, giúp sao lưu và khôi phục 
nhanh hơn đáng kể. Ngoài ra, việc di chuyển 
mạch. Nhìn chung, việc chuyển sang AWS 
giúp giảm chi phí, cải thiện hiệu quả và cung 
phát triển của Amazon (Bảng 3, 4). 
Trong ngành chăm sóc sức khỏe: Ngành 
Data. Nghiên cứu của [5] cho thấy các tổ chức 
mây để phân tích dữ liệu bệnh nhân, từ đó cải 
sáng kiến nghiên cứu. Ví dụ, Mayo Clinic sử 
điều trị mới, chẩn đoán bệnh chính xác hơn và 
cải thiện hiệu quả chăm sóc.  
Và trong ngành dịch vụ tài chính: Phân 
chính. Các nghiên cứu điển hình của [6] cho 
để xác định các giao dịch gian lận, đánh giá 
rủi ro tín dụng và quản lý danh mục đầu tư. Ví 
dụ, JPMorgan Chase sử dụng Big Data để 
phát hiện các trường hợp rửa tiền, ngăn chặn 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 38 
đầu tư.  
vực. Các giải pháp Big Data dựa trên nền tảng 
doanh đầy biến động. Bảng 2 sau đây thống 
kê các ứng dụng:
Bảng 2: Ứng dụng thực tiễn của Big Data 
Quản lý hàng tồn kho, cá nhân hóa 
chiến dịch tiếp thị, thu thập thông tin 
khách hàng, đề xuất sản 
[4, 18] 
Phân tích dữ liệu bệnh nhân, cải thiện 
chất lượng chăm sóc, thúc đẩy nghiên 
cứu 
phương pháp điều trị mới, 
[5] 
Quản lý rủi ro, phát hiện gian lận, 
đánh giá rủi ro tín dụng, quản lý danh 
rửa tiền, ngăn chặn gian lận 
thẻ tín dụng, tối ưu hóa danh 
[6] 
Bảng 3: Bảng so sánh Lưu trữ truyền thống vs Lưu trữ đám mây Amazon S3 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Chi phí trả trước cao cho phần cứng băng, dung 
lượng trung tâm dữ liệu và giấy phép phần mềm. 
Mô hình trả tiền theo nhu cầu, loại 
bỏ chi phí trả trước. 
liệu ngày càng tăng. 
của Amazon. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 39 
Lưu trữ truyền thống với tape (qua băng đĩa) 
Sao lưu và phục hồi chậm do thời gian đọc băng. 
kể so với băng. 
liệu, dễ bị lỗi phần cứng. 
11 số chín (99.999999999%). 
tầng băng. 
cầu quản lý tối thiểu. 
Bảng 4: Bảng so sánh Máy chủ cục bộ vs AWS EC2 đám mây 
Máy chủ On-premises 
trung tâm dữ liệu cục bộ. 
chuyên dụng để giao tiếp liền mạch. 
máy chủ web, cơ sở dữ liệu và 
các công cụ. 
AWS. 
3. Giải quyết thách thức và triển khai hiệu 
toán đám mây mang lại nhiều lợi ích, nó cũng 
thận. Bảo mật dữ liệu là một trong những 
mối quan tâm hàng đầu. Pearson (2013) [7] 
vệ dữ liệu nhạy cảm trên đám mây. Các biện 
pháp này bao gồm: mã hóa dữ liệu, kiểm soát 
quyền truy cập, và tuân thủ các quy định. 
truy cập trái phép. Kiểm soát quyền truy cập 
vào dữ liệu và mức độ truy cập của dữ liệu đó. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 40 
Đối với việc tuân thủ các quy định, như là, 
bảo mật dữ liệu, chẳng hạn như quy định về 
bảo vệ dữ liệu (GDPR) của Liên minh châu 
Âu. 
pháp Big Data trên nền tảng đám mây. 
Achanta (2023) [8] và Setiyawan & Patel 
(2019) [9] đã nêu bật tầm quan trọng của 
việc: chất lượng dữ liệu, và tuân thủ quy định. 
việc xem xét dữ liệu tính chính xác, đầy đủ 
và nhất quán để có thể phân tích hiệu quả. 
quản lý dữ liệu, chẳng hạn như Sarbanes-
Oxley Act (SOX) của Hoa Kỳ.  
Ngoài ra, còn có một số thách thức khác 
Big Data trên nền tảng đám mây, bao gồm: sự 
tương tác, kỹ năng nhân sự, và chi phí triển 
khai – vận hành. Về khả năng tương tác, thì 
doanh nghiệp. Đối với các kỹ năng, thì doanh 
đám mây. Còn lại, đối với quản lý chi phí, thì 
toán đám mây. 
thức và cân nhắc này, các doanh nghiệp có thể 
tranh (Bảng 3). 
Bảng 5: Giải quyết thách thức và triển khai hiệu quả Big Data dựa trên điện toán đám mây 
Mã hóa mạnh mẽ, kiểm soát quyền truy 
cập, tuân thủ quy định 
[7] 
Đảm bảo chất lượng dữ liệu, tuân thủ 
[8, 9] 
[10] 
[11, 12] 
[13, 14] 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 41 
4. Tương lai của việc ra quyết định dựa 
mây. Các xu hướng mới nổi như điện toán 
lý và sử dụng dữ liệu. 
Theo Banjanovic & Husaković (2023) 
[15], điện toán biên tích hợp phân tích Big 
liệu thời gian thực tại ranh giới của mạng. 
nguồn khác nhanh chóng và hiệu quả hơn. 
McGrath & Brenner (2017) [16] cho rằng 
liệu. Nhờ vậy, doanh nghiệp có thể thúc đẩy 
đổi mới và tăng trưởng nhanh hơn. 
Sự kết hợp của Big Data, điện toán đám 
tâm trong việc ra quyết định, đổi mới và tăng 
trưởng (Bảng 4). Doanh nghiệp cần nắm bắt 
đại dữ liệu. 
Bảng 6: Tương lai của việc ra quyết định dựa trên dữ liệu 
biên 
mạng 
liệu nhanh chóng, hiệu quả 
[15] 
chủ 
[16] 
năng lưu trữ, xử lý và phân tích dữ liệu mạnh 
mẽ. Nhờ đó, doanh nghiệp có thể nâng cao 
hiệu quả hoạt động, hiểu rõ hơn về khách 
hàng, phát triển sản phẩm mới và gia tăng lợi 
thế cạnh tranh. Việc nắm bắt sức mạnh của Big 
số. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 42 
mây hiệu quả, doanh nghiệp cần xác định rõ 
mục tiêu, lựa chọn giải pháp phù hợp, xây 
dựng chiến lược quản trị dữ liệu, đầu tư đào 
án nhỏ đến mở rộng dần. Việc áp dụng thành 
doanh nghiệp thành công trong kỷ nguyên số. 
hiệu quả, doanh nghiệp có thể khai thác sức 
đạt được những lợi ích to lớn. 
[1] Laney, D. (2001) 3D Data Management: 
Controlling Data Volume, Velocity and Variety. 
META Group Research Note, 6. 
[2] Armbrust, M., Griffith, R., Joseph, A. D., Katz, 
R., Konwinski, A., Lee, G., Patterson, D., et al. 
(2010). 
A 
view 
of 
computing. 
Communications of the ACM, 53(4), 50-58. 
ACM. 
[3] Dikaiakos, M., Katsaros, D., Mehra, P., Pallis, G., 
& Vakali, A. (2009). “Cloud computing: 
scientific research”. IEEE Internet Computing, 
13(5), 10-13. 
[4] Chen, W., Li, J., & Jin, X. J. (2016). The 
replenishment policy of agri-products with 
stochastic demand in integrated agricultural 
supply chains. Expert Systems with Applications, 
48, 55-66. 
[5] Halamka, J. (2014). The Argonaut Project 
Charter. Life as a Healthcare CIO. 
[6] Rizvi, S. (2021). Role of big data in financial 
institutions for financial fraud. SSRN Electronic 
Journal, 4, 35. 
[7] Pearson, S. (2013). Privacy, Security and Trust in 
Cloud Computing. In: Pearson, S., Yee, G. (eds) 
Privacy and Security for Cloud Computing. 
and 
Networks. 
Springer, London. https://doi.org/10.1007/978-1-
4471-4189-1_1  
[8]  Achanta, M. (2023). Data governance in the age of 
cloud computing: Strategies and considerations. 
(IJSR), 12, 1338-1343. 
[9]  Setiyawan, D., & Patel, C. (2019). A proposed 
and data management in higher education. SSRN 
Electronic Journal, 6, 19-25. 
[10] Agrawal, D. & Das, S. & Abbadi, A. (2011). Big 
Data and Cloud Computing: Current State and 
Opportunities. 
ACM 
Series. 
530-533. 
10.1145/1951365.1951432. 
[11] Ghaleb, E.A.A.; Dominic, P.D.D.; Fati, S.M.; 
Muneer, A.; Ali, R.F. 2021. The Assessment of Big 
Data Adoption Readiness with a Technology–
Organization–Environment 
Framework: 
A 
Employees. 
2021, 
13, 
8379. 
https://doi.org/10.3390/su13158379 
[12] Shamim, S., Zeng, J., Choksy, U.S. & Shariq, S. 
M. 2020. Connecting big data management 
employee level, International Business Review, 
Volume 29, Issue 6, 101604, ISSN 0969-5931, 
https://doi.org/10.1016/j.ibusrev.2019.101604. 
KỶ YẾU HỘI THẢO KHOA HỌC CÔNG NGHỆ LẦN THỨ 6 
Trang 43 
[13] Muniswamaiah, M., Agerwala, T. & Tappert, C. 
(2019). Big data in cloud computing review and 
opportunities.  International Journal of Computer 
Science & Information Technology (IJCSIT) Vol 
11, 
No 
4. 
https://arxiv.org/ftp/arxiv/papers/1912/1912.108
21.pdf 
[14] El-Seoud, S. A., El-Sofany, H. F., Abdelfattah, 
M. A. F., & Mohamed, R. (2017). Big Data and 
Cloud Computing: Trends and Challenges. 
(iJIM), 
11(2), 
pp. 
34–52. 
https://doi.org/10.3991/ijim.v11i2.6561 
[15] Banjanovic, M. L., & Husaković, A. (2023). Edge 
AI: Reshaping the Future of Edge Computing 
with 
Intelligence. 
10.5644/PI2023.209.07. 
[16] McGrath, G., & Brenner, P. R. (2017). 
"Serverless 
Computing: 
Design, 
Implementation, and Performance," 2017 IEEE 
Computing Systems Workshops (ICDCSW), 
Atlanta, GA, USA, 2017, pp. 405-410, doi: 
10.1109/ICDCSW.2017.36. 
[17] [Amazon Web Services]. (2012, December 10). 
AWS re: Invent ENT 205- Drinking Our Own 
[Video]. 
Https://www.Youtube.com/User/AmazonWebSe
rvices/Cloud 
. 
https://www.youtube.com/watch?v=f45Uo5rw6
YY  
[18] Chavan, A. (2020, September 21). How Amazon 
retail (Amazon.Com) uses the AWS cloud. 
Medium. 
June 
7, 
2024, 
from 
https://ankush-chavan.medium.com/how-
amazon-retail-amazon-com-uses-the-aws-cloud-
View publication stats

Removed lines from sybca-bigdata-ppt.pdf:
Introduction to Big Data
What is Data?
The quantities, characters, or symbols on which operations are performed by a computer, 
magnetic, optical, or mechanical recording media.
What is Big Data?
Big Data is also data but with a huge size. Big Data is a term used to describe a 
collection of data that is huge in volume and yet growing exponentially with time. In 
tools are able to store it or process it efficiently.
“Extremely large data sets that may be analyzed computationally to reveal patterns , 
trends and association, especially relating to human behavior and interaction are 
known as Big Data.”

Following are some the examples of Big Data-
The New York Stock Exchange generates about one terabyte of new trade data per day.
Social Media
The statistic shows that 500+terabytes of new data get ingested into the databases of social 
media site Facebook, every day. This data is mainly generated in terms of photo and video 
uploads, message exchanges, putting comments etc.
A single Jet engine can generate 10+terabytes of data in 30 minutes of flight time. With many 
thousand flights per day, generation of data reaches up to many Petabytes.
Name
Size(In Bytes)
Bit
1/8
1/2 (rare)
Byte
1
1024 bytes
1024
1, 024kilobytes
1, 048, 576
1, 024 megabytes
1, 073, 741, 824
1, 024 gigabytes
1, 099, 511, 627, 776
1, 024 terrabytes
1, 125, 899, 906, 842, 624
1, 024 petabytes
1, 152, 921, 504, 606, 846, 976
1, 024 exabytes
1, 180, 591, 620, 717, 411, 303, 424
1, 024 zettabytes
1, 208, 925, 819, 614, 629, 174, 706, 176
Characteristics Of Big Data
•
The following are known as “Big Data Characteristics”.
1. Volume
2. Velocity
3. Variety
4. Veracity
1. Volume:
Volume means “How much Data is generated”. Now-a-days, 
very vast amount of Data say TB(Tera Bytes) to PB(Peta Bytes) to Exa
Byte(EB) and more.
2. Velocity:
Velocity means “How fast produce Data”. Now-a-days, Organizations or 
fast rate.
3. Variety:
Variety means “Different forms of Data”. Now-a-days, Organizations or 
rate in different formats. We will discuss in details about different formats of 
Data soon.
4. Veracity
Veracity means “The Quality or Correctness or Accuracy of Captured Data”. 
Out of 4Vs, it is most important V for any Big Data Solutions. Because without 
Correct Information or Data, there is no use of storing large amount of data at 
fast rate and different formats. That data should give correct business value.
Types of Digital Data
1. Structured
2. Unstructured
3. Semi-structured
Structured

Any data that can be stored, accessed and processed in the form of fixed format is 
termed as a 'structured' data. 

Over the period of time, talent in computer science has achieved greater success in 
developing techniques for working with such kind of data (where the format is well 
known in advance) and also deriving value out of it.

However, nowadays, we are foreseeing issues when a size of such data grows to a huge 
extent, typical sizes are being in the range of multiple zettabytes.
Do you know? 1021 bytes equal to 1 zettabyte or one billion terabytes forms a zettabyte.
given and imagine the challenges involved in its storage and processing.
Do you know? Data stored in a relational database management system is one 
example of a 'structured' data.
• Examples Of Structured Data
An 'Employee' table in a database is an example of Structured Data
2365
Male
650000
3398
650000
7465
Male
500000
7500
Male
500000
7699
550000
Unstructured

Any data with unknown form or the structure is classified as unstructured data.

In addition to the size being huge, un-structured data poses multiple challenges in terms 
of its processing for deriving value out of it.

combination of simple text files, images, videos etc. 

Now day organizations have wealth of data available with them but unfortunately, they 
don't know how to derive value out of it since this data is in its raw form or unstructured 
format.
• Examples Of Un-structured Data
The output returned by 'Google Search'
Semi-structured

Semi-structured data can contain both the forms of data. 

We can see semi-structured data as a structured in form but it is actually not defined 
with e.g. a table definition in relational DBMS.

Example of semi-structured data is a data represented in an XML file.
Examples Of Semi-structured Data
Personal data stored in an XML file-
<rec><name>Prashant Rao</name><sex>Male</sex><age>35</age></rec>
<rec><name>Seema R.</name><sex>Female</sex><age>41</age></rec>
<rec><name>Satish Mane</name><sex>Male</sex><age>29</age></rec>
<rec><name>Subrato Roy</name><sex>Male</sex><age>26</age></rec>
<rec><name>Jeremiah J.</name><sex>Male</sex><age>35</age></rec>
Big Data Analytics
Big Data Analytics: 

Big Data analytics is the process of collecting, organizing and analyzing 
large sets of data (called Big Data) to discover patterns and other useful 
information.

that is most important to the business and future business decisions. 
from analyzing the data.
High-Performance Analytics Required: 

To analyze such a large volume of data, Big Data analytics is typically 
performed using specialized software tools and applications for predictive 
analytics, data mining, text mining, forecasting and data optimization. 

high-performance analytics.

large volumes of data that a business has collected to determine which data is 
relevant and can be analyzed to drive better business decisions in the future.
The Challenges:

For most organizations, Big Data analysis is a challenge. Consider the sheer 
volume of data and the different formats of the  
data(both structured and unstructured data) that is collected across the entire 
combined, contrasted and analyzed to find patterns and other useful business 
information.

organization stores in different places and often in different systems. 

easily as structured data. 

This massive volume of data is typically so large that it's difficult to process 
using traditional database and software methods.
How Big Data Analytics is Used Today:

data improves, business can be transformed in all sorts of ways. 

Today's advances in analyzing big data allow researchers to decode human DNA in 
minutes, predict where terrorists plan to attack, determine which gene is mostly likely 
to be responsible for certain diseases and, of course, which ads you are most likely to 
respond to on Facebook.

Another example comes from one of the biggest mobile carriers in the world.

France's Orange launched its Data for Development project by releasing subscriber 
data for customers in the Ivory Coast.

The 2.5 billion records, which were made anonymous, included details on calls and 
text messages exchanged between 5 million users.

as the foundation for development projects to improve public health and safety.

cell phone data to map where people went after emergencies; another showed how to 
use cellular data for disease containment. (source)
The Benefits of Big Data Analytics:

data. Many big data projects originate from the need to answer specific 
business questions. With the right big data analytics platforms in place, an 
enterprise can boost sales, increase efficiency, and improve operations, 
customer service and risk management.

Webopedia parent company, QuinStreet, surveyed 540 enterprise decision-
companies plan to use Big Data analytics to improve operations. About half 
of all respondents said they were applying big data analytics to improve 
customer retention, help with product development and gain a competitive 
advantage.

Notably, the business area getting the most attention relates to increasing 
efficiency and optimizing operations. Specifically, 62 percent of respondents 
said that they use big data analytics to improve speed and reduce complexity.
Application of Big Data 
Here is the list of top Big Data applications in today’s world:
•
•
•
Big Data in E-commerce
•
•
•
•
•
Let’s discuss the applications of Big Data in detail.
1. Big Data in Retail

The retail industry is the one that faces the most fierce competition of all. Retailers 
constantly hunt for ways that will give them a competitive edge over others. 
Customers are the real king sounds legit for the retail industry in particular.

For retailers to thrive in this competitive world, they need to understand their 
customers in a better way. If they are aware of their customers’ needs and how to 
fulfill those needs in the best possible way, then they know everything.

– Big Data in Retail.

Through advanced analysis of their customer’s data, retailers are now able to 
understand them from every angle possible. They gather this data from various 
sources such as social media, loyalty programs, etc.

Even a minute detail about any customer has now become significant for them. They are 
now closer to their customers than they have ever been. This empowers them to provide 
customers with more personalized services and predict their demands in advance.

This helps them in building a loyal customer base. Some of the biggest names in the retail 
world like Walmart, Sears and Holdings, Costco, Walgreens, and many more now have Big 
Data as an integral part of their organizations.

are responsible for as much as 30% of retail annual sales.
2. Big Data in Healthcare

Big Data and healthcare are an ideal match. It complements the healthcare industry better 
than anything ever will. The amount of data the healthcare industry has to deal with is 
unimaginable.
Gone are the days when healthcare practitioners were incapable of harnessing this data. 
From finding a cure to cancer to detecting Ebola and much more, Big Data has got it all 
under its belt and researchers have seen some life-saving outcomes through it.

medications. Data analysts are harnessing this data to develop more and more effective 
treatments. Identifying unusual patterns of certain medicines to discover ways for 
developing more economical solutions is a common practice these days.

Explore how Big Data helps to speed up the treatment process – Big Data in 
Healthcare.

people of all age groups. This generates massive amounts of real-time data in the 
form of alerts which helps in saving the lives of the people.
3. Big Data in Education

When you ask people about the use of the data that an educational institute gathers, the 
need it for future references.

Even you had the same perception about this data, didn’t you? But the fact is, this data 
holds enormous importance. Big Data is the key to shaping the future of the people and 
has the power to transform the education system for better.

curriculum. Additionally, universities can even track the dropout rates of the students 
and are taking the required measures to reduce this rate as much as possible.
4. Big Data in E-commerce

One of the greatest revolutions this generation has seen is that of E-commerce. It is now part 
and parcel of our routine life. Whenever we need to buy something, the first thought that 
provokes our mind is E-commerce. And not your surprise, Big Data has been the face of it.

Some of the biggest E-commerce companies of the world like Amazon, Flipkart, Alibaba, and 
popularity Big Data has gained in recent times.

Big Data is now as important as anyone else in these organizations. Amazon, the biggest E-
commerce firm in the world and one of the pioneers of Big Data and analytics, has Big Data as 
the backbone of its system. Flipkart, the biggest E-commerce firm in India, has one of the most 
robust data platforms in the country.

See how Flipkart used Big Data to have one of the most robust data platforms.

Big Data’s recommendation engine is one of the most amazing applications the Big Data world 
has ever witnessed. It furnishes the companies with a 360-degree view of its customers.

Companies then suggest customers accordingly. Customers now experience more personalized 
services than they have ever had. Big Data has completely redefined people’s online shopping 
experiences.
5. Big Data in Media and Entertainment

sheer piece of art. Art and science are often considered to be the two completely 
contrasting domains but when employed together, they do make a deadly duo and Big 
Data’s endeavors in the media industry are a perfect example of it.

Viewers these days need content according to their choices only. Content that is 
relatively new to what they saw the previous time. Earlier the companies 
broadcasted the Ads randomly without any kind of analysis.

But after the advent of Big Data analytics in the industry, companies now are 
aware of the kind of Ads that attracts a customer and the most appropriate time to 
broadcast it for seeking maximum attention.

Customers are now the real heroes of the Media and entertainment industry -
courtesy to Big Data and Analytics.
6. Big Data in Finance

data is one of the toughest challenges any financial firm faces. Data has been the second most 
important commodity for them after money.

Even before Big Data gained popularity, the finance industry was already conquering the 
technical field. In addition to it, financial firms were among the earliest adopters of Big Data 
and Analytics.

has been at the heart of it. Big Data is bossing the key areas of financial firms such as fraud 
detection, risk analysis, algorithmic trading, and customer contentment.

This has brought much-needed fluency in their systems. They are now empowered to focus 
more on providing better services to their customers rather than focussing on security issues. 
Big Data has now enhanced the financial system with answers to its hardest of the challenges.
7. Big Data in Travel Industry

with it, the travel industry was a bit late to realize its worth. Better late than never though. 
Having a stress-free traveling experience is still like a daydream for many.

And now Big Data’s arrival is like a ray of hope, that will mark the departure of all the 
hindrances in our smooth traveling experience.
See how Big Data is revolutionizing the travel & tourism sector.

Through Big Data and analytics, travel companies are now able to offer more 
customized traveling experience. They are now able to understand their customer’s 
requirements in a much-enhanced way.

From providing them with the best offers to be able to make suggestions in real-time, 
Big Data is certainly a perfect guide for any traveler. Big Data is gradually taking the 
window seat in the travel industry.
8. Big Data in Telecom

The telecom industry is the soul of every digital revolution that takes place around the world. 
With the ever-increasing popularity of smartphones, it has flooded the telecom industry with 
massive amounts of data.

And this data is like a goldmine, telecom companies just need to know how to dig it properly. 
Through Big Data and analytics, companies are able to provide the customers with smooth 
connectivity, thus eradicating all the network barriers that the customers have to deal with.
Companies now with the help of Big Data and analytics can track the areas with the lowest as 
well as the highest network traffics and thus doing the needful to ensure hassle-free network 
connectivity.
Big Data alike other industries have helped the telecom industry to understand its customers 
pretty well. 
Telecom industries now provide customers with offers as customized as possible.

Big Data has been behind the data revolution we are currently experiencing.
9. Big Data in Automobile

“A business like an automobile, has to be driven, in order to get results.” B.C. Forbes

smoothly. Big Data is driving the automobile industry towards some unbelievable and never 
before results.

wings to it. Big Data has helped the automobile industry achieve things that were beyond our 

From analyzing the trends to understanding the supply chain management, from taking care 
of its customers to turning our wildest dream of connected cars a reality, Big Data is well 
and truly driving the automobile industry crazy.

Removed lines from TNG_QUAN_V_D_LIU_LN_BIGDATA.pdf:
1 
TỔNG QUAN Vӄ DӲ LIӊU LӞN (BIGDATA) 
Ks. Nguyễn Công Hoan 
Trung Tâm Thông tin Khoa học thống kê (Viện KHTK) 
Trước đây, chúng ta mới chỉ biết đến dữ liệu có cấu trúc (structure data), ngày 
nay, với sự kết hợp của dữ liệu và internet, đã xuất hiện một dạng khác của dữ liệu -  Big 
data (dịch là “dữ liệu lớn”). Dữ liệu này có thể từ các nguồn như: hồ sơ hành chính,giao 
dịch điện tử, dòng trạng thái (status), chia sẻ hình ảnh, bình luận, nhắn tin...của chính 
chúng ta, nói cách khác chúng là dữ liệu được sản sinh qua quá trình chia sẻ thông tin 
trực tuyến liên tục của người sử dụng. Để cung cấp cái nhìn tổng quan, chúng tôi xin giới 
liệu lớn mang lại.    
1.  Khái niӋm, đặc trưng của dӳ liӋu lӟn và sự khác biӋt vӟi dӳ liӋu truyӅn thống 
1.1. Khái niệm về dữ liệu lớn 
- Theo wikipedia: Big data là một thuật ngữ chỉ bộ dữ liệu lớn hoặc phức tạp mà các 
phương pháp truyền thống không đӫ các ứng dөng để xử lỦ dữ liệu này. 
-  Theo Gartner: Dữ liệu lớn là những nguồn thông tin có đặc điểm chung khối lượng lớn,  
tốc độ nhanh và dữ liệu định dạng dưới nhiều hình thức khác nhau, do đó muốn khai thác 
được  đòi hỏi phải có hình thức xử lỦ mới để đưa ra quyết định, khám phá và tối ưu hóa 
quy trình.  
1.2. Nguồn hình thành dữ liệu và phương pháp khai thác và quản lý dữ liệu lớn. 
Qua thống kê và tổng hợp, nguồn dữ liệu lớn được hình thành chӫ yếu từ 6 nguồn: 
(1) Dữ liệu hành chính (phát sinh từ chương trình cӫa một tổ chức, có thể là chính phӫ 
hay phi chính phӫ). Ví dө, hồ sơ y tế điện tử ở bệnh viện, hồ sơ bảo hiểm, hồ sơ ngân 
hàng...; (2) Dữ liệu từ hoạt động thương mại (phát sinh từ các giao dịch giữa hai thực 
thể). Ví dө, các giao dịch thẻ tín dөng, giao dịch trên mạng, bao gồm cả từ các thiết bị di 
động; (3) Dữ liệu từ các thiết bị cảm biến như thiết bị chөp hình ảnh vệ tinh, cảm biến 
đường, cảm biến khí hậu; (4) Dữ liệu từ các thiết bị theo dõi, ví dө theo dõi dữ liệu từ 
điện thoại di động, GPS; (5) Dữ liệu từ các hành vi, ví dө như tìm kiếm trực tuyến về 
(một sản phẩm, một dịch vө hay thông tin khác), đọc các trang mạng trực tuyến...; (6) Dữ 
liệu từ các thông tin về  Ủ kiến, quan điểm cӫa các cá nhân, tổ chức, trên các phương tiện 
thông tin xã hội. 
theo các nguồn hình thành dữ liệu lớn. Mỗi nguồn dữ liệu lớn khác nhau sẽ có phương 
pháp khai thác và quản lỦ dữ liệu lớn khác nhau. Tuy nhiên, hiện nay phần lớn các tổ 
dữ liệu lớn. 
1.3. Đặc trưng 5V cͯa dữ liệu lớn 
Dữ liệu lớn có 5 đặc trưng cơ bản như sau (mô hình 5V):   
(1) Khối lượng dữ liệu (Volume)  
2 
Đây là đặc điểm tiêu biểu nhất cӫa dữ liệu lớn, khối lượng dữ liệu rất lớn. Kích cỡ 
cӫa Big Data đang từng ngày tăng lên, và tính đến năm 2012 thì nó có thể nằm trong 
khoảng vài chөc terabyte cho đến nhiều petabyte (1 petabyte = 1024 terabyte) chỉ cho 
một tập hợp dữ liệu. Dữ liệu truyền thống có thể lưu trữ trên các thiết bị đĩa mềm, đĩa 
cứng. Nhưng với dữ liệu lớn chúng ta sẽ sử dөng công nghệ “đám mây” mới đáp ứng khả 
năng lưu trữ được dữ liệu lớn. 
 (2) Tốc độ (Velocity) 
Tốc độ có thể hiểu theo 2 khía cạnh: (a) Khối lượng dữ liệu gia tăng rất nhanh (mỗi 
giây có tới 72.9 triệu các yêu cầu truy cập tìm kiếm trên web bán hàng cӫa Amazon); (b) 
Xử lỦ dữ liệu nhanh ở mức thời gian thực (real-time), có nghĩa dữ liệu được xử lỦ ngay 
tức thời ngay sau khi chúng phát sinh (tính đến bằng mili giây). Các ứng dөng phổ biến 
trên lĩnh vực Internet, Tài chính, Ngân hàng, Hàng không, Quân sự, Y tế – Sức khỏe như 
hiện nay phần lớn dữ liệu lớn được xử lỦ real-time. Công nghệ xử lỦ dữ liệu lớn ngày nay 
đã cho phép chúng ta xử lỦ tức thì trước khi chúng được lưu trữ vào cơ sở dữ liệu. 
(3) Đa dạng (Variety) 
Đối với dữ liệu truyền thống chúng ta hay nói đến dữ liệu có cấu trúc, thì ngày nay 
hơn 80% dữ liệu được sinh ra là phi cấu trúc (tài liệu, blog, hình ảnh, vi deo, bài hát, dữ 
liệu từ thiết bị cảm biến vật lỦ, thiết bị chăm sóc sức khỏe...). Big Data cho phép liên kết 
và phân tích nhiều dạng dữ liệu khác nhau. Ví dө, với các bình luận cӫa một nhóm người 
dùng nào đó trên Facebook với thông tin video được chia sẻ từ Youtube và Twitter. 
(4) Độ tin cậy/chính xác (Veracity) 
Một trong những tính chất phức tạp nhất cӫa Dữ liệu lớn là độ tin cậy/chính xác cӫa 
dữ liệu. Với xu hướng phương tiện truyền thông xã hội (Social Media) và mạng xã hội 
(Social Network) ngày nay và sự gia tăng mạnh mẽ tính tương tác và chia sẻ cӫa người 
dùng Mobile làm cho bức tranh xác định về độ tin cậy & chính xác cӫa dữ liệu ngày một 
khó khăn hơn. Bài toán phân tích và loại bỏ dữ liệu thiếu chính xác và nhiễu đang là tính 
chất quan trọng cӫa BigData. 
 (5) Giá trị (Value) 
Giá trị là đặc điểm quan trọng nhất cӫa dữ liệu lớn, vì khi bắt đầu triển khai xây 
thông tin mang lại như thế nào, khi đó chúng ta mới có quyết định có nên triển khai dữ 
liệu lớn hay không. Nếu chúng ta có dữ liệu lớn mà chỉ nhận được 1% lợi ích từ nó, thì 
không nên đầu tư phát triển dữ liệu lớn. Kết quả dự báo chính xác thể hiện rõ nét nhất về 
giá trị cӫa dữ liệu lớn mang lại. Ví dө, từ khối dữ liệu phát sinh trong quá trình khám, 
chữa bệnh sẽ giúp dự báo về sức khỏe được chính xác hơn, sẽ giảm được chi phí điều trị 
và các chi phí liên quan đến y tế.  
1.4. Sự khác biệt giữa dữ liệu lớn với dữ liệu truyền thống  
3 
Dữ liệu lớn khác với dữ liệu truyền thống (ví dө, kho dữ liệu - Data Warehouse) ở 4 
điểm cơ bản: Dữ liệu đa dạng hơn; lưu trữ dữ liệu lớn hơn; truy vấn nhanh hơn; độ chính 
xác cao hơn. 
(1) Dữ liệu đa dạng hơn: Khi khai thác dữ liệu truyền thống (Dữ liệu có cấu trúc), 
chúng ta thường phải trả lời các câu hỏi: Dữ liệu lấy ra kiểu gì? định dạng dữ liệu như thế 
nào? Đối với dữ liệu lớn, không phải trả lời các câu hỏi trên. Hay nói khác, khi khai thác, 
chúng; điều quan tâm là giá trị mà dữ liệu mang lại có đáp ứng được cho công việc hiện 
tại và tương lai hay không. 
(2) Lưu trữ dữ liệu lớn hơn: Lưu trữ dữ liệu truyền thống vô cùng phức tạp và luôn 
đặt ra câu hỏi lưu như thế nào? dung lượng kho lưu trữ bao nhiêu là đӫ? gắn kèm với câu 
hỏi đó là chi phí đầu tư tương ứng. Công nghệ lưu trữ  dữ liệu lớn hiện nay đã phần nào 
có thể giải quyết được vấn đề trên nhờ những công nghệ lưu trữ đám mây, phân phối lưu 
xác và xử lỦ nhanh trong thời gian thực. 
(3) Truy vấn dữ liệu nhanh hơn: Dữ liệu lớn được cập nhật liên tөc, trong khi đó 
tin đáp ứng theo yêu cầu. 
(4) Độ chính xác cao hơn: Dữ liệu lớn khi đưa vào sử dөng thường được kiểm định 
lại dữ liệu với những điều kiện chặt chẽ, số lượng thông tin được kiểm tra thông thường 
rất lớn, và đảm bảo về nguồn lấy dữ liệu không có sự tác động cӫa con người vào thay 
đổi số liệu thu thập. 
2. Bͱc tranh tổng thể ͱng dụng dữ liệu lớn  
 Dữ liệu lớn đã được ứng dөng trong nhiều lĩnh vực như: hoạt động chính trị; giao 
thông; y tế; thể thao; tài chính; thương mại; thống kê... dưới đây là một số ví dө về ứng 
dөng dữ liệu lớn. 
2.1. Ͱng dụng dữ liệu lớn trong hoạt động chính trị 
cӫa mình. Ông xây dựng một đội ngũ nhân viên chuyên đi 
triển khai về dữ liệu lớn. Đội ngũ nhân viên này thu thập tất 
cả thông tin về người dân ở các khu vực, sau đó phân tích và 
chỉ ra một số thông tin quan trọng về người dân Mỹ như: 
Thích đọc sách gì, thích mua loại thuốc gì, thích sử dөng phương tiện gì... Thậm chí còn 
biết được cả thông tin về mẹ cӫa cử tri đó đã bỏ phiếu tín nhiệm ai ở lần bầu cử trước. 
Trên cơ sở những thông tin này, Tổng thống Obama đã đưa ra kế hoạch vận động phù 
hợp, giúp ông tái đắc cử Tổng thống nước Mỹ lần thứ 2. 
4 
dөng như: Hệ thống chính phӫ điện tử; phân tích quy định và việc tuân thӫ quy định; 
phân tích, giám sát, theo dõi và phát hiện gian lận, mối đe dọa, an ninh mạng. 
2.2. Ͱng dụng dữ liệu lớn trong giao thông 
dòng giao thông trong thành phố vào các giờ cao điểm, từ đó có 
những kế hoạch phân luồng giao thông chi tiết, hợp lỦ giúp giảm 
thiểu kẹt xe. Ngoài ra còn đưa ra thông tin cho người tham gia 
đi vào giờ nào để tránh kẹt xe, hoặc đi đường nào là ngắn nhất.v.v. Ngoài ra dữ liệu lớn 
còn giúp phân tích định vị người dùng thiết bị di động, ghi nhận chi tiết cuộc gọi trong 
thời gian thực; và giảm thiểu tình trạng ùn tắc giao thông. 
2.3. Ͱng dụng dữ liệu lớn trong y tế 
để đưa ra dự đoán về nguy cơ mắc bệnh. Đồng thời cũng đưa ra 
được xu hướng lây lan cӫa bệnh. Ví dө, ứng dөng Google Flu 
dөng này dựa trên từ khóa tìm kiếm ở một khu vực nào đó, sau đó 
kiếm đó, sau cùng là đưa ra dự báo về xu hướng dịch cúm tại khu 
vực đó. Qua đó cho biết tình hình cúm tại khu vực đó sẽ diễn ra như thế nào để đưa ra các 
giải pháp phòng tránh. Những kết quả mà Google Flu Trend đưa ra, hoàn toàn phù hợp 
với báo cáo cӫa Tổ chức y tế thế giới WHO về tình hình bệnh cúm tại các khu vực đó. 
2.4. Ͱng dụng dữ liệu lớn trong thể thao 
cӫa đội tuyển Đức (hình bên) đã đưa ra những điểm bất hợp lỦ 
trong cấu trúc cӫa đội tuyển Đức, từ đó giúp cho đội tuyển Đức 
khắc phөc được điểm yếu và đã dành được World cup 2014. 
2.5. Ͱng dụng dữ liệu lớn trong tài chính 
Từ những dữ liệu chính xác, kịp thời thu thập được thông qua các giao dịch cӫa 
khách hàng, tiến hành phân tích, xếp hạng và quản lỦ các rӫi ro trong đầu tư tài chính, tín 
dөng. 
2.6. Ͱng dụng dữ liệu lớn trong thương mại 
sau: Phân khúc thị trường và khách hàng; phân tích hành vi khách hàng tại cửa hàng; tiếp 
thị trên nền tảng định vị; phân tích tiếp thị chéo kênh, tiếp thị đa kênh; quản lỦ các chiến 
dịch tiếp thị và khách hàng thân thiết; So sánh giá; Phân tích và quản lỦ chuỗi cung ứng; 
Phân tích hành vi, thói quen người tiêu dùng. 
2.7. Ͱng dụng dữ liệu lớn trong thống kê 
5 
thức, Ӫy ban Thống kê Liên hợp quốc cũng như các tổ chức thống kê khu vực và cơ quan 
thống kê quốc gia cӫa nhiều nước đã triển khai hàng loạt các hoạt động về Bigdata như: 
Hàn Quốc sử dөng ảnh vệ tinh để thống kê nông nghiệp và một số lĩnhvực khác;Australia 
sử dөng ảnh vệ tinh để thống kê diện tích đất nông nghiệp và năng suất; Italia sử dөng dữ 
liệu điện thoại di động để thống kê di cư; Bhutan dùng thiết bị di động để tính toán chỉ số 
giá tiêu dùng; Estonia dùng điện thoại di động định vị vệ tinh để thống kê du lịch; 
3. Nhӳng cơ hội và thách thͱc khi ͱng dụng Big data trong thống kê chính thͱc 
3.1 Cơ hội  
(1) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp cho chúng ta có thêm phương án 
giải quyết, xử lỦ và đối phó với những thách thức đối sản xuất số liệu thống kê chính thức 
trong hiện tại và tương lai. Những nghiên cứu thực nghiệm cần phải được tiến hành để 
khám phá những ứng dөng tiềm năng cӫa dữ liệu lớn trong số liệu thống kê chính thức, 
và nghiên cứu thực nghiệm đó phải là một phần trong quy trình sản xuất số liệu thống kê. 
(2) Nghiên cứu về dữ liệu lớn cần phải có cơ sở hạ tầng công nghệ thông tin hiện 
đại, đáp ứng các yêu cầu xử lỦ khối lượng lớn dữ liệu và nhanh, đồng thời có thể tập hợp 
dữ liệu từ nhiều nguồn khác nhau. Thực hiện được điều này chúng ta có được đội ngũ 
qua kinh nghiệm thực tế. 
(3) Tiếp cận và nghiên cứu về dữ liệu lớn sẽ giúp chúng ta có được những văn bản 
được khai thác dữ liệu thông qua hồ sơ hành chính, ngoài ra dữ liệu cũng được bảo đảm 
và giữ bí mật nhờ những văn bản pháp lỦ bổ sung này. 
(4) Sử dөng dữ liệu lớn đem lại niềm tin cӫa cộng đồng với thống kê chính thức do 
tác động chӫ Ủ cӫa con người. 
3.2 Thách thͱc  
(1)Tài chính 
Nhiều đơn vị, tổ chức không đo lường được vấn đề sẽ phát sinh trong quá trình triển 
khai thực hiện, dự toán kinh phí chưa chính xác, do vậy dự án không thực hiện được. Để 
triển khai được thành công, yếu tố tài chính có Ủ nghĩa rất quan trọng, một số tập đoàn 
Big data như IBM, website bán hàng thương mại điện tử Amazon ... 
(2) Chính sách, quy định Luật pháp về truy cập và sử dụng dữ liệu 
Việc sử dөng và khai thác dữ liệu lớn phө thuộc vào luật quy định cӫa mỗi quốc gia. 
1 Xem Báo cáo “Thống kê chính thức với Big data: Kinh nghiệm quốc tế và định hướng của Thống kê Việt Nam. 
6 
  Ví dө: ở Canada người dùng có thể được tiếp cận dữ liệu từ cả hai tổ chức chính 
phӫ và phi chính phӫ, nhưng ở những nước khác như Ireland thì phải được sự cho phép 
từ các cơ quan chính phӫ. Điều này có thể dẫn đến những hạn chế để truy cập vào một số 
loại dữ liệu lớn. 
 (3) Trình độ khai thác và quản lý dữ liệu  
quản lỦ là cũng khác nhau tuy nhiên, Một vấn đề liên quan đến quản lỦ thông tin hiện nay 
là nguồn nhân lực. Khoa học dữ liệu lớn đang phát triển mạnh trong những tổ chức tư 
nhân, trong khi đó bộ phận này chưa được liên kết với những tổ chức cӫa chính phӫ một 
cách chặt chẽ dẫn đến việc quản lỦ vẫn còn nhiều vướng mắc.. 
(4) Hạ tầng Công nghệ thông tin 
sử dөng giao diện ứng dөng cӫa Chương trình chuyên sâu tiêu chuẩn (API) để truy cập 
dữ liệu. Bằng cách này, nó có thể kết nối các ứng dөng cho dữ liệu thu về và xử lỦ dữ liệu 
trực tiếp với dữ liệu hành chính. Ngoài ra hệ thống khai thác dữ liệu lớn cũng cần phải 
được tính toán để có thể kết nối vào được kho cơ sở dữ liệu truyền thống, đó cũng là một 
trong những thách thức lớn cần được giải quyết.  
data, những lợi ích mà Big data mang lại cho chúng ta. Bên cạnh đó cũng chỉ ra những 
thách thức khi triển khai áp dөng khai thác Big data. 
cung cấpthông tin để chung ta xử lỦ được tình huống nhanh nhất, chính xác nhất và giá trị 
cӫa Big data mang lại luôn có tính định hướng đến tương  lai ? giải đáp những câu hỏi tại 
sao việc ấy lại xảy ra?;  Sau chuyện đó thì điều gì sẽ sảy ra? và chúng ta nên ứng phó như 
thế nào trong hoàn cảnh đó? 
1. Tài liệu cơ hội và thách thức với bigdata –E cӫa Liên Hợp Quốc: 
http://unstats.un.org/unsd/statcom/doc14/2014-11-BigData-E.pdf 
2. Báo cáo Hội thảo về tương lai cӫa Thống kê học London: 
https://statistics.stanford.edu/statistics-and-science-london-workshop-report 
3. Tài liệu về các khái niệm và đặc trưng cӫa Big data: 
https://viblo.asia/dovv/posts/3OEqGjWwv9bL 

Removed lines from what-is-big-data-ebook-4421383.pdf:
What is Big Data? 
04 
  08 
Big Data Use Cases                                                                      10 
                                                             13 
15 
18 
4 
5 
What exactly is big data? 
To put it simply: big data is larger, more 
complex data sets, especially from new data 
sources. These data sets are so voluminous that 
traditional data processing software just can’t 
manage them. But these massive volumes of 
you wouldn’t have been able to tackle before. 
To really understand big data, it’s helpful to have 
some historical background. Here’s Gartner’s 
defnition, circa 2001(which is still the go-to 
defnition): 
“Big data is data that contains greater variety 
arriving in increasing volumes and with ever 
higher velocity. This is known as the three Vs.” 
• Volume.The amount of data matters. With 
big data, you’ll have to process high volumes 
of low-density, unstructured data. This can be 
data of unknown value, such as Twitter data 
feeds, clickstreams on a webpage or a mobile 
app, or sensor-enabled equipment. For some 
organizations, this might be tens of 
terabytes of data. For others, it may be 
hundreds of petabytes. 
Velocity. Velocity is the fast rate at which data 
is received and (perhaps) acted on. Normally, 
memory versus being written to disk.  Some 
internet-enabled smart products operate in real 
real-time evaluation and action. 
• Variety. In today’s big data world, data 
comes in new unstructured data types. 
Unstructured and semi-structured data types, 
such as text, audio, and video require addition 
support metadata. 
6 
Volume
1 
2 
3 
THE VALUE—AND TRUTH—OF 
Since 2001, two more Vs have become apparent: 
value and veracity. Data has intrinsic value. But 
it’s of no use until that value is discovered. 
Equally important: How truthful is your data—and 
how much can you rely on it? 
Today, big data has become capital. Think of 
some of the world’s biggest tech companies. A 
their data, which they’re constantly analyzing to 
new products. 
and compute, making it easier and less expensive 
to store more data than ever before. With an 
increased volume of big data now cheaper and 
more accessible, you can make more accurate 
and precise business decisions. 
Finding value in big data isn’t only about 
analyzing it (which is a whole other beneft). 
It’s an entire discovery process that requires 
insightful analysts, business users, and 
executives who ask the right questions, recognize 
patterns, make informed assumptions, and 
predict behavior. 
But how did we get here? 
7 
8 
Around 2005, people began to realize just how 
much data users generated through Facebook, 
YouTube, and other online services. Hadoop (an 
open-source framework created specifcally to 
store and analyze big data sets) was developed 
that same year. NoSQL also began to gain 
popularity during this time. 
The development of open-source frameworks, 
such as Hadoop (and more recently, Spark) was 
store. In the years since then, the volume of big 
data has skyrocketed. Users are still generating 
huge amounts of data—but it’s not just humans. 
With the advent of Internet of Things (IoT) , more 
objects and devices are connected to the internet, 
product performance. The emergence of machine 
learning has produced still more data. 
While big data has come far, its popularity is only 
just beginning. Cloud computing has expanded 
big data possibilities even further. 
The cloud offers a truly elastic scalability, where 
test around a subset of data. It’s an exciting time 
to see what’s going to happen next. 
THE VALUE OF BIG DATA COMES IS TWOFOLD: 
1. Big data makes it possible for you to gain 
2. More complete answers means more 
confdence in the data–which means 
a completely different approach to 
9 
10 
cases that you haven’t been able to fully delve 
into before. Here are just a few.  (More use cases 
are on our solutions page): 
Companies like Netfix and Procter & Gamble 
use big data to anticipate customer demand. 
products or services, and then modeling the 
commercial success of the offerings, they build 
predictive models for new products and services. 
In addition, P&G uses data and analytics from 
focus groups, social media, test markets, and 
early store rollouts to plan, produce, and launch 
new products. 
be deeply buried in structured data, such as the 
equipment year, make, and model, as well as 
entries, senor data, error messages, and engine 
temperature., By analyzing these indications of 
potential issues before the problems happen, 
equipment uptime. 
The race for customers is on. A clearer view of 
ever before. Big data enables you to gather data 
from social media, web visits, call logs, and 
experience and maximize the value delivered. 
Start delivering personalized offers, reduce 
customer churn, and handle issues proactively. 
When it comes to security, it’s not just a few 
rogue hackers; you’re up against entire expert 
teams. Security landscapes and compliance 
requirements are constantly evolving. Big 
indicate fraud and aggregate large volumes of 
much faster. 
now. And data—specifcally big data—is one of 
the reasons why. It’s only recently that we’ve 
them. And the availability of big data to train 
machine-learning models makes that happen. 
11 
news, but it’s an area in which big data is having 
the most impact. With big data, you can analyze 
and assess production, customer feedback and 
returns, and other factors to reduce outages and 
anticipate future demands. Big data can also be 
used to improve decision-making in line with 
current market demand. 
interdependencies between humans, institutions, 
entities, and process and then determining new 
ways to use those insights. Use data insights to 
considerations. Examine trends and what 
services. Implement dynamic pricing. There are 
endless possibilities. 
While big data holds a lot of promise, it is not 
without its challenges.  
First, big data is... big. Although new 
technologies have been developed to store data, 
data volumes are doubling in size around every 
two years. Organizations still struggle to keep 
store it. 
But it’s not enough to just store the data. Data 
must be used to be valuable, and that depends 
on curation. Clean data, or data that’s relevant 
meaningful analysis requires a lot of work. 
Data scientists spend 50 to 80 percent of their 
actually be used. 
Finally, big data technology is changing at a fast 
pace. A few years ago, Apache Hadoop was the 
popular technology used to handle big data. That 
is, until Apache Spark was introduced in 2014. 
Today, a combination of the two frameworks 
appears to be the best approach. Keeping up with 
big data technology is an ongoing challenge. 
12 
13 
Oracle Cloud for  Big  Data  Analytics 
Data 
Enterprise Apps 
Data 
new opportunities and business models. Getting 
started involves three key actions:   
disparate sources and applications. Traditional 
data integration mechanisms, such as ETL 
(extract, transform, and load) generally aren’t 
up to the task. It requires new strategies and 
technologies to analyze big data sets at terabyte, 
or even petabyte, scale. At the same time, big 
data has the same requirements for quality, 
governance, and confdence as traditional data 
sources. During integration, you need to bring in 
the data, process it, and make sure it’s formatted 
analysts can get started with. 
Big data requires storage. Your storage solution 
can be in the cloud, on-premises, or both. 
on an on-demand basis. Many people choose 
data is currently residing. The cloud is gradually 
gaining popularity because it supports your 
to spin up resources as needed. 
analyze and act on your data. Get new clarity with 
a visual analysis of your varied data sets. Explore 
the data further to make new discoveries. Share 
your fndings with others. Build data models with 
machine learning and artifcial intelligence. Put 
your data to work. 
To help you on your big data journey, we’ve put 
in mind. Here are our guidelines for building a 
successful big data foundation. 
14 
15 
#1: ALIGN BIG DATA WITH 
new discoveries. To that end, it is important to 
base new investments in skills, organization, 
or infrastructure with a strong business-
investments and funding. To determine if 
you are on the right track, ask how big data 
supports and enables your top business and IT 
priorities. Examples include understanding how 
behavior, deriving sentiment from social 
media and customer support interactions, and 
and their relevance for customer, product, 
manufacturing, and engineering data. 
#2: EASE SKILLS SHORTAGE 
shortage. You can mitigate this risk by ensuring 
that big data technologies, considerations, 
governance program. 
Standardizing your approach will allow you 
to manage costs and leverage resources. 
proactively identify any potential skill gaps. These 
can be addressed by training/cross-training 
existing resources, hiring new resources, and 
leveraging consulting frms. 
#3: OPTIMIZE KNOWLEDGE 
Use a Center of Excellence approach to share 
knowledge, control oversight, and manage 
project communications. Whether big data is a 
new or an expanding investment, the soft and 
hard costs can be shared across the enterprise. 
Leveraging this approach can help increase 
systematic way. 
#4:TOP PAYOFF IS ALIGNING 
own.But you can bring even greater business 
already using today. 
16 
Whether you are capturing customer, product, 
equipment, or environmental big data, the goal 
core master and analytical summaries, leading 
to better conclusions. For example, there is 
sentiment from that of only your best customers. 
capabilities, data warehousing platform, and 
information architecture. 
processes and models can be both human- and 
machine-based. Big data analytical capabilities 
include statistics, spatial analysis, semantics, 
interactive discovery, and visualization. Using 
analytical models, you can correlate different 
and meaningful discoveries. 
#5: PLAN YOUR DISCOVERY LAB 
straightforward. Sometimes we don’t even 
know what we’re looking for. That’s expected. 
Management and IT needs to support this “lack 
of direction” or “lack of clear requirement.” 
At the same time, it’s important for analysts and 
requirements. To accommodate the interactive 
statistical algorithms, you need high-performance 
work areas. Be sure that sandbox environments 
have the power they need—and are 
properly governed. 
#6: ALIGN WITH THE CLOUD 
jobs. A big data solution includes all data 
realms including transactions, master data, 
reference data, and summarized data. Analytical 
sandboxes should be created on demand. 
control of the entire data fow including pre- 
and post-processing, integration, in-database 
summarization, and analytical modeling. A well-
supporting these changing requirements. 
17 
18 
Clearly, big data has tremendous potential. 
customers, make more accurate decisions, and 
create new growth opportunities. Contact us to 
learn more. 
See how Oracle can help your big data journey. 
Start your free trial today. 
Contact us URL: 
https://www.oracle.com/marketingcloud/contact­
sales.html 
Free Trial URL: 
https://go.oracle.com/LP=50758/? 
19 
Oracle Corporation 
Copyright © 2019, Oracle and/or its affiliates. All rights reserved. This document is provided for information purposes only, and the contents hereof are subject 
to change without notice. This document is not warranted to be error-free, nor subject to any other warranties or conditions, whether expressed orally or 
implied in law, including implied warranties and conditions of merchantability or fitness for a particular purpose. We specifically disclaim any liability with 
500 Oracle Parkway 
respect to this document, and no contractual obligations are formed either directly or indirectly by this document. This document may not be reproduced or 
transmitted in any form or by any means, electronic or mechanical, for any purpose, without our prior written permission. 
CA 94065 
Oracle and Java are registered trademarks of Oracle and/or its affiliates. Other names may be trademarks of their respective owners. 
USA 
Intel and Intel Xeon are trademarks or registered trademarks of Intel Corporation. All SPARC trademarks are used under license and are trademarks or 
registered trademarks of SPARC International, Inc. AMD, Opteron, the AMD logo, and the AMD Opteron logo are trademarks or registered trademarks of 
Advanced Micro Devices. UNIX is a registered trademark of The Open Group. 
Phone: +1.650.506.7000 
+1.800.ORACLE1 
Fax: 
+1.650.506.7200 
oracle.com 
